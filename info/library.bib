@book{Alaimo2013,
address = {Ciudad Autónoma de Buenos Aires},
author = {Alaimo, Diego Mart{\'{i}}n},
edition = {1},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Alaimo - 2013 - Proyectos {\'{a}}giles con {\#}Scrum flexibilidad, aprendizaje, innovaci{\'{o}}n y colaboraci{\'{o}}n en contextos complejos.pdf:pdf},
isbn = {9789874515810},
pages = {123},
publisher = {Kleer},
title = {{Proyectos {\'{a}}giles con {\#}Scrum : flexibilidad, aprendizaje, innovaci{\'{o}}n y colaboraci{\'{o}}n en contextos complejos}},
year = {2013}
}
@misc{Vaishnavi2013,
author = {Vaishnavi, Vijay and Kuechler, Bill},
booktitle = {2004},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Vaishnavi, Kuechler - 2013 - Design Science Research in Information Systems.pdf:pdf},
pages = {54},
title = {{Design Science Research in Information Systems}},
year = {2013}
}
@article{Isaza2014,
abstract = {Prediction of process behavior is important and useful to understand the system status and to take early control actions during operation. This paper presents a fuzzy clustering approach for predicting situations (functional states) in complex process industries. The proposed methodology combines a static measurement, such as the result of a fuzzy classifier trained with historical process data, and an estimation algorithm based on Markov‘s theory for discrete event systems. The situation prediction function is integrated into a process monitoring system without increasing the computational cost, which makes real-time implementation feasible. The monitoring strategy includes two principal stages: an offline stage for designing the fuzzy classifier and the predictor, and an online stage for identifying current process situations and for estimating predicted functional states. Thus, at each sample time, the results of a fuzzy classifier are used as inputs in the prediction procedure. An attractive feature of our proposed method, for situation prediction, is that it provides information about the evolution of the process. The proposed approach was tested on a monitoring system for a power transmission line, and also for monitoring a boiler subsystem of a steam generator. Experimental results indicate that our proposed technique in this paper is effective and can be used as a tool, for operators, to be used in industrial process decision making.},
author = {Isaza, Claudia V. and Sarmiento, Henry O. and Kempowsky-Hamon, Tatiana and LeLann, Marie-Veronique},
doi = {10.1016/j.ins.2014.04.030},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Isaza et al. - 2014 - Situation prediction based on fuzzy clustering for industrial complex processes.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Complex process,Fuzzy clustering,Markov's chain,Situation prediction,personal},
mendeley-tags = {personal},
month = {sep},
number = {7},
pages = {785--804},
publisher = {Elsevier Inc.},
title = {{Situation prediction based on fuzzy clustering for industrial complex processes}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025514004770 http://linkinghub.elsevier.com/retrieve/pii/S0020025514004770},
volume = {279},
year = {2014}
}
@article{Quevedo2010,
abstract = {This paper presents a signal analysis methodology to validate (detect) and reconstruct the missing and false data of a large set of flow meters in the telecontrol system of a water distribution network. The proposed methodology is based on two time-scale forecasting models: a daily model based on a ARIMA time series, while the 10-min model is based on distributing the daily flow using a 10-min demand pattern. The demand patterns have been determined using two methods: correlation analysis and an unsupervised fuzzy logic classification, named LAMDA algorithm. Finally, the proposed methodology has been applied to the Barcelona water distribution network, providing very good results. {\textcopyright} 2010 Elsevier Ltd.},
author = {Quevedo, J. and Puig, V. and Cembrano, G. and Blanch, J. and Aguilar, J. and Saporta, D. and Benito, G. and Hedo, M. and Molina, A.},
doi = {10.1016/j.conengprac.2010.03.003},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Quevedo et al. - 2010 - Validation and reconstruction of flow meter data in the Barcelona water distribution network.pdf:pdf},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Fault detection,Flow meter,Fuzzy logic classifier,Sensor failure,Telecontrol system,Water distribution network,personal},
mendeley-tags = {personal},
month = {jun},
number = {6},
pages = {640--651},
publisher = {Elsevier},
title = {{Validation and reconstruction of flow meter data in the Barcelona water distribution network}},
url = {http://dx.doi.org/10.1016/j.conengprac.2010.03.003 http://linkinghub.elsevier.com/retrieve/pii/S0967066110000791},
volume = {18},
year = {2010}
}
@article{Botia2013,
abstract = {Fuzzy clustering allows finding classes through the historical data in order to associate them with functional states useful to represent the complex industrial processes behavior. By means of classes, an automaton can be established that determines the current and the next connections of functional states of a process. When fuzzy clustering is used, the connections in the historical data are considered but it does not find other important connections. To solve this limitation, a new method to seek the most important connections among functional states is proposed. Initially, the approach defines an initial transition degrees matrix, where all connections are taken into account. Through a proposed update step, the most important connections are obtained, which they describe the real behavior of a process. In addition, a new distance criterion is defined to improve the update step. The final transition degrees matrix is used to construct a fuzzy automaton that it is validated by human operator's experience. The approach was tested in a steam generator process. Applying three fuzzy clustering algorithms in case of study, the proposed method finds the same transition matrix. The new connections were validated by the human operator. Crown Copyright {\textcopyright} 2012 Published by Elsevier Ltd. All rights reserved.},
author = {Bot{\'{i}}a, Javier F. and Isaza, Claudia and Kempowsky, Tatiana and {Le Lann}, Marie V{\'{e}}ronique and Aguilar-Mart{\'{i}}n, Joseph},
doi = {10.1016/j.engappai.2012.11.003},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bot{\'{i}}a et al. - 2013 - Automaton based on fuzzy clustering methods for monitoring industrial processes.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Fuzzy automaton,Fuzzy clustering method,Hebbian functions,personal},
mendeley-tags = {personal},
month = {apr},
number = {4},
pages = {1211--1220},
title = {{Automaton based on fuzzy clustering methods for monitoring industrial processes}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0952197612002886},
volume = {26},
year = {2013}
}
@book{Bosch2013,
abstract = {@incollection{\{} year={\{}2013{\}}, isbn={\{}978-3-642-36582-9{\}}, booktitle={\{}Systems and Software Variability Management{\}}, editor={\{}Capilla, Rafael and Bosch, Jan and Kang, Kyo-Chul{\}}, doi={\{}10.1007/978-3-642-36583-6{\_}1{\}}, title={\{}Software Product Line Engineering{\}}, url={\{}http://dx.doi.org/10.1007/978-3-642-36583-6{\_}1{\}}, publisher={\{}Springer Berlin Heidelberg{\}}, author={\{}Bosch, Jan{\}}, language={\{}English{\}}}},
address = {Berlin, Heidelberg},
author = {Bosch, Jan and Deelstra, Sybren and Sinnema, Marco},
booktitle = {Systems and Software Variability Management},
doi = {10.1007/978-3-642-36583-6},
editor = {Capilla, Rafael and Bosch, Jan and Kang, Kyo-Chul},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bosch, Deelstra, Sinnema - 2013 - Systems and Software Variability Management.pdf:pdf},
isbn = {978-3-642-36582-9},
pages = {10},
publisher = {Springer Berlin Heidelberg},
title = {{Systems and Software Variability Management}},
url = {10.1007/978-3-642-36583-6{\%}5Cnhttp://link.springer.com/10.1007/978-3-642-36583-6 http://link.springer.com/10.1007/978-3-642-36583-6},
year = {2013}
}
@article{Petersen2015,
abstract = {Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
doi = {10.1016/j.infsof.2015.03.007},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Petersen, Vakkalanka, Kuzniarz - 2015 - Guidelines for conducting systematic mapping studies in software engineering An update.pdf:pdf},
isbn = {0360-1315},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Guidelines,Software engineering,Systematic mapping studies},
month = {aug},
pages = {1--18},
pmid = {25246403},
publisher = {Elsevier B.V.},
title = {{Guidelines for conducting systematic mapping studies in software engineering: An update}},
url = {http://dx.doi.org/10.1016/j.infsof.2015.03.007 http://linkinghub.elsevier.com/retrieve/pii/S0950584915000646},
volume = {64},
year = {2015}
}
@book{Simpson,
address = {New York, NY},
author = {Simpson, Timothy W and Jiao, Jianxin Roger},
doi = {10.1007/978-1-4614-7937-6},
editor = {Simpson, Timothy W. and Jiao, Jianxin and Siddique, Zahed and H{\"{o}}ltt{\"{a}}-Otto, Katja},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Simpson, Jiao - 2014 - Advances in Product Family and Product Platform Design.pdf:pdf},
isbn = {978-1-4614-7936-9},
keywords = {naranja},
mendeley-tags = {naranja},
publisher = {Springer New York},
title = {{Advances in Product Family and Product Platform Design}},
url = {http://link.springer.com/10.1007/978-1-4614-7937-6},
year = {2014}
}
@book{,
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
isbn = {978-1-4516-5530-8},
title = {{No Title}}
}
@inproceedings{Kalagnanam2004,
address = {New York, New York, USA},
author = {Kalagnanam, Jayant and Singh, Moninder and Verma, Sudhir and Patek, Michael and Wong, Yuk Wah},
booktitle = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '04},
doi = {10.1145/1014052.1016918},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kalagnanam et al. - 2004 - A system for automated mapping of bill-of-materials part numbers.pdf:pdf},
isbn = {1581138889},
pages = {805},
publisher = {ACM Press},
title = {{A system for automated mapping of bill-of-materials part numbers}},
url = {http://portal.acm.org/citation.cfm?doid=1014052.1016918},
year = {2004}
}
@article{EncyclopædiaBritannicaInc.2015,
author = {{Jianxin Jiao} and Tseng, M. M. and {Qinhai Ma} and {Yi Zou}},
doi = {10.1177/1063293X0000800404},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Jianxin Jiao et al. - 2000 - Generic Bill-of-Materials-and-Operations for High-Variety Production Management.pdf:pdf},
issn = {1063-293X},
journal = {Concurrent Engineering},
keywords = {article,britannica,encyclopaedia,encyclopedia,production management},
month = {dec},
number = {4},
pages = {297--321},
title = {{Generic Bill-of-Materials-and-Operations for High-Variety Production Management}},
url = {http://www.britannica.com/topic/production-management http://cer.sagepub.com/cgi/doi/10.1177/1063293X0000800404},
volume = {8},
year = {2000}
}
@inproceedings{Yang2015a,
author = {{Guanzhong Yang} and {Yaru Zhang}},
booktitle = {2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)},
doi = {10.1109/FSKD.2015.7382185},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Guanzhong Yang, Yaru Zhang - 2015 - A feature-oriented modeling approach for embedded product line engineering.pdf:pdf},
isbn = {978-1-4673-7682-2},
keywords = {-,concept,domain,embedded,feature modeling,gris,naranja},
mendeley-tags = {gris,naranja},
month = {aug},
pages = {1607--1612},
publisher = {IEEE},
title = {{A feature-oriented modeling approach for embedded product line engineering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7382185},
year = {2015}
}
@article{Dhungana2010,
abstract = {The scale and complexity of product lines means that it is practically infeasible to develop a single model of the entire system, regardless of the languages or notations used. The dynamic nature of real-world systems means that product line models need to evolve continuously to meet new customer requirements and to reflect changes of product line artifacts. To address these challenges, product line engineers need to apply different strategies for structuring the modeling space to ease the creation and maintenance of models. This paper presents an approach that aims at reducing the maintenance effort by organizing product lines as a set of interrelated model fragments defining the variability of particular parts of the system. We provide support to semi-automatically merge fragments into complete product line models. We also provide support to automatically detect inconsistencies between product line artifacts and the models representing these artifacts after changes. Furthermore, our approach supports the co-evolution of models and their respective meta-models. We discuss strategies for structuring the modeling space and show the usefulness of our approach using real-world examples from our ongoing industry collaboration. ?? 2010 Elsevier Inc. All rights reserved.},
author = {Dhungana, Deepak and Gr??nbacher, Paul and Rabiser, Rick and Neumayer, Thomas},
doi = {10.1016/j.jss.2010.02.018},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Dhungana et al. - 2010 - Structuring the modeling space and supporting evolution in software product line engineering.pdf:pdf},
isbn = {10.1016/j.jss.2010.02.018},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Model evolution,Product line engineering,Variability modeling},
number = {7},
pages = {1108--1122},
publisher = {Elsevier Inc.},
title = {{Structuring the modeling space and supporting evolution in software product line engineering}},
url = {http://dx.doi.org/10.1016/j.jss.2010.02.018},
volume = {83},
year = {2010}
}
@incollection{PeterJ.Olver2013,
abstract = {Highly dynamic software systems are applications whose operations are particularly affected by changing requirements and uncertainty in their execution environments. Ideally such systems must evolve while they execute. To achieve this, highly dynamic software systems must be instrumented with self-adaptation mechanisms to monitor selected requirements and environment conditions to assess the need for evolution, plan desired changes, as well as validate and verify the resulting system. This chapter introduces fundamental concepts, methods, and techniques gleaned from self-adaptive systems engineering, as well as discusses their application to runtime evolution and their relationship with off-line software evolution theories. To illustrate the presented concepts, the chapter revisits a case study conducted as part of our research work, where self-adaptation techniques allow the engineering of a dynamic context monitoring infrastructure that is able to evolve at runtime. In other words, the monitoring infrastructure supports changes in monitoring requirements without requiring maintenance tasks performed manually by developers. The goal of this chapter is to introduce practitioners, researchers and students to the foundational elements of self-adaptive software, and their application to the continuos evolution of software systems at runtime.},
address = {Berlin, Heidelberg},
author = {Botterweck, Goetz and Pleuss, Andreas},
booktitle = {Evolving Software Systems},
doi = {10.1007/978-3-642-45398-4_9},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Botterweck, Pleuss - 2014 - Evolution of Software Product Lines.pdf:pdf},
isbn = {978-3-642-45397-7},
keywords = {naranja},
mendeley-tags = {naranja},
pages = {265--295},
publisher = {Springer Berlin Heidelberg},
title = {{Evolution of Software Product Lines}},
url = {http://link.springer.com/10.1007/978-3-642-45398-4{\_}9},
volume = {234},
year = {2014}
}
@article{Pablo2012,
author = {Pablo, S and Garc, Diego and Zorrilla, Marta and Matem, Dpto},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pablo et al. - 2012 - Software Product Line Engineering for e-Learning Applications A Case Study.pdf:pdf},
isbn = {978-1-4673-4743-3},
journal = {Proceedings of the 2012 International Symposium on Computers in Education (SIIE)},
keywords = {gris,purpura},
mendeley-tags = {gris,purpura},
pages = {1 -- 6},
title = {{Software Product Line Engineering for e-Learning Applications : A Case Study}},
year = {2012}
}
@book{Pohl2005,
author = {Pohl, Klaus and B{\"{o}}ckle, G{\"{u}}nter and van der Linden, Frank},
booktitle = {Springer-Verlag Berlin Heidelberg},
doi = {10.1007/3-540-28901-1},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pohl, B{\"{o}}ckle, van der Linden - 2005 - Software Product Line Engineering Foundations, Principles and Techniques.pdf:pdf},
isbn = {3-540-24372-0},
title = {{Software Product Line Engineering: Foundations, Principles and Techniques}},
year = {2005}
}
@article{B??can2015,
abstract = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM – as defined by its feature hierarchy and feature groups – may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontolog-ical semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weak-nesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs.},
author = {B{\'{e}}can, Guillaume and Acher, Mathieu and Baudry, Benoit and Nasr, Sana Ben},
doi = {10.1007/s10664-014-9357-1},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/B{\'{e}}can et al. - 2016 - Breathing ontological knowledge into feature model synthesis an empirical study.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Feature model,Model management,Refactoring,Reverse engineering,Software product lines,Variability,gris,personal},
mendeley-tags = {gris,personal},
month = {aug},
number = {4},
pages = {1794--1841},
title = {{Breathing ontological knowledge into feature model synthesis: an empirical study}},
url = {http://link.springer.com/10.1007/s10664-014-9357-1},
volume = {21},
year = {2016}
}
@incollection{Acher2010,
abstract = {The use of Feature Models (FMs) to define the valid combinations of features in Software Product Lines (SPL) is becoming commonplace. To enhance the scalability of FMs, support for composing FMs describing different SPL aspects is needed. Some composition operators, with interesting property preservation capabilities, have already been defined but a comprehensive and efficient implementation is still to be proposed. In this paper, we systematically compare strengths and weaknesses of different implementation approaches. The study provides some evidence that using generic model composition frameworks are not helping much in the realization, whereas a specific solution is finally necessary and clearly stands out by its qualities.},
annote = {Un diagrama de caracteristicas es la representaci{\'{o}}n de una familia.
Un FM describe el conjunto de las combinaciones validas entre caracteristicas.
En la actualidad la creaci{\'{o}}n de los modelos de caracteristicas depende de los procesos industriales(servicios),cada uno tiene su modelo de caracteristicas y el modelo de caracteristicas general se puede sacar con la intersecci{\'{o}}n de los modelos en todo el proceso.
se conocen como Multi level features
trees
AGG (sistema de trasnformaci{\'{o}}n algebraico)Taentzer, G es una interfaz gr{\'{a}}fica que pretende pasar del algebra relacional a los grafos y generar nuevos grafos a partir de unos ya existentes.
En este articulo se presenta una comparativa de estos software 
EL sosftware AGG, Kompose provee una resistencia a fallar en las operaciones donde no se puede generar un FM por que no hay suficientes caracteristicas.
MBE y AOM no tienen destreza para implementar la union de modelos de caracteristicas.
Las trasnformaciones semanticas o los modelos de preservaci{\'{o}}n semantica guardan o incluyen la semantica a diferencia de los anteriores},
author = {Acher, Mathieu and Collet, Philippe and Lahire, Philippe and France, Robert},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-13595-8_3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Acher et al. - 2010 - Comparing Approaches to Implement Feature Model Composition.pdf:pdf},
isbn = {3642135943},
issn = {03029743},
keywords = {naranja,rojo},
mendeley-tags = {naranja,rojo},
pages = {3--19},
title = {{Comparing Approaches to Implement Feature Model Composition}},
url = {http://link.springer.com/10.1007/978-3-642-13595-8{\_}3},
volume = {6138 LNCS},
year = {2010}
}
@article{Bagheri2012a,
abstract = {Feature modeling an attractive technique for capturing commonality as well as variability within an application domain for generative programming and software product line engineering. Feature models symbolize an overarching representation of the possible application configuration space, and can hence be customized based on specific domain requirements and stakeholder goals. Most interactive or semiautomated feature model customization processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called P(N). Furthermore, we formalize the representation of soft constraints in fuzzy P(N) and explain how semiautomated feature model customization is performed in this setting. The model configuration derivation process that we propose respects the soundness and completeness properties. {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
author = {Bagheri, Ebrahim and Noia, Tommaso Di and Gasevic, Dragan and Ragone, Azzurra},
doi = {10.1002/smr.534},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bagheri et al. - 2012 - Formalizing interactive staged feature model configuration.pdf:pdf},
isbn = {9781450330565},
issn = {20477473},
journal = {Journal of Software: Evolution and Process},
keywords = {Feature models,Logic languages,Soft constraints,Software product lines,Variability,gris,rojo},
mendeley-tags = {gris,rojo},
month = {jun},
number = {4},
pages = {375--400},
pmid = {67195556},
title = {{Formalizing interactive staged feature model configuration}},
url = {http://doi.wiley.com/10.1002/smr.534},
volume = {24},
year = {2012}
}
@inproceedings{Salinesi2009a,
abstract = {Product line models (PLM) are important artifacts in product line engineering. Due to their size and complexity, it is difficult to detect defects in PLMs. The challenge is however important: any error in a PLM will inevitably impact configuration, generating issues such as incorrect product models, inconsistent architectures, poor reuse, difficulty to customize products, etc. Surveys on feature-based PLM verification approaches show that there are many verification criteria, that these criteria are defined in different ways, and that different ways of working are proposed to look for defect. The goal of this paper is to systematize PLM verification. Based on our literature review, we propose a list of 23 verification criteria that we think cover those available in the literature.},
author = {Salinesi, Camille and Rolland, Colette and Diaz, Daniel and Mazo, Ra{\'{u}}l},
booktitle = {2009 17th IEEE International Requirements Engineering Conference},
doi = {10.1109/RE.2009.57},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Salinesi et al. - 2009 - Looking for Product Line Feature Models Defects Towards a Systematic Classification of Verification Criteria.pdf:pdf},
isbn = {978-0-7695-3761-0},
issn = {1090705X},
keywords = {gris},
mendeley-tags = {gris},
month = {aug},
pages = {385--386},
publisher = {IEEE},
title = {{Looking for Product Line Feature Models Defects: Towards a Systematic Classification of Verification Criteria}},
url = {http://ieeexplore.ieee.org/document/5328480/},
year = {2009}
}
@article{Muller2011,
abstract = {Today many software systems are developed for a wide even anonymous audience. If needs of customers are too diverse, offering one software product can be too inflexible. In that situation offering a Software Product Line (SPL) can be economically advantageous. However, product management of SPLs has to decide what features to realize. This decision requires information on the utility features spend to customers and customer's willingness to pay (WTP). Conjoint Analysis (CA) is a promising way to measure both. However, the design and implementation of a conjoint study is costly even though much of the work to design a conjoint study has already been done in Software Product Line Engineering (SPLE). To utilize the work of SPLE for the design of conjoint surveys, we propose a procedure to automatically derive conjoint surveys from Feature Models and report on our implementation of a conjoint survey generator and a web-based surveying tool.},
author = {Muller, Johannes and Lillack, Max},
doi = {10.1109/SEAA.2011.73},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Muller, Lillack - 2011 - Conjoint Analysis of Software Product Lines A Feature Based Approach.pdf:pdf},
isbn = {978-1-4577-1027-8},
journal = {2011 37th EUROMICRO Conference on Software Engineering and Advanced Applications},
keywords = {-software product line,conjoint analysis,feature modeling,gris,marketing,ment,product manage-,scoping},
mendeley-tags = {gris},
pages = {374--377},
title = {{Conjoint Analysis of Software Product Lines: A Feature Based Approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6068370},
year = {2011}
}
@inproceedings{Lian2015,
abstract = {As an important research issue in software product line, feature selection is extensively studied. Besides the basic functional requirements (FRs), the non-functional requirements (NFRs) are also critical during feature selection. Some NFRs have numerical constraints, while some have not. Without clear criteria, the latter are always expected to be the best possible. However, most existing selection methods ignore the combination of constrained and unconstrained NFRs and FRs. Meanwhile, the complex constraints and dependencies among features are perpetual challenges for feature selection. To this end, this paper proposes a multi-objective optimization algorithm IVEA to optimize the selection of features with NFRs and FRs by considering the relations among these features. Particularly, we first propose a two-dimensional fitness function. One dimension is to optimize the NFRs without quantitative constraints. The other one is to assure the selected features satisfy the FRs, and conform to the relations among features. Second, we propose a violation-dominance principle, which guides the optimization under FRs and the relations among features. We conducted comprehensive experiments on two feature models with different sizes to evaluate IVEA with state-of-the-art multi-objective optimization algorithms, including IBEAHD, IBEA$\epsilon$+, NSGA-II and SPEA2. The results showed that the IVEA significantly outperforms the above baselines in the NFRs optimization. Meanwhile, our algorithm needs less time to generate a solution that meets the FRs and the constraints on NFRs and fully conforms to the feature model.},
author = {Lian, Xiaoli and Zhang, Li},
booktitle = {2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
doi = {10.1109/SANER.2015.7081829},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Lian, Zhang - 2015 - Optimized feature selection towards functional and non-functional requirements in Software Product Lines.pdf:pdf},
isbn = {978-1-4799-8469-5},
keywords = {Evolutionary computation,Feature Models,Feature Selection,IBEAHD,IBEA$\epsilon$+,IVEA,Multi-objective Optimization,NFR optimization,NSGA-II,Non-functional requirements optimization,Optimization,Portals,SPEA2,Security,Sociology,Software,Software Product Line,Statistics,feature selection,genetic algorithms,multiobjective optimization algorithm,nonfunctional requirements,numerical constraint,optimized feature selection,purpura,rojo,selection method,software product line,software product lines,two-dimensional fitness function,violation-dominance principle},
mendeley-tags = {purpura,rojo},
month = {mar},
pages = {191--200},
publisher = {IEEE},
title = {{Optimized feature selection towards functional and non-functional requirements in Software Product Lines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7081829},
year = {2015}
}
@inproceedings{Czarnecki2008,
abstract = {We present probabilistic feature models (PFMs) and illustrate their use by discussing modeling, mining and interactive configuration. PFMs are formalized as a set of formulas in a certain probabilistic logic. Such formulas can express both hard and soft constraints and have a well defined semantics by denoting a set of joint probability distributions over features. We show how PFMs can be mined from a given set of feature configurations using data mining techniques. Finally, we demonstrate how PFMs can be used in configuration in order to provide automated support for choice propagation based on both hard and soft constraints. We believe that these results constitute solid foundations for the construction of reverse engineering tools for software product lines and configurators using soft constraints.},
author = {Czarnecki, Krzysztof and She, Steven and Wasowski, Andrzej},
booktitle = {2008 12th International Software Product Line Conference},
doi = {10.1109/SPLC.2008.49},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Czarnecki, She, Wasowski - 2008 - Sample Spaces and Feature Models There and Back Again.pdf:pdf},
isbn = {978-0-7695-3303-2},
keywords = {gris,purpura},
mendeley-tags = {gris,purpura},
month = {sep},
pages = {22--31},
publisher = {IEEE},
title = {{Sample Spaces and Feature Models: There and Back Again}},
url = {http://ieeexplore.ieee.org/document/4626837/},
year = {2008}
}
@inproceedings{Martinez2014,
abstract = {Software Product Line Engineering is a mature approach enabling the derivation of product variants by assembling reusable assets. In this context, domain experts widely use Feature Models as the most accepted formalism for capturing commonality and variability in terms of features. Feature Models also describe the constraints in feature combinations. In industrial settings, domain experts often deal with Software Product Lines with high numbers of features and constraints. Furthermore, the set of features are often regrouped in different subsets that are overseen by different stakeholders in the process. Consequently, the management of the complexity of large Feature Models becomes challenging. In this paper we propose a dedicated interactive visualisation paradigm to help domain experts and stakeholders to manage the challenges in maintaining the constraints among features. We build Feature Relations Graphs (Frogs) by mining existing product configurations. For each feature, we are able to display a Frog which shows the impact, in terms of constraints, of the considered feature on all the other features. The objective is to help domain experts to 1) obtain a better understanding of feature constraints, 2) potentially refine the existing feature model by uncovering and formalizing missing constraints and 3) serve as a recommendation system, during the configuration of a new product, based on the tendencies found in existing configurations. The paper illustrates the visualisation paradigm with the industrial case study of Renault's Electric Parking System Software Product Line.},
author = {Martinez, Jabier and Ziadi, Tewfik and Mazo, Raul and Bissyande, Tegawende F. and Klein, Jacques and Traon, Yves Le},
booktitle = {2014 Second IEEE Working Conference on Software Visualization},
doi = {10.1109/VISSOFT.2014.18},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Martinez et al. - 2014 - Feature Relations Graphs A Visualisation Paradigm for Feature Constraints in Software Product Lines.pdf:pdf},
isbn = {978-1-4799-6150-4},
keywords = {rojo},
mendeley-tags = {rojo},
month = {sep},
pages = {50--59},
publisher = {IEEE},
title = {{Feature Relations Graphs: A Visualisation Paradigm for Feature Constraints in Software Product Lines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6980213},
year = {2014}
}
@article{Soares2014,
abstract = {Software Product Lines (SPL) approach has been widely developed in academia and successfully applied in industry. Based on the selection of features, stakeholders can efficiently derive tailor-made programs satisfying different requirements. While SPL was very successful at building products based on identified features, achievements and preservation of many nonfunctional properties (NFPs) remain challenging. A knowledge how to deal with NFPs is still not fully obtained. In this paper, we present a systematic literature review of NFPs analysis for SPL products, focusing on runtime NFPs. The goal of the paper is twofold: (i) to present an holistic overview of SPL approaches that have been reported regarding the analysis of runtime NFPs, and (ii) to categorize NFPs treated in the scientific literature regarding development of SPLs. We analyzed 36 research papers, and identified that system performance attributes are typically the most considered. The results also aid future research studies in NFPs analysis by providing an unbiased view of the body of empirical evidence and by guiding future research directions.},
author = {Soares, Larissa Rocha and Potena, Pasqualina and Machado, Ivan Do Carmo and Crnkovic, Ivica and {De Almeida}, Eduardo Santana D},
doi = {10.1109/SEAA.2014.48},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Soares et al. - 2014 - Analysis of non-functional properties in software product lines A systematic review.pdf:pdf},
isbn = {9781479957941},
journal = {Proceedings - 40th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2014},
keywords = {Non-functional Properties,Product Derivation,Software Product Lines,Systematic Literature Review,gris},
mendeley-tags = {gris},
pages = {328--335},
title = {{Analysis of non-functional properties in software product lines: A systematic review}},
year = {2014}
}
@inproceedings{Zhang2014b,
abstract = {As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25{\%} features in each of the 20 products can be configured automatically.},
author = {Zhang, Bo and Becker, Martin},
booktitle = {2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
doi = {10.1109/SEAA.2014.34},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Becker - 2014 - Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement.pdf:pdf},
isbn = {978-1-4799-5795-8},
keywords = {feature correlation mining,naranja,product line configuration improvement,variability reverse engineering},
mendeley-tags = {naranja},
month = {aug},
pages = {320--327},
publisher = {IEEE},
title = {{Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6928830},
year = {2014}
}
@inproceedings{Davril2015b,
author = {Davril, Jean-Marc and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu},
booktitle = {2015 IEEE Second International Workshop on Artificial Intelligence for Requirements Engineering (AIRE)},
doi = {10.1109/AIRE.2015.7337624},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Davril et al. - 2015 - Using fuzzy modeling for consistent definitions of product qualities in requirements.pdf:pdf},
isbn = {978-1-5090-0125-5},
keywords = {Cameras,Documentation,Frequency modulation,Portable computers,Pragmatics,Product design,Quality assessment,purpura},
mendeley-tags = {purpura},
month = {aug},
pages = {1--8},
publisher = {IEEE},
title = {{Using fuzzy modeling for consistent definitions of product qualities in requirements}},
url = {http://ieeexplore.ieee.org/document/7337624/},
year = {2015}
}
@inproceedings{Al2013,
abstract = {Migrating software product variants which are deemed similar into a product line is a challenging task with main impact in software reengineering. To exploit existing software variants to build a software product line (SPL), the ﬁrst step is to mine the feature model of this SPL which involves extracting common and optional features. Thus, we propose, in this paper, a new approach to mine features from the object-oriented source code of software variants by using lexical and structural similarity. To validate our approach, we applied it on ArgoUML, Health Watcher and Mobile Media software. The results of this evaluation showed that most of the features were identiﬁed.},
author = {Al-msie'deen, R. and Seriai, A.-D. and Huchard, Marianne and Urtado, Christelle and Vauttier, Sylvain},
booktitle = {2013 IEEE 14th International Conference on Information Reuse {\&} Integration (IRI)},
doi = {10.1109/IRI.2013.6642522},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Al-msie'deen et al. - 2013 - Mining features from the object-oriented source code of software variants by combining lexical and structur.pdf:pdf},
isbn = {978-1-4799-1050-2},
keywords = {Code Dependencies,Computer Science/Software Engineering,Feature Mining,Formal Concept Analysis,Informatique/G{\'{e}}nie Logiciel,Latent Semantic Indexing,Software Product Line,Software Product Variants,Structural Similarity,naranja},
mendeley-tags = {naranja},
month = {aug},
pages = {586--593},
publisher = {IEEE},
title = {{Mining features from the object-oriented source code of software variants by combining lexical and structural similarity}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6642522},
year = {2013}
}
@inproceedings{Zhang2013,
abstract = {As a Software Product Line (SPL) evolves variability specifications in problem space and variability realizations in solution space erode over time and impact productivity during development. On the one hand, the variability model tends to be incomplete and inconsistent with the core assets; on the other hand, the core assets become overly complex, which make them difficult to understand and maintain. In this paper, we present the RECoVar framework towards reverse engineering of SPL variability. The framework includes two approaches: a) code-based variability model extraction; and b) complex feature correlation mining. These two approaches help to extract various variability information, so that variability specifications and realizations can be maintained in an efficient way.},
author = {Zhang, Bo and Becker, Martin},
booktitle = {2013 4th International Workshop on Product LinE Approaches in Software Engineering (PLEASE)},
doi = {10.1109/PLEASE.2013.6608664},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Becker - 2013 - RECoVar A solution framework towards reverse engineering variability.pdf:pdf},
isbn = {978-1-4673-6449-2},
keywords = {Variability modeling,naranja,product line analysis,reverse engineering},
mendeley-tags = {naranja},
month = {may},
pages = {45--48},
publisher = {IEEE},
title = {{RECoVar: A solution framework towards reverse engineering variability}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6608664},
year = {2013}
}
@inproceedings{Huang2013,
abstract = {The mining software repositories (MSR) analyze data stored in software repositories and discover meaningful information to support software development. However, MSR is complex due to conducting large scale data collection with various repositories. To help practitioners perform MSR analysis, one possible way is to apply the approaches of software product line (SPL) to the MSR domain to understand variability and commonality for the domain, and to construct domain specific languages (DSLs) because DSLs have high readability to reduce the complexity of the procedure of MSR. In this paper, we construct a SQL-based DSL to support MSR and provide a systematic approach to conduct Feature-Oriented Domain Analysis (FODA) for MSR towards the construction of the DSL. We provide the syntax of the DSL and explain how to locate language elements of the DSL to the four-layer structure used in FODA. {\textcopyright} 2013 IEEE.},
author = {Huang, Changyun and Yamashita, Kazuhiro and Kamei, Yasutaka and Hisazumi, Kenji and Ubayashi, Naoyasu},
booktitle = {2013 4th International Workshop on Product LinE Approaches in Software Engineering (PLEASE)},
doi = {10.1109/PLEASE.2013.6608663},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2013 - Domain analysis for mining software repositories Towards feature-based DSL construction.pdf:pdf},
isbn = {978-1-4673-6449-2},
keywords = {naranja,rojo},
mendeley-tags = {naranja,rojo},
month = {may},
pages = {41--44},
publisher = {IEEE},
title = {{Domain analysis for mining software repositories: Towards feature-based DSL construction}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6608663{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6608663 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6608663},
year = {2013}
}
@inproceedings{Somprasertsri2010,
abstract = {In web pages, the reviews are written in natural language and are unstructured-free-texts scheme. Online product reviews is considered as a significant informative resource which is useful for both potential customers and product manufacturers. The task of manually scanning through large amounts of review one by one is computational burden and is not practically implemented with respect to businesses and customer perspectives. Therefore it is more efficient to automatically process the various reviews and provide the necessary information in a suitable form. The task of product feature and opinion is to find product features that customers refer to their topic reviews. It would be useful to characterize the opinions about product. In this paper, we propose an approach to extract product features and to identify the opinions associated with these features from reviews through syntactic information based on dependency analysis. {\textcopyright}2010 IEEE.},
author = {Sompras, Gamgarn and Lalitrojwong, Pattarachai},
booktitle = {2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery},
doi = {10.1109/FSKD.2010.5569865},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Sompras, Lalitrojwong - 2010 - Extracting product features and opinions from product reviews using dependency analysis.pdf:pdf},
isbn = {978-1-4244-5931-5},
keywords = {Customer review,Dependency analysis,Opinion extraction,Opinion mining,rojo},
mendeley-tags = {rojo},
month = {aug},
number = {Fskd},
pages = {2358--2362},
publisher = {IEEE},
title = {{Extracting product features and opinions from product reviews using dependency analysis}},
url = {http://ieeexplore.ieee.org/document/5569865/},
volume = {5},
year = {2010}
}
@article{Hariri2013,
abstract = {Domain analysis is a labor-intensive task in which related software systems are analyzed to discover their common and variable parts. Many software projects include extensive domain analysis activities, intended to jumpstart the requirements process through identifying potential features. In this paper, we present a recommender system that is designed to reduce the human effort of performing domain analysis. Our approach relies on data mining techniques to discover common features across products as well as relationships among those features. We use a novel incremental diffusive algorithm to extract features from online product descriptions, and then employ association rule mining and the (k)-nearest neighbor machine learning method to make feature recommendations during the domain analysis process. Our feature mining and feature recommendation algorithms are quantitatively evaluated and the results are presented. Also, the performance of the recommender system is illustrated and evaluated within the context of a case study for an enterprise-level collaborative software suite. The results clearly highlight the benefits and limitations of our approach, as well as the necessary preconditions for its success.},
author = {Hariri, Negar and Castro-Herrera, Carlos and Mirakhorli, Mehdi and Cleland-Huang, Jane and Mobasher, Bamshad},
doi = {10.1109/TSE.2013.39},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hariri et al. - 2013 - Supporting Domain Analysis through Mining and Recommending Features from Online Product Listings.pdf:pdf},
isbn = {0098-5589 VO - 39},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Domain analysis,association rule mining,clustering,k-nearest neighbor,naranja,purpura,recommender systems,rojo},
mendeley-tags = {naranja,purpura,rojo},
month = {dec},
number = {12},
pages = {1736--1752},
title = {{Supporting Domain Analysis through Mining and Recommending Features from Online Product Listings}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6582404},
volume = {39},
year = {2013}
}
@article{Ali2009,
abstract = {Software product line engineering aims at achieving systematic reuse by exploiting commonalities among related products in order to reduce cost and time-to market. Before adopting this approach, organizations are likely to estimate the benefits they can expect to achieve and the level of investment required to transition to product line engineering. Several economic models and analysis approaches have been developed in order to help make a sound business case. There is a need to review the existing approaches in order to better understand the overall landscape of economic models. To this objective, this paper provides an overview of some existing economic models and discusses important issues and directions in product line economic modeling.},
author = {Ali, M S and Babar, M A and Schmid, K},
doi = {10.1109/SEAA.2009.89},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ali, Babar, Schmid - 2009 - A Comparative Survey of Economic Models for Software Product Lines.pdf:pdf},
isbn = {1089-6503},
issn = {1089-6503},
journal = {Software Engineering and Advanced Applications, 2009. SEAA '09. 35th Euromicro Conference on},
keywords = {commonalities,comparative survey,economic models,software engineering,software product lines,systematic reuse},
pages = {275--278},
title = {{A Comparative Survey of Economic Models for Software Product Lines}},
year = {2009}
}
@article{Kastner2014a,
abstract = {Software product line engineering is an efficient means to generate a set of tailored software products from a common implementation. However, adopting a product-line approach poses a major challenge and significant risks, since typically legacy code must be migrated toward a product line. Our aim is to lower the adoption barrier by providing semi-automatic tool support-called variability mining -to support developers in locating, documenting, and extracting implementations of product-line features from legacy code. Variability mining combines prior work on concern location, reverse engineering, and variability-aware type systems, but is tailored specifically for the use in product lines. Our work pursues three technical goals: (1) we provide a consistency indicator based on a variability-aware type system, (2) we mine features at a fine level of granularity, and (3) we exploit domain knowledge about the relationship between features when available. With a quantitative study, we demonstrate that variability mining can efficiently support developers in locating features.},
author = {Kastner, Christian and Dreiling, Alexander and Ostermann, Klaus},
doi = {10.1109/TSE.2013.45},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kastner, Dreiling, Ostermann - 2014 - Variability mining Consistent semi-automatic detection of product-line features(2).pdf:pdf},
isbn = {0098-5589 VO - 40},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {LEADT,Variability,feature,feature location,mining,reverse engineering,software product line},
number = {1},
pages = {67--82},
title = {{Variability mining: Consistent semi-automatic detection of product-line features}},
volume = {40},
year = {2014}
}
@article{Achanta2007,
author = {Achanta, Radhakrishna},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Achanta - 2007 - Using Context Information for Image Classification.pdf:pdf},
journal = {Ppt},
keywords = {context variability,software product lines,staged configurations},
number = {July},
pages = {37--51},
title = {{Using Context Information for Image Classification}},
volume = {15},
year = {2007}
}
@inproceedings{Mu2009,
abstract = {The acquisition of requirements assets are important in software product line (SPL) engineering for it help enhancing the effectiveness of reuse. Traditional methods are heavily based on manual effort. This appears to be a barrier for many organizations which tend to launch a SPL. In this paper, we propose an approach to extract functional requirements by analyzing text-based software requirements specifications (SRSs). We analyze the linguistic characterization of SRSs. According to it we define extended functional requirements framework (EFRF) which consists of 10 semantic cases, then we generate converting rules. We introduce an NLP (natural language process) approach to build EFRFs from documents based on the concept of EFRF and the converting rules. The extracted EFRFs are suitable for expression and modeling of functional requirements variability. We apply our method to an auto-marker software product line. The result shows the approach has high accuracy and efficiency, and the approach is readily scalable and extensible.},
author = {Mu, Yunhe and Wang, Yinglin and Guo, Jianmei},
booktitle = {2009 International Conference on Information and Multimedia Technology},
doi = {10.1109/ICIMT.2009.47},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Mu, Wang, Guo - 2009 - Extracting Software Functional Requirements from Free Text Documents.pdf:pdf},
isbn = {978-1-4244-5383-2},
keywords = {Functional requirement,Information extract,Product line,Rule-based,Variability model,naranja},
mendeley-tags = {naranja},
pages = {194--198},
publisher = {IEEE},
title = {{Extracting Software Functional Requirements from Free Text Documents}},
url = {http://ieeexplore.ieee.org/document/5381217/},
year = {2009}
}
@inproceedings{Niu2008,
abstract = {We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.},
author = {Niu, Nan and Easterbrook, Steve},
booktitle = {2008 12th International Software Product Line Conference},
doi = {10.1109/SPLC.2008.11},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Niu, Easterbrook - 2008 - On-Demand Cluster Analysis for Product Line Functional Requirements.pdf:pdf},
isbn = {978-0-7695-3303-2},
keywords = {DP industry,on demand cluster analysis,pattern clustering,product line functional requirements,purpura,semantic analysis,software engineering,software product line engineering,statistical analysis,systems analysis},
mendeley-tags = {purpura},
month = {sep},
pages = {87--96},
publisher = {IEEE},
title = {{On-Demand Cluster Analysis for Product Line Functional Requirements}},
url = {http://ieeexplore.ieee.org/document/4626843/},
year = {2008}
}
@inproceedings{Chen2014a,
author = {Chen, Qian and Zhu, Wenhao and Ju, Chaoyou and Zhang, Wu},
booktitle = {2014 10th International Conference on Natural Computation (ICNC)},
doi = {10.1109/ICNC.2014.6975936},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Cross domain web information extraction with multi-level feature model.pdf:pdf},
isbn = {978-1-4799-5151-2},
keywords = {Cross domain,Information extraction,Multi-level feature model,naranja},
mendeley-tags = {naranja},
month = {aug},
pages = {780--784},
publisher = {IEEE},
title = {{Cross domain web information extraction with multi-level feature model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6975936},
year = {2014}
}
@article{Jiao2005,
abstract = {It has been well recognized that product portfolio planning has far-reaching impact on the company's business success in competition. In general, product portfolio planning involves two main stages, namely portfolio identification and portfolio evaluation and selection. The former aims to capture and understand customer needs effectively and accordingly to transform them into specifications of product offerings. The latter concerns how to determine an optimal configuration of these identified offerings with the objective of achieving best profit performance. Current research and industrial practice have mainly focused on the economic justification of a given product portfolio, whereas the portfolio identification issue has been received only limited attention. This article intends to develop explicit decision support to improve product portfolio identification by efficient knowledge discovery from past sales and product records. As one of the important applications of data mining, association rule mining lends itself to the discovery of useful patterns associated with requirement analysis enacted among customers, marketing folks, and designers. An association rule mining system (ARMS) is proposed for effective product portfolio identification. Based on a scrutiny into the product definition process, the article studies the fundamental issues underlying product portfolio identification. The ARMS differentiates the customer needs from functional requirements involved in the respective customer and functional domains. Product portfolio identification entails the identification of functional requirement clusters in conjunction with the mappings from customer needs to these clusters. While clusters of functional requirements are identified based on fuzzy clustering analysis, the mapping mechanism between the customer and functional domains is incarnated in association rules. The ARMS architecture and implementation issues are discussed in detail. An application of the proposed methodology and system in a consumer electronics company to generate a vibration motor portfolio for mobile phones is also presented.},
author = {Jiao, Jianxin and Zhang, Yiyang},
doi = {10.1016/j.cad.2004.05.006},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Jiao, Zhang - 2005 - Product portfolio identification based on association rule mining.pdf:pdf},
isbn = {6567904143},
issn = {00104485},
journal = {Computer-Aided Design},
keywords = {association rules,customer satisfaction,data mining,mass customization,product,product portfolio,purpura,requirement management,rojo,variety},
mendeley-tags = {purpura,rojo},
month = {feb},
number = {2},
pages = {149--172},
title = {{Product portfolio identification based on association rule mining}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010448504001010},
volume = {37},
year = {2005}
}
@article{Nickel2015,
abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on tensor factorization methods and related latent variable models. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from theWeb. In particular, we discuss Google's Knowledge Vault project.},
author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
doi = {10.1109/JPROC.2015.2483592},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowledge Graphs.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {graph-based models,knowledge extraction,knowledge graphs,latent feature models,purpura,rojo,statistical relational},
mendeley-tags = {purpura,rojo},
month = {jan},
number = {1},
pages = {11--33},
title = {{A Review of Relational Machine Learning for Knowledge Graphs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7358050},
volume = {104},
year = {2016}
}
@article{Davril2015,
abstract = {Feature models have become one of the most widely used formalism for representing the variability among the products of a product line. The design of a feature model from a set of existing products can help stakeholders communicate on the commonalities and differences between the products, facilitate the adoption of mass customization strategies, or support the definition of the solution space of a product configurator (i.e. The sets of products that will be and will not be offered to the targeted customers). As the manual construction of feature models proves to be a time-consuming and error prone task, researchers have proposed various approaches for automatically deriving feature models from available product data. Existing reverse engineering techniques mostly rely on data mining algorithms that search for frequently occurring patterns between the features of the available product configurations. However, when the number of features is too large, the sparsity among the configurations can reduce the quality of the extracted model. In this paper, we discuss motivations for the development of dimensionality reduction techniques for product lines in order to support the extraction of feature models in the case of high-dimensional product spaces. We use a real world dataset to illustrate the problems arising with high dimensionality and present four research questions to address them.},
author = {Davril, Jean Marc and Heymans, Patrick and B{\'{e}}can, Guillaume and Acher, Mathieu},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Davril et al. - 2015 - On breaking the curse of dimensionality in reverse engineering feature models.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {naranja},
mendeley-tags = {naranja},
pages = {19--22},
title = {{On breaking the curse of dimensionality in reverse engineering feature models}},
url = {http://ceur-ws.org/Vol-1453/04{\_}DavrilHeymansBecanAcher{\_}OnBreakingTheCurseOfDimensionality{\_}Confws-15{\_}p19.pdf},
volume = {1453},
year = {2015}
}
@inproceedings{Woznica2012,
abstract = {A common problem with most of the feature selection meth- ods is that they often produce feature sets–models–that are not stable with respect to slight variations in the training data. Different authors tried to improve the feature selec- tion stability using ensemble methods which aggregate dif- ferent feature sets into a single model. However, the ex- isting ensemble feature selection methods suffer from two main shortcomings: (i) the aggregation treats the features independently and does not account for their interactions, and (ii) a single feature set is returned, nevertheless, in var- ious applications there might be more than one feature sets, potentially redundant, with similar information content. In this work we address these two limitations. We present a general framework in which we mine over different feature models produced from a given dataset in order to extract patterns over the models. We use these patterns to derive more complex feature model aggregation strategies that ac- count for feature interactions, and identify core and distinct feature models. We conduct an extensive experimental eval- uation of the proposed framework where we demonstrate its effectiveness over a number of high-dimensional problems from the fields of biology and text-mining.},
address = {New York, New York, USA},
author = {Woznica, Adam and Nguyen, Phong and Kalousis, Alexandros},
booktitle = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
doi = {10.1145/2339530.2339674},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Woznica, Nguyen, Kalousis - 2012 - Model mining for robust feature selection.pdf:pdf},
isbn = {9781450314626},
keywords = {all or part of,classification,data,feature selection,high-dimensional,model mining,naranja,or hard copies of,permission to make digital,purpura,stability,this work for},
mendeley-tags = {naranja,purpura},
pages = {913},
publisher = {ACM Press},
title = {{Model mining for robust feature selection}},
url = {http://dl.acm.org/citation.cfm?doid=2339530.2339674},
year = {2012}
}
@article{Zou2005a,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the pn case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zou, Hastie - 2005 - Regularization and variable selection via the elastic net.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection,purpura,rojo},
mendeley-tags = {purpura,rojo},
month = {apr},
number = {2},
pages = {301--320},
pmid = {20713001},
title = {{Regularization and variable selection via the elastic net}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
volume = {67},
year = {2005}
}
@article{Kastner2014,
abstract = {Software product line engineering is an efficient means to generate a set of tailored software products from a common implementation. However, adopting a product-line approach poses a major challenge and significant risks, since typically legacy code must be migrated toward a product line. Our aim is to lower the adoption barrier by providing semi-automatic tool support-called variability mining -to support developers in locating, documenting, and extracting implementations of product-line features from legacy code. Variability mining combines prior work on concern location, reverse engineering, and variability-aware type systems, but is tailored specifically for the use in product lines. Our work pursues three technical goals: (1) we provide a consistency indicator based on a variability-aware type system, (2) we mine features at a fine level of granularity, and (3) we exploit domain knowledge about the relationship between features when available. With a quantitative study, we demonstrate that variability mining can efficiently support developers in locating features.},
author = {Kastner, Christian and Dreiling, Alexander and Ostermann, Klaus},
doi = {10.1109/TSE.2013.45},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kastner, Dreiling, Ostermann - 2014 - Variability Mining Consistent Semi-automatic Detection of Product-Line Features.pdf:pdf},
isbn = {0098-5589 VO - 40},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {LEADT,Variability,feature,feature location,gris,mining,naranja,reverse engineering,software product line},
mendeley-tags = {gris,naranja},
month = {jan},
number = {1},
pages = {67--82},
title = {{Variability Mining: Consistent Semi-automatic Detection of Product-Line Features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6613490},
volume = {40},
year = {2014}
}
@article{Afzal2016,
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product.},
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
doi = {10.1016/j.csi.2016.03.003},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Afzal, Mahmood, Shaikh - 2016 - Intelligent software product line configurations A literature review(2).pdf:pdf},
issn = {09205489},
journal = {Computer Standards {\&} Interfaces},
keywords = {Artificial intelligence,Automated feature selection,Inconsistencies,Industrial SPL tools,Literature review,Predictive analytics,Software product line,purpura,rojo},
mendeley-tags = {purpura,rojo},
month = {nov},
pages = {30--48},
publisher = {Elsevier B.V.},
title = {{Intelligent software product line configurations: A literature review}},
url = {http://dx.doi.org/10.1016/j.csi.2016.03.003 http://linkinghub.elsevier.com/retrieve/pii/S0920548916300198},
volume = {48},
year = {2016}
}
@article{Thurimella2012,
abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Thurimella, Anil Kumar and Bruegge, Bernd},
doi = {10.1016/j.infsof.2012.02.005},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Thurimella, Bruegge - 2012 - Issue-based variability management.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Empirical software engineering,Product line engineering,Rationale management,Requirements engineering,naranja,rojo},
mendeley-tags = {naranja,rojo},
month = {sep},
number = {9},
pages = {933--950},
publisher = {Elsevier B.V.},
title = {{Issue-based variability management}},
url = {http://dx.doi.org/10.1016/j.infsof.2012.02.005 http://linkinghub.elsevier.com/retrieve/pii/S0950584912000481},
volume = {54},
year = {2012}
}
@article{Manco2016,
author = {Manco, Giuseppe and Rullo, Pasquale and Gallucci, Lorenzo and Paturzo, Mirko},
doi = {10.1016/j.eswa.2016.04.022},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Manco et al. - 2016 - Rialto A Knowledge Discovery suite for data analysis.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Business a,Data mining,Knowledge Discovery process,azul,gris,knowledge discovery process},
mendeley-tags = {azul,gris},
month = {oct},
pages = {145--164},
publisher = {Elsevier Ltd},
title = {{Rialto: A Knowledge Discovery suite for data analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417416301889},
volume = {59},
year = {2016}
}
@article{Benavides2010a,
abstract = {Software product line engineering is about producing a set of related products that share more commonalities than variabilities. Feature models are widely used for variability and commonality management in software product lines. Feature models are information models where a set of products are represented as a set of features in a single model. The automated analysis of feature models deals with the computer-aided extraction of information from feature models. The literature on this topic has contributed with a set of operations, techniques, tools and empirical results which have not been surveyed until now. This paper provides a comprehensive literature review on the automated analysis of feature models 20 years after of their invention. This paper contributes by bringing together previously disparate streams of work to help shed light on this thriving area. We also present a conceptual framework to understand the different proposals as well as categorise future contributions. We finally discuss the different studies and propose some challenges to be faced in the future. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Benavides, David and Segura, Sergio and Ruiz-Cort??s, Antonio},
doi = {10.1016/j.is.2010.01.001},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Benavides, Segura, Ruiz-Corts - 2010 - Automated analysis of feature models 20 years later A literature review.pdf:pdf},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Automated analyses,Feature models,Literature review,Software product lines},
number = {6},
pages = {615--636},
title = {{Automated analysis of feature models 20 years later: A literature review}},
volume = {35},
year = {2010}
}
@article{Afzal2016a,
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product.},
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
doi = {10.1016/j.csi.2016.03.003},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Afzal, Mahmood, Shaikh - 2016 - Intelligent software product line configurations A literature review.pdf:pdf},
issn = {09205489},
journal = {Computer Standards and Interfaces},
keywords = {Artificial intelligence,Automated feature selection,Inconsistencies,Industrial SPL tools,Literature review,Predictive analytics,Software product line},
pages = {30--48},
publisher = {Elsevier B.V.},
title = {{Intelligent software product line configurations: A literature review}},
url = {http://dx.doi.org/10.1016/j.csi.2016.03.003},
volume = {48},
year = {2016}
}
@article{Rabiser2010,
abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Rabiser, Rick and Gr??nbacher, Paul and Dhungana, Deepak},
doi = {10.1016/j.infsof.2009.11.001},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Rabiser, Grnbacher, Dhungana - 2010 - Requirements for product derivation support Results from a systematic literature review and an exp.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Product derivation,Product line engineering,Software product line,Systematic literature review},
number = {3},
pages = {324--346},
publisher = {Elsevier B.V.},
title = {{Requirements for product derivation support: Results from a systematic literature review and an expert survey}},
url = {http://dx.doi.org/10.1016/j.infsof.2009.11.001},
volume = {52},
year = {2010}
}
@article{Seni2010,
abstract = {Ensemble methods have been called the most influential development in Data Mining and Machine Learning in the past decade. They combine multiple models into one usually more accurate than the best of its components. Ensembles can provide a critical boost to industrial challenges -- from investment timing to drug discovery, and fraud detection to recommendation systems -- where predictive accuracy is more vital than model interpretability. Ensembles are useful with all modeling algorithms, but this book focuses on decision trees to explain them most clearly. After describing trees and their strengths and weaknesses, the authors provide an overview of regularization -- today understood to be a key reason for the superior performance of modern ensembling algorithms. The book continues with a clear description of two recent developments: Importance Sampling (IS) and Rule Ensembles (RE). IS reveals classic ensemble methods -- bagging, random forests, and boosting -- to be special cases of a single algorithm, thereby showing how to improve their accuracy and speed. REs are linear rule models derived from decision tree ensembles. They are the most interpretable version of ensembles, which is essential to applications such as credit scoring and fault diagnosis. Lastly, the authors explain the paradox of how ensembles achieve greater accuracy on new data despite their (apparently much greater) complexity. This book is aimed at novice and advanced analytic researchers and practitioners -- especially in Engineering, Statistics, and Computer Science. Those with little exposure to ensembles will learn why and how to employ this breakthrough method, and advanced practitioners will gain insight into building even more powerful models. Throughout, snippets of code in R are provided to illustrate the algorithms described and to encourage the reader to try the techniques. The authors are industry experts in data mining and machine learning who are also adjunct professors and popular speakers. Although early pioneers in discovering and using ensembles, they here distill and clarify the recent groundbreaking work of leading academics (such as Jerome Friedman) to bring the benefits of ensembles to practitioners.},
author = {Seni, Giovanni and Elder, John F.},
doi = {10.2200/S00240ED1V01Y200912DMK002},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Seni, Elder - 2010 - Ensemble Methods in Data Mining Improving Accuracy Through Combining Predictions.pdf:pdf},
isbn = {9781608452842},
issn = {2151-0067},
journal = {Synthesis Lectures on Data Mining and Knowledge Discovery},
month = {jan},
number = {1},
pages = {1--126},
title = {{Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions}},
url = {http://www.amazon.com/Ensemble-Methods-Data-Mining-Predictions-ebook/dp/B0093QMDT8/ref=sr{\_}1{\_}2?ie=UTF8{\&}qid=1398840878{\&}sr=8-2{\&}keywords=ensemble+methods http://www.morganclaypool.com/doi/abs/10.2200/S00240ED1V01Y200912DMK002},
volume = {2},
year = {2010}
}
@article{Farid2014,
abstract = {In this paper, we introduce two independent hybrid mining algorithms to improve the classification accuracy rates of decision tree (DT) and na{\"{i}}ve Bayes (NB) classifiers for the classification of multi-class problems. Both DT and NB classifiers are useful, efficient and commonly used for solving classification problems in data mining. Since the presence of noisy contradictory instances in the training set may cause the generated decision tree suffers from overfitting and its accuracy may decrease, in our first proposed hybrid DT algorithm, we employ a na{\"{i}}ve Bayes (NB) classifier to remove the noisy troublesome instances from the training set before the DT induction. Moreover, it is extremely computationally expensive for a NB classifier to compute class conditional independence for a dataset with high dimensional attributes. Thus, in the second proposed hybrid NB classifier, we employ a DT induction to select a comparatively more important subset of attributes for the production of na{\"{i}}ve assumption of class conditional independence. We tested the performances of the two proposed hybrid algorithms against those of the existing DT and NB classifiers respectively using the classification accuracy, precision, sensitivity-specificity analysis, and 10-fold cross validation on 10 real benchmark datasets from UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed methods have produced impressive results in the classification of real life challenging multi-class problems. They are also able to automatically extract the most valuable training datasets and identify the most effective attributes for the description of instances from noisy complex training databases with large dimensions of attributes. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Farid, Dewan Md and Zhang, Li and Rahman, Chowdhury Mofizur and Hossain, M.A. and Strachan, Rebecca},
doi = {10.1016/j.eswa.2013.08.089},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Farid et al. - 2014 - Hybrid decision tree and na{\"{i}}ve Bayes classifiers for multi-class classification tasks.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Data mining,Decision tree,Hybrid,Na{\"{i}}ve Bayes classifier,rojo},
mendeley-tags = {rojo},
month = {mar},
number = {4},
pages = {1937--1946},
publisher = {Elsevier Ltd},
title = {{Hybrid decision tree and na{\"{i}}ve Bayes classifiers for multi-class classification tasks}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.08.089 http://linkinghub.elsevier.com/retrieve/pii/S0957417413007100},
volume = {41},
year = {2014}
}
@article{Bocca2016,
abstract = {Crop yield models can assist decision makers within any agro-industrial supply chain, even with regard to decisions that are unrelated to the crop production. Considering the characteristics of the mechanisms and data related to yield, data mining techniques are suitable candidates for modelling. The use of these techniques within a context with feature engineering, feature selection, and proper tuning can further improve performance beyond a simple replacement of multiple linear regression. To evaluate the impact of the different steps in the mentioned context, we evaluated sugarcane (Saccharum spp.) yield modelling with data obtained from a sugarcane mill. For a combination of six techniques, tuning, feature selection, and feature engineering, leading to 66 combinations, we assessed final model performance. Average performance across combinations resulted in a mean absolute error (MAE) of 6.42Mgha−1. Using different techniques led to a range of MAE from 4.57 to 8.80Mgha−1 on average. The best and worst performances for an individual model were MAEs of 4.11 and 9.00Mgha−1. Models with lower performance were close to simply predicting yield from the average yield for each number of cuts (MAE of 9.86Mgha−1). Tuning and feature engineering reduced the MAE on average by 1.17 and 0.64Mgha−1, respectively. Feature selection removed nearly 40{\%} of the features but increased the MAE by 0.19Mgha−1. The performance of models was improved by simple strategies such as decomposing weather attributes and detailing fertilisation. Evaluation of feature importance provided by the RReliefF feature selection algorithm was used to explain the performance gains. If empirical models are needed, they will rely on using advanced techniques, but they will need proper algorithm tuning and feature engineering to extract most of the information from datasets. Based on the results, we recommend following the presented workflow for the development of yield models.},
author = {Bocca, Felipe F. and Rodrigues, Luiz Henrique Antunes},
doi = {10.1016/j.compag.2016.08.015},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bocca, Rodrigues - 2016 - The effect of tuning, feature engineering, and feature selection in data mining applied to rainfed sugarcane y.pdf:pdf},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {artificial neural networks,boosted regression trees,naranja,purpura},
mendeley-tags = {naranja,purpura},
pages = {67--76},
publisher = {Elsevier B.V.},
title = {{The effect of tuning, feature engineering, and feature selection in data mining applied to rainfed sugarcane yield modelling}},
url = {http://dx.doi.org/10.1016/j.compag.2016.08.015},
volume = {128},
year = {2016}
}
@article{Wang2012,
abstract = {In an imbalanced dataset, the positive and negative classes can be quite different in both size and distribution. This degrades the performance of many feature extraction methods and classifiers. This paper proposes a method for extracting minimum positive and maximum negative features (in terms of absolute value) for imbalanced binary classification. This paper develops two models to yield the feature extractors. Model 1 first generates a set of candidate extractors that can minimize the positive features to be zero, and then chooses the ones among these candidates that can maximize the negative features. Model 2 first generates a set of candidate extractors that can maximize the negative features, and then chooses the ones that can minimize the positive features. Compared with the traditional feature extraction methods and classifiers, the proposed models are less likely affected by the imbalance of the dataset. Experimental results show that these models can perform well when the positive class and negative class are imbalanced in both size and distribution. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Wang, Jinghua and You, Jane and Li, Qin and Xu, Yong},
doi = {10.1016/j.patcog.2011.09.004},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2012 - Extract minimum positive and maximum negative features for imbalanced binary classification.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Feature subspace extraction,Imbalanced binary classification,Maximum negative feature,Minimum positive feature,Pattern classification,purpura,rojo},
mendeley-tags = {purpura,rojo},
month = {mar},
number = {3},
pages = {1136--1145},
publisher = {Elsevier},
title = {{Extract minimum positive and maximum negative features for imbalanced binary classification}},
url = {http://dx.doi.org/10.1016/j.patcog.2011.09.004 http://linkinghub.elsevier.com/retrieve/pii/S0031320311003827},
volume = {45},
year = {2012}
}
@article{Bakar2015a,
abstract = {Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
doi = {10.1016/j.jss.2015.05.006},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bakar, Kasirun, Salleh - 2015 - Feature extraction approaches from natural language requirements for reuse in software product lines (2).pdf:pdf},
isbn = {01641212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Feature extractions,Natural language requirements,Requirements reuse,Software product lines,Systematic literature review,naranja},
mendeley-tags = {naranja},
month = {aug},
pages = {132--149},
publisher = {Elsevier Ltd.},
title = {{Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.jss.2015.05.006 http://linkinghub.elsevier.com/retrieve/pii/S0164121215001004},
volume = {106},
year = {2015}
}
@article{Chen2014,
abstract = {The Three Gorges region of central western China is one of the most landslide-prone regions in the world. However, landslide detection based on field surveys and optical remote sensing and synthetic aperture radar (SAR) techniques remains difficult owing to the dense vegetation cover and mountain shadow. In the present study, an area of Zigui County in the Three Gorges region was selected to test the feasibility of detecting landslides by employing novel features extracted from a LiDAR-derived DTM. Additionally, two small sites-Site 1 and Site 2-were selected for training and were used to classify each other. In addition to the aspect, DTM, and slope images, the following feature sets were proposed to improve the accuracy of landslide detection: (1) the mean aspect, DTM, and slope textures based on four texture directions; (2) aspect, DTM, and slope textures based on aspect; and (3) the moving average and standard deviation (stdev) filter of aspect, DTM, and slope. By combining a feature selection method and the RF algorithm, the classification accuracy was evaluated and landslide boundaries were determined. The results can be summarized as follows. (1) The feature selection method demonstrated that the proposed features provided information useful for effective landslide identification. (2) Feature selection achieved an improvement of about 0.44{\%} in the overall classification accuracy, with the feature set reduced by 74{\%}, from 39 to 10; this can speed up the training of the RF model. (3) When fifty randomly selected 20{\%} of landslide pixels (PLS) and 20{\%} of non-landslide pixels (PNLS) (i.e., 20{\%} of PLS and PNLS) were utilized in addition to the selected feature subsets for training, the test sets (i.e., the remaining 80{\%} of PLS and PNLS) yielded an average overall classification accuracy of 78.24{\%}. The cross training and classification for Site 1 and Site 2 provided overall classification accuracies of 62.65{\%} and 64.50{\%}, respectively. This shows that the random sampling design (which suffered some of the effects of spatial auto-correlation) and the proposed method in this present study contribute jointly to the classification accuracy. (4) Using the Canny operator to delineate landslide boundaries based on the classification results of PLS and PNLS, we obtained results consistent with the referenced landslide inventory maps. Thus, the proposed procedure, which combines LiDAR data, a feature selection method, and the RF algorithm, can identify forested landslides effectively in the Three Gorges region. {\textcopyright} 2014 Elsevier Inc.},
author = {Chen, Weitao and Li, Xianju and Wang, Yanxin and Chen, Gang and Liu, Shengwei},
doi = {10.1016/j.rse.2014.07.004},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Forested landslide detection using LiDAR data and the random forest algorithm A case study of the Three Gorges, Chi.pdf:pdf},
isbn = {00344257},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Feature selection,Landslide mapping,LiDAR,Random forest,The Three Gorges,Topographic analysis,purpura},
mendeley-tags = {purpura},
month = {sep},
pages = {291--301},
publisher = {Elsevier Inc.},
title = {{Forested landslide detection using LiDAR data and the random forest algorithm: A case study of the Three Gorges, China}},
url = {http://dx.doi.org/10.1016/j.rse.2014.07.004 http://linkinghub.elsevier.com/retrieve/pii/S0034425714002491},
volume = {152},
year = {2014}
}
@article{Dadaneh2016,
abstract = {Feature selection (FS) is one of the most important fields in pattern recognition, which aims to pick a subset of relevant and informative features from an original feature set. There are two kinds of FS algorithms depending on the presence of information about dataset class labels: supervised and unsupervised algorithms. Supervised approaches utilize class labels of dataset in the process of feature selection. On the other hand, unsupervised algorithms act in the absence of class labels, which makes their process more difficult. In this paper, we propose unsupervised probabilistic feature selection using ant colony optimization (UPFS). The algorithm looks for the optimal feature subset in an iterative process. In this algorithm, we utilize inter-feature information which shows the similarity between the features that leads the algorithm to decreased redundancy in the final set. In each step of the ACO algorithm, to select the next potential feature, we calculate the amount of redundancy between current feature and all those which have been selected thus far. In addition, we utilize a matrix to hold ant related pheromone which shows the rate of the co-presence of every pair of features in solutions. Afterwards, features are ranked based on a probability function extracted from the matrix; then, their m-top is returned as the final solution. We compare the performance of UPFS with 15 well-known supervised and unsupervised feature selection methods using different classifiers (support vector machine, naive Bayes, and k-nearest neighbor) on 10 well-known datasets. The experimental results show the efficiency of the proposed method compared to the previous related methods.},
author = {Dadaneh, Behrouz Zamani and Markid, Hossein Yeganeh and Zakerolhosseini, Ali},
doi = {10.1016/j.eswa.2016.01.021},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Dadaneh, Markid, Zakerolhosseini - 2016 - Unsupervised probabilistic feature selection using ant colony optimization.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Ant colony optimization,Classification accuracy,Feature selection,Filter approaches,Unsupervised methods,azul,naranja,purpura},
mendeley-tags = {azul,naranja,purpura},
month = {jul},
pages = {27--42},
publisher = {Elsevier Ltd},
title = {{Unsupervised probabilistic feature selection using ant colony optimization}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.021 http://linkinghub.elsevier.com/retrieve/pii/S0957417416000312},
volume = {53},
year = {2016}
}
@article{Sepulveda2016,
abstract = {Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46{\%} of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6{\%} of the languages have been empirically validated, 41{\%} report some kind of industry adoption and 71{\%} of the languages are independent from any development process. Last but not least, 57{\%} of the languages have been proposed by the academia, while 43{\%} have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.},
author = {Sep{\'{u}}lveda, Samuel and Cravero, Ania and Cachero, Cristina},
doi = {10.1016/j.infsof.2015.08.007},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Sep{\'{u}}lveda, Cravero, Cachero - 2016 - Requirements modeling languages for software product lines A systematic literature review.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Modeling languages,Requirements engineering,Software product lines,Systematic literature review,naranja},
mendeley-tags = {naranja},
month = {jan},
pages = {16--36},
title = {{Requirements modeling languages for software product lines: A systematic literature review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584915001494},
volume = {69},
year = {2016}
}
@article{Asadi2014,
abstract = {Context A software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model. Objective In this work we address the feature model configuration problem and propose a framework to automatically select suitable features that satisfy both the functional and non-functional preferences and constraints of stakeholders. Additionally, interdependencies between various non-functional properties are taken into account in the framework. Method The proposed framework combines Analytical Hierarchy Process (AHP) and Fuzzy Cognitive Maps (FCM) to compute the non-functional properties weights based on stakeholders' preferences and interdependencies between non-functional properties. Afterwards, Hierarchical Task Network (HTN) planning is applied to find the optimal feature model configuration. Result Our approach improves state-of-art of feature model configuration by considering positive or negative impacts of the features on non-functional properties, the stakeholders' preferences, and non-functional interdependencies. The approach presented in this paper extends earlier work presented in [1] from several distinct perspectives including mechanisms handling interdependencies between non-functional properties, proposing a novel tooling architecture, and offering visualization and interaction techniques for representing functional and non-functional aspects of feature models. Conclusion our experiments show the scalability of our configuration approach when considering both functional and non-functional requirements of stakeholders. ?? 2014 Elsevier B.V. All rights reserved.},
author = {Asadi, Mohsen and Soltani, Samaneh and Gasevic, Dragan and Hatala, Marek and Bagheri, Ebrahim},
doi = {10.1016/j.infsof.2014.03.005},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Asadi et al. - 2014 - Toward automated feature model configuration with optimizing non-functional requirements.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Feature model configuration,Non-functional interdependencies,Software product lines,Stakeholders' preferences,gris,naranja,rojo},
mendeley-tags = {gris,naranja,rojo},
month = {sep},
number = {9},
pages = {1144--1165},
publisher = {Elsevier B.V.},
title = {{Toward automated feature model configuration with optimizing non-functional requirements}},
url = {http://dx.doi.org/10.1016/j.infsof.2014.03.005 http://linkinghub.elsevier.com/retrieve/pii/S0950584914000640},
volume = {56},
year = {2014}
}
@article{Tuarob2015,
author = {Tuarob, Suppawong and Tucker, Conrad S.},
doi = {10.1115/1.4030049},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Tuarob, Tucker - 2015 - Automated Discovery of Lead Users and Latent Product Features by Mining Large Scale Social Media Networks.pdf:pdf},
issn = {1050-0472},
journal = {Journal of Mechanical Design},
keywords = {rojo},
mendeley-tags = {rojo},
month = {jul},
number = {7},
pages = {071402},
title = {{Automated Discovery of Lead Users and Latent Product Features by Mining Large Scale Social Media Networks}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4030049},
volume = {137},
year = {2015}
}
@article{Wang2014a,
abstract = {Data mining can extract useful information from large databases. This paper presents the evolution of the intellectual structure in tourism destination literature as determined by means of bibliometric and social network analysis of 17 552 citations of 414 articles published in Social Sciences Citation Index and Sciences Citation Index journals from 1955 to 2011. This study found that tourism destination research is organized into four different concentrations of interest: destination image, tourist experience and stakeholder involvement, structural equation modeling, and customer relationship management. Future tourism destination research will probably continue to focus on these topics. This study presents a new way for researchers to profile development patterns objectively and provides a key reading method for searching useful research directions. Copyright {\textcopyright} 2014 by ASTM International.},
author = {Wang, Cheng-Hua and Chen, Shiu-Chun},
doi = {10.1520/JTE20120285},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Chen - 2014 - Bibliometric and Social Network Analysis for Data Mining The Intellectual Structure of Tourism Destination Literatur.pdf:pdf},
isbn = {0090-3973},
issn = {00903973},
journal = {Journal of Testing and Evaluation},
keywords = {associate professor,bibliometric,business and operations management,data mining,graduate school of,intellectual structure,rojo,social network analysis,tourism destination},
mendeley-tags = {rojo},
month = {jan},
number = {1},
pages = {20120285},
title = {{Bibliometric and Social Network Analysis for Data Mining: The Intellectual Structure of Tourism Destination Literature}},
url = {http://www.astm.org/doiLink.cgi?JTE20120285},
volume = {42},
year = {2014}
}
@article{Wang2015,
abstract = {In this paper, we propose a data-driven network analysis based approach to predict individual choice set for customer choice modeling. Taking into account product associations and customer heterogeneity, we apply data analytics to mine existing data of customer choice set, which is then used to predict choice set for individual customers in a new choice modeling scenario. Product association network is constructed first to identify product communities based on existing data of customer choice sets, where links between products reflect the proximity or similarity of two products in customers' perceptual space. To account for customer heterogeneity, customers are classified into clusters (segments) based on their profile attributes and for each cluster the product consideration frequency is computed. For predicting choice sets, a probabilistic sampling approach is proposed integrating product associations, customer segments, and the link strengths in the product association network. In case studies, we first implement the approach using an example with simulated choice set data. The quality of predicted choice sets is examined by assessing the estimation bias of the developed choice model. We then demonstrate the proposed approach using actual survey data of vehicle choice, illustrating the impact of choice sets on the customer utility representation and the agreement between choice model and reality. From both examples, improved choice modeling results are consistently observed using the predicted choice sets, demonstrating the benefits of the proposed method for choice modeling.},
author = {Wang, Mingxian and Chen, Wei},
doi = {10.1115/1.4030160},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Chen - 2015 - A Data-Driven Network Analysis Approach to Predicting Customer Choice Sets for Choice Modeling in Engineering Design.pdf:pdf},
issn = {1050-0472},
journal = {Journal of Mechanical Design},
keywords = {analytics,choice modeling,choice set,customer preference,data,network analysis,product association,purpura},
mendeley-tags = {purpura},
month = {jul},
number = {7},
pages = {071409},
title = {{A Data-Driven Network Analysis Approach to Predicting Customer Choice Sets for Choice Modeling in Engineering Design}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4030160},
volume = {137},
year = {2015}
}
@article{Harding2006,
abstract = {The paper reviews applications of data mining in manufacturing engineering, in particu- lar production processes, operations, fault detection, maintenance, decision support, and product quality improvement. Customer relationship management, information integra- tion aspects, and standardization are also briefly discussed. This review is focused on demonstrating the relevancy of data mining to manufacturing industry, rather than dis- cussing the data mining domain in general. The volume of general data mining literature makes it difficult to gain a precise view of a target area such as manufacturing engineer- ing, which has its own particular needs and requirements for mining applications. This review reveals progressive applications in addition to existing gaps and less considered areas such as manufacturing planning and shop floor control.},
author = {Harding, J. a. and Shahbaz, M. and Srinivas and Kusiak, a.},
doi = {10.1115/1.2194554},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Harding et al. - 2006 - Data Mining in Manufacturing A Review.pdf:pdf},
issn = {10871357},
journal = {Journal of Manufacturing Science and Engineering},
number = {4},
pages = {969},
title = {{Data Mining in Manufacturing: A Review}},
volume = {128},
year = {2006}
}
@article{Bakar2015,
abstract = {Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
doi = {10.1016/j.jss.2015.05.006},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bakar, Kasirun, Salleh - 2015 - Feature extraction approaches from natural language requirements for reuse in software product lines A s.pdf:pdf},
isbn = {01641212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Feature extractions,Natural language requirements,Requirements reuse,Software product lines,Systematic literature review},
pages = {132--149},
publisher = {Elsevier Ltd.},
title = {{Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.jss.2015.05.006},
volume = {106},
year = {2015}
}
@book{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Tobergte, Curtis - 2013 - No Title No Title.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Bouguila2010,
abstract = {Discrete data are an important component in many image processing and computer vision applications. In this work we propose an unsupervised statistical approach to learn structures of this kind of data. The central ingredient in our model is the introduction of the generalized Dirichlet distribution as a prior to the multinomial. An estimation algorithm, based on leave-one-out likelihood and empirical Bayesian inference, for the parameters is developed. This estimation algorithm can be viewed as a hybrid expectation-maximization (EM) which alternates EM iterations with Newton-Raphson iterations using the Hessian matrix. We propose then the use of our model as a parametric basis for support vector machines within a hybrid generative/discriminative framework. In a series of experiments involving scene modeling and classification using visual words, and color texture modeling we show the efficiency of the proposed approach. ?? 2010 Elsevier Inc. All rights reserved.},
author = {Bouguila, Nizar and Ghimire, Mukti Nath},
doi = {10.1016/j.jvcir.2010.04.001},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bouguila, Ghimire - 2010 - Discrete visual features modeling via leave-one-out likelihood estimation and applications.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Dirichlet,Discrete features,Finite mixture models,Generalized Dirichlet,Generative/discriminative,Leave-one-out likelihood,Multinomial,SVM,Scene classification,Visual words},
number = {7},
pages = {613--626},
publisher = {Elsevier Inc.},
title = {{Discrete visual features modeling via leave-one-out likelihood estimation and applications}},
url = {http://dx.doi.org/10.1016/j.jvcir.2010.04.001},
volume = {21},
year = {2010}
}
@article{Chung2013,
abstract = {Health Informatics is emerging as a promising research area. As average life expectancy increases due to medical technology development, health issues remain most sensitive agenda in most of countries in the world. However, heal th technology requires more intelligent mechanisms by which users' requirement for more accurate prediction about their health problems can be ful- filled. Furthermore, such intelligent mechanisms must provide very flexible and robust procedures by which complicated but necessary decision support functions are allowed. In this sense, this paper proposes General Bayesian Network (GBN) to predict appropriate diets and restaurants that would benefit users' health. We compared the performance of GBN with other competing techniques such as NBN (naive Bayesian Network), TAN (Tree Augmented naive Bayesian Network), and decision tree. Expe r- iments with real health dataset revealed that GBN results outperform other techniques with statistical validity.},
author = {Chung, Dahee and Lee, Kun Chang and Seong, Seung Chang},
doi = {10.1016/j.sbspro.2013.06.461},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Chung, Lee, Seong - 2013 - General Bayesian Network Approach to Health Informatics Prediction Emphasis on Performance Comparison.pdf:pdf},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {General Bayesian Network,Health Informatics,Na{\"{i}}ve Bayesian Network,Tree-Augmented NBN},
pages = {465--468},
publisher = {Elsevier B.V.},
title = {{General Bayesian Network Approach to Health Informatics Prediction: Emphasis on Performance Comparison}},
url = {http://www.sciencedirect.com/science/article/pii/S1877042813015280},
volume = {81},
year = {2013}
}
@article{Apte1997,
author = {Apte, C and Weiss, S M and Apte, Chidanand and Weiss, Sholom},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Apte et al. - 1997 - Data Mining with Decision Trees and Decision Rules.pdf:pdf},
journal = {Computer Systems},
keywords = {data mining,decision tree,rule induction},
number = {November},
pages = {197--210},
title = {{Data Mining with Decision Trees and Decision Rules}},
volume = {13},
year = {1997}
}
@article{Ristoski2016,
abstract = {Data Mining and Knowledge Discovery in Databases (KDD) is a research field concerned with deriving higher-level insights from data. The tasks performed in that field are knowledge intensive and can often benefit from using additional knowledge from various sources. Therefore, many approaches have been proposed in this area that combine Semantic Web data with the data mining and knowledge discovery process. This survey article gives a comprehensive overview of those approaches in different stages of the knowledge discovery process. As an example, we show how Linked Open Data can be used at various stages for building content-based recommender systems. The survey shows that, while there are numerous interesting research works performed, the full potential of the Semantic Web and Linked Open Data for data mining and KDD is still to be unlocked.},
author = {Ristoski, Petar and Paulheim, Heiko},
doi = {10.1016/j.websem.2016.01.001},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ristoski, Paulheim - 2016 - Semantic Web in data mining and knowledge discovery A comprehensive survey.pdf:pdf},
issn = {15708268},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
keywords = {Data mining,Knowledge discovery,Linked Open Data,Semantic Web},
pages = {1--22},
publisher = {Elsevier B.V.},
title = {{Semantic Web in data mining and knowledge discovery: A comprehensive survey}},
url = {http://www.sciencedirect.com/science/article/pii/S1570826816000020},
volume = {36},
year = {2016}
}
@article{Song2002,
author = {Song, Shuang and Dong, Andy and Agogino, Alice},
doi = {10.1115/1.1528921},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Song, Dong, Agogino - 2002 - Modeling Information Needs in Engineering Databases Using Tacit Knowledge.pdf:pdf},
issn = {15309827},
journal = {Journal of Computing and Information Science in Engineering},
keywords = {purpura},
mendeley-tags = {purpura},
number = {3},
pages = {199},
title = {{Modeling Information Needs in Engineering Databases Using Tacit Knowledge}},
url = {http://computingengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1399467},
volume = {2},
year = {2002}
}
@article{Tucker2008,
abstract = {This paper addresses two important fundamental areas in product family formulation that have recently begun to receive great attention. First is the incorporation of market demand that we address through a data mining approach where realistic customer preference data are translated into performance design targets. Second is product architecture reconfiguration that we model as a dynamic design entity. The dynamic approach to product architecture optimization differs from conventional static approaches in that a product architecture is not fixed at the initial stage of product design, but rather evolves with fluctuations in customer performance preferences. The benefits of direct customer input in product family design will be realized through the cell phone product family example presented in this work. An optimal family of cell phones is created with modularity decisions made analytically at the engineering level that maximize company profit. Copyright {\textcopyright} 2008 by ASME.},
author = {Tucker, Conrad S. and Kim, Harrison M.},
doi = {10.1115/1.2838336},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Tucker, Kim - 2008 - Optimal Product Portfolio Formulation by Merging Predictive Data Mining With Multilevel Optimization.pdf:pdf},
isbn = {1563478234},
issn = {10500472},
journal = {Journal of Mechanical Design},
keywords = {rojo},
mendeley-tags = {rojo},
number = {4},
pages = {041103},
title = {{Optimal Product Portfolio Formulation by Merging Predictive Data Mining With Multilevel Optimization}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1449682},
volume = {130},
year = {2008}
}
@article{Hamraz2012,
abstract = {Engineering change (EC) is a source of uncertainty. While the number of changes to a design can be optimized, their existence cannot be eliminated. Each change is accompanied by intended and unintended impacts both of which might propagate and cause further knock-on changes. Such change propagation causes uncertainty in design time, cost, and quality and thus needs to be predicted and controlled. Current engineering change propagation models map the product connectivity into a single-domain network and model change propagation as spread within this network. Those models miss out most dependencies from other domains and suffer from {\^{a}}€{\oe}hidden dependencies{\^{a}}€ . This paper proposes the function-behavior-structure (FBS) linkage model, a multidomain model which combines concepts of both the function-behavior-structure model from Gero and colleagues with the change prediction method (CPM) from Clarkson and colleagues. The FBS linkage model is represented in a network and a corresponding multidomain matrix of structural, behavioral, and functional elements and their links. Change propagation is described as spread in that network using principles of graph theory. The model is applied to a diesel engine. The results show that the FBS linkage model is promising and improves current methods in several ways: The model (1) accounts explicitly for all possible dependencies between product elements, (2) allows capturing and modeling of all relevant change requests, (3) improves the understanding of why and how changes propagate, (4) is scalable to different levels of decomposition, and (5) is flexible to present the results on different levels of abstraction. All these features of the FBS linkage model can help control and counteract change propagation and reduce uncertainty and risk in design.},
author = {Hamraz, Bahram and Caldwell, Nicholas H. M. and {John Clarkson}, P.},
doi = {10.1115/1.4007397},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hamraz, Caldwell, John Clarkson - 2012 - A Multidomain Engineering Change Propagation Model to Support Uncertainty Reduction and Risk Ma.pdf:pdf},
isbn = {1050-0472},
issn = {10500472},
journal = {Journal of Mechanical Design},
keywords = {change propagation,design structure matrix,engineering change management,functional reasoning,gris,mechanical design,rojo},
mendeley-tags = {gris,rojo},
number = {10},
pages = {100905},
title = {{A Multidomain Engineering Change Propagation Model to Support Uncertainty Reduction and Risk Management in Design}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1484826},
volume = {134},
year = {2012}
}
@article{Ndjodo2010,
author = {Ndjodo, Marcel Fouda and Ngoumou, Amougou},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ndjodo, Ngoumou - 2010 - Product Lines ' Feature-Oriented Engineering for Reuse A Formal Approach.pdf:pdf},
keywords = {feature-,product line engineering,rojo},
mendeley-tags = {rojo},
number = {5},
title = {{Product Lines ' Feature-Oriented Engineering for Reuse : A Formal Approach}},
volume = {7},
year = {2010}
}
@article{Miotto2016,
author = {Miotto, Riccardo and Li, Li and Kidd, Brian A. and Dudley, Joel T.},
doi = {10.1038/srep26094},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Miotto et al. - 2016 - Deep Patient An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
keywords = {rojo},
mendeley-tags = {rojo},
month = {may},
number = {April},
pages = {26094},
pmid = {27185194},
title = {{Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records}},
url = {http://www.nature.com/articles/srep26094},
volume = {6},
year = {2016}
}
@article{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ghahramani - 2015 - Probabilistic machine learning and artificial intelligence.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
keywords = {naranja},
mendeley-tags = {naranja},
month = {may},
number = {7553},
pages = {452--459},
pmid = {26017444},
title = {{Probabilistic machine learning and artificial intelligence}},
url = {http://dx.doi.org/10.1038/nature14541 http://www.nature.com/doifinder/10.1038/nature14541},
volume = {521},
year = {2015}
}
@article{GIRALDO2014,
abstract = {Feature Models (FMs) are a notation to represent differences and commonalities between products derived from a product line. However, product line modelers could unintentionally incorporate dead features in FMs. A dead feature is a type of defect, which implies that one or more features are not present in any product of the product line. Some authors have used ontologies in product lines, but they have not exploited ontology reasoning to identify and explain causes for defects in FMs in natural language. In this paper, we propose an ontology that represents FMs in OWL (Web Ontology Language). Then, we use SQWRL (Semantic Query-enhanced Web Rule Language) to identify dead features in a FM and identify and explain certain causes of this defect in natural language. Our preliminary empirical evaluation confirms the benefits of our approach.},
author = {GIRALDO, GLORIA LUCIA and RINC{\'{O}}N-PEREZ, LUISA and MAZO, RAUL},
doi = {10.15446/dyna.v81n183.36348},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/GIRALDO, RINC{\'{O}}N-PEREZ, MAZO - 2014 - IDENTIFYING DEAD FEATURES AND THEIR CAUSES IN PRODUCT LINE MODELS AN ONTOLOGICAL APPROACH.pdf:pdf},
issn = {2346-2183},
journal = {DYNA},
keywords = {Product lines,SQWRL.,dead features,feature models,ontologies,rojo},
mendeley-tags = {rojo},
month = {jan},
number = {183},
pages = {68},
title = {{IDENTIFYING DEAD FEATURES AND THEIR CAUSES IN PRODUCT LINE MODELS: AN ONTOLOGICAL APPROACH}},
url = {http://dyna.medellin.unal.edu.co/en/verResumenEN.php?id{\_}articulo=v81n183a08 http://www.revistas.unal.edu.co/index.php/dyna/article/view/36348},
volume = {81},
year = {2014}
}
@article{White2008,
abstract = {Product-line architectures (PLAs) are an effective mechanism for facilitating the reuse of software components on different mobile devices. Mobile applications are typically delivered to devices using over-the-air provisioning services that allow a mobile phone to download and install software over a cellular network connection. Current techniques for automating product-line variant selection do not address the unique requirements (such as the need to consider resource constraints) of dynamically selecting a variant for over- the-air provisioning. This paper presents the following contributions to product-line variant selection for mobile devices: (1) it describes how a constraint solver can be used to dynamically select a product-line variant while adhering to resource constraints, (2) it presents architectures for automatically discovering device capabilities mapping them to product-line feature models, (3) and it includes results from experiments and field tests with an automated variant selector, and (4) it describes PLA design rules that can be used to increase the performance of automated constraint-based variant selection. Our empirical results show that fast au- tomated variant selection from a feature model is possible if certain product-line design guidelines are followed.},
author = {White, Jules and Schmidt, DouglasC. C and Wuchner, Egon and Nechypurenko, Andrey},
doi = {10.1007/BF03192550},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/White et al. - 2008 - Automatically composing reusable software components for mobile devices.pdf:pdf},
isbn = {01046500 (ISSN)},
issn = {0104-6500},
journal = {Journal of the Brazilian Computer Society},
keywords = {Constraint Satisfaction,constraint,feature modeling,product-lines,rojo,satisfaction,software reuse},
mendeley-tags = {rojo},
month = {mar},
number = {1},
pages = {25--44},
title = {{Automatically composing reusable software components for mobile devices}},
url = {http://dx.doi.org/10.1007/BF03192550 http://link.springer.com/10.1007/BF03192550},
volume = {14},
year = {2008}
}
@inproceedings{Weyns2014,
address = {New York, New York, USA},
author = {Weyns, Danny},
booktitle = {Proceedings of the 18th International Software Product Line Conference on Companion Volume for Workshops, Demonstrations and Tools - SPLC '14},
doi = {10.1145/2647908.2655959},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Weyns - 2014 - Variability.pdf:pdf},
isbn = {9781450327398},
keywords = {naranja},
mendeley-tags = {naranja},
pages = {12--12},
publisher = {ACM Press},
title = {{Variability}},
url = {http://dl.acm.org/citation.cfm?doid=2647908.2655959},
year = {2014}
}
@inproceedings{Zhou2015b,
abstract = {A feature model is able to identify commonality and variability within a product line, helping stakeholders configure product variants and seize opportunities for reuse. However, no direct customer preference information is incorporated in the feature model when it comes to the question-how many product variants are needed in order to satisfy individual customer needs. This paper proposes to mine customer preference information for individual product features by sentiment analysis of online product reviews. The features commented by the users of a product are used to augment a simple feature model predefined with customer opinionated preference information. In such a way, the customer preference information is considered as one attribute of the features in the model, helping designers make informed decisions when trading off between commonality and variability of a product line. Finally, we present a Kindle Fire tablet case study to demonstrate the proposed method.},
author = {Zhou, F. and Jiao, R. J.},
booktitle = {2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
doi = {10.1109/IEEM.2015.7385935},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Jiao - 2015 - Feature model augmentation with sentiment analysis for product line planning.pdf:pdf},
isbn = {978-1-4673-8066-9},
keywords = {Analytical models,Consumer electronics,Electronic publishing,Feature extraction,Feature model,Fires,High definition video,Kindle fire tablet case study,Sentiment analysis,commonality,customer opinionated preference information,customer preference,customer preference information,customer services,feature model augmentation,individual customer need,individual product feature,naranja,online product review,product line planning,product variant,production planning,rojo,sentiment analysis,variability},
mendeley-tags = {naranja,rojo},
month = {dec},
pages = {1689--1693},
publisher = {IEEE},
title = {{Feature model augmentation with sentiment analysis for product line planning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7385935},
year = {2015}
}
@inproceedings{Farias2016,
address = {New York, New York, USA},
author = {{de F. Farias}, M{\'{a}}rio Andr{\'{e}} and Novais, Renato and J{\'{u}}nior, Methanias Cola{\c{c}}o and {da Silva Carvalho}, Lu{\'{i}}s Paulo and Mendon{\c{c}}a, Manoel and Sp{\'{i}}nola, Rodrigo Oliveira},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing - SAC '16},
doi = {10.1145/2851613.2851786},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/de F. Farias et al. - 2016 - A systematic mapping study on mining software repositories.pdf:pdf},
isbn = {9781450337397},
keywords = {Systematic mapping study,empirical,empirical software engineering,gris,mining software repository,secondary study,software engineering,systematic mapping study},
mendeley-tags = {gris},
pages = {1472--1479},
publisher = {ACM Press},
title = {{A systematic mapping study on mining software repositories}},
url = {http://dl.acm.org/citation.cfm?doid=2851613.2851786},
year = {2016}
}
@inproceedings{Valov2015,
address = {New York, New York, USA},
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791069},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Valov, Guo, Czarnecki - 2015 - Empirical comparison of regression methods for variability-aware performance prediction.pdf:pdf},
isbn = {9781450336130},
keywords = {purpura},
mendeley-tags = {purpura},
pages = {186--190},
publisher = {ACM Press},
title = {{Empirical comparison of regression methods for variability-aware performance prediction}},
url = {http://dl.acm.org/citation.cfm?id=2791060.2791069 http://dl.acm.org/citation.cfm?doid=2791060.2791069},
year = {2015}
}
@article{Utomo2013,
author = {Utomo, Chandra Prasetyo},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Utomo - 2013 - The Hybrid of Classification Tree and Extreme Learning Machine for Permeability Prediction in Oil Reservoir.pdf:pdf},
journal = {International Journal of Computer Science Issues},
keywords = {classification tree,extreme learning machine,hybrid intelligent systems,oil reservoir,permeability prediction},
number = {1},
pages = {52--60},
title = {{The Hybrid of Classification Tree and Extreme Learning Machine for Permeability Prediction in Oil Reservoir}},
volume = {10},
year = {2013}
}
@article{Li2008,
abstract = {When engineering content is created and applied during the product life cycle, it is often stored and forgotten. Since search remains word based, engineers do not have the effective means to harness and reuse past designs and experiences. Current information retrieval approaches based on statistical methods and keyword matching do not satisfy users needs in the engineering domain. Therefore, we propose a new computational framework that includes an ontological basis and algorithms to retrieve unstructured engineering documents while handling complex queries. The results from the preliminary test demonstrate that our method outperforms the traditional keyword-based search with respect to the standard information retrieval measurement.},
author = {Li, Zhanjun and Raskin, Victor and Ramani, Karthik},
doi = {10.1115/1.2830851},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Li, Raskin, Ramani - 2008 - Developing Engineering Ontology for Information Retrieval.pdf:pdf},
isbn = {15309827 (ISSN)},
issn = {15309827},
journal = {Journal of Computing and Information Science in Engineering},
number = {1},
pages = {011003},
pmid = {22149783},
title = {{Developing Engineering Ontology for Information Retrieval}},
url = {http://link.aip.org/link/JCISB6/v8/i1/p011003/s1{\&}Agg=doi},
volume = {8},
year = {2008}
}
@book{Rashid2011,
abstract = {Software product lines provide a systematic means of managing variability in a suite of products. They have many benefits but there are three major barriers that can prevent them from reaching their full potential. First, there is the challenge of scale: a large number of variants may exist in a product line context and the number of interrelationships and dependencies can rise exponentially. Second, variations tend to be systemic by nature in that they affect the whole architecture of the software product line. Third, software product lines often serve different business contexts, each with its own intricacies and complexities. The AMPLE (http://www.ample-project.net/) approach tackles these three challenges by combining advances in aspect-oriented software development and model-driven engineering. The full suite of methods and tools that constitute this approach are discussed in detail in this edited volume and illustrated using three real-world industrial case studies. {\textcopyright} Cambridge University Press 2011.},
author = {Rashid, A and Royer, J.-C. and Rummler, A},
booktitle = {Aspect-Oriented, Model-Driven Software Product Lines: The AMPLE Way},
doi = {10.1017/CBO9781139003629},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Rashid, Royer, Rummler - 2011 - Aspect-oriented, model-driven software product lines The AMPLE way.pdf:pdf},
isbn = {9780521767224},
pages = {1--464},
title = {{Aspect-oriented, model-driven software product lines: The AMPLE way}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84923405380{\&}partnerID=40{\&}md5=f8ae074bdfd181d3b7889ce2bfd266bd},
year = {2011}
}
@article{Wang2009,
abstract = {Wang et al. (Softw. Pract. Exper. 2007; 37(7):727–745) observed a phenomenon of performance inconsistency in the graphics of Java Abstract Window Toolkit (AWT)/Swing among different Java runtime environments (JREs) on Windows XP. This phenomenon makes it difficult to predict the performance of Java game applications. Therefore, they proposed a portable AWT/Swing architecture, called CYC Window Toolkit (CWT), to provide programmers with high and consistent rendering performance for Java game development among different JREs. They implemented a DirectX version to demonstrate the feasibility of the architecture. This paper extends the above research to other environments in two aspects. First, we evaluate the rendering performance of the original Java AWT with different combinations of JREs, image application programming interfaces, system properties and operating systems (OSs), including Windows XP, Windows Vista, Fedora and Mac OS X. The evaluation results indicate that the performance inconsistency of Java AWT also exists among the four OSs, even if the same hardware configuration is used. Second, we design an OpenGL version of CWT, named CWT-GL, to take advantage of modern 3D graphics cards, and compare the rendering performance of CWT with Java AWT/Swing. The results show that CWT-GL achieves more consistent and higher rendering performance in JREs 1.4 to 1.6 on the four OSs. The results also hint at two approaches: (a) decouple the rendering pipelines of Java AWT/Swing from the JREs for faster upgrading and supporting old JREs and (b) use other graphics libraries, such as CWT, instead of Java AWT/Swing to develop cross-platform Java games with higher and more consistent rendering performance. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
archivePrefix = {arXiv},
arxivId = {1008.1900},
author = {Wang, Yi Hsien and Wu, I. Chen},
doi = {10.1002/spe},
eprint = {1008.1900},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Wu - 2009 - Achieving high and consistent rendering performance of java AWTSwing on multiple platforms.pdf:pdf},
isbn = {0000000000000},
issn = {00380644},
journal = {Software - Practice and Experience},
keywords = {CYC Window Toolkit,Directx,Linux,Mac OS x,OpenGL,Windows},
number = {7},
pages = {701--736},
pmid = {20926156},
title = {{Achieving high and consistent rendering performance of java AWT/Swing on multiple platforms}},
volume = {39},
year = {2009}
}
@article{Barbantan2016,
abstract = {Purpose - Improving healthcare services by developing assistive technologies includes both the health aid devices and the analysis of the data collected by them. The acquired data modeled as a knowledge base give more insight into each patient's health status and needs. Therefore, the ultimate goal of a health-care system is obtaining recommendations provided by an assistive decision support system using such knowledge base, benefiting the patients, the physicians and the healthcare industry. This paper aims to define the knowledge flow for a medical assistive decision support system by structuring raw medical data and leveraging the knowledge contained in the data proposing solutions for efficient data search, medical investigation or diagnosis and medication prediction and relationship identification. Design/methodology/approach - The solution this paper proposes for implementing a medical assistive decision support system can analyze any type of unstructured medical documents which are processed by applying Natural Language Processing (NLP) tasks followed by semantic analysis, leading to the medical concept identification, thus imposing a structure on the input documents. The structured information is filtered and classified such that custom decisions regarding patients' health status can be made. The current research focuses on identifying the relationships between medical concepts as defined by the REMed (Relation Extraction from Medical documents) solution that aims at finding the patterns that lead to the classification of concept pairs into concept-to-concept relations. Findings - This paper proposed the REMed solution expressed as a multi-class classification problem tackled using the support vector machine classifier. Experimentally, this paper determined the most appropriate setup for the multi-class classification problem which is a combination of lexical, context, syntactic and grammatical features, as each feature category is good at representing particular relations, but not all. The best results we obtained are expressed as F1-measure of 74.9 per cent which is 1.4 per cent better than the results reported by similar systems. Research limitations/implications - The difficulty to discriminate between TrIP and TrAP relations revolves around the hierarchical relationship between the two classes as TrIP is a particular type (an instance) of TrAP. The intuition behind this behavior was that the classifier cannot discern the correct relations because of the bias toward the majority classes. The analysis was conducted by using only sentences from electronic health record that contain at least two medical concepts. This limitation was introduced by the availability of the annotated data with reported results, as relations were defined at sentence level. Originality/value - The originality of the proposed solution lies in the methodology to extract valuable information from the medical records via semantic searches; concept-to-concept relation identification; and recommendations for diagnosis, treatment and further investigations. The REMed solution introduces a learning-based approach for the automatic discovery of relations between medical concepts. We propose an original list of features: lexical - 3, context - 6, grammatical - 4 and syntactic - 4. The similarity feature introduced in this paper has a significant influence on the classification, and, to the best of the authors' knowledge, it has not been used as feature in similar solutions.},
author = {Barbantan, Ioana and Porumb, Mihaela and Lemnaru, Camelia and Potolea, Rodica},
doi = {10.1108/IJWIS-03-2016-0015},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Barbantan et al. - 2016 - Feature Engineered Relation Extraction – Medical Documents Setting.pdf:pdf},
issn = {1744-0084},
journal = {International Journal of Web Information Systems},
month = {aug},
number = {3},
pages = {336--358},
title = {{Feature Engineered Relation Extraction – Medical Documents Setting}},
url = {http://www.emeraldinsight.com/doi/10.1108/IJWIS-03-2016-0015},
volume = {12},
year = {2016}
}
@article{Pedram2015,
abstract = {The software has become a modern asset and competitive product. The product line that has long been used in manufacturing and construction industries nowadays has attracted a lot of attention in software industry. Most importance of product line engineering approach is in cost and time issues involved in marketing. Feature model is one of the most important methods of documenting variability in product line that shows product features and their dependencies. Because of the magnitude and complexity of the product line, build and maintain feature models are complex and time-consuming work. In this article feature model importance and position in product line is discussed and feature model extraction methods are reviewed and compared.},
author = {Pedram, Saba and Mohsenzadeh, Mehran and Ahrabi, Amir Azimi Alasti},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pedram, Mohsenzadeh, Ahrabi - 2015 - A Review of Feature Model Position in the Software Product Line and Its Extraction Methods.pdf:pdf},
journal = {International Journal of Computer Science and Security (ICSS)},
keywords = {extraction method review,feature model,software product line},
number = {5},
pages = {274--279},
title = {{A Review of Feature Model Position in the Software Product Line and Its Extraction Methods}},
volume = {9},
year = {2015}
}
@inproceedings{Zaid2009,
abstract = {Feature models are models used to capture differences and commonalities between software features, enabling the representation of variability within software. There are many variations of feature models and different notations are often used to represent the same information. Currently support for validating or integrating feature models is missing. In this paper, we provide an ontology framework for feature modeling which consists of an ontology that formally provides a specification for feature models. In addition, we provide means to integrate segmented feature models and provide a rule based model consistency check and conflict detection. We use SWRL rules to implement the rules and a DL reasoner to evaluate the rules and infer extra interesting information regarding the variability of the software.},
address = {New York, New York, USA},
author = {Zaid, Lamia Abo and Kleinermann, Frederic and {De Troyer}, Olga},
booktitle = {Proceedings of the 2009 ACM symposium on Applied Computing - SAC '09},
doi = {10.1145/1529282.1529563},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zaid, Kleinermann, De Troyer - 2009 - Applying semantic web technology to feature modeling.pdf:pdf},
isbn = {9781605581668},
keywords = {Feature models,OWL,naranja,ontologies,software variabil},
mendeley-tags = {naranja},
pages = {1252},
publisher = {ACM Press},
title = {{Applying semantic web technology to feature modeling}},
url = {http://portal.acm.org/citation.cfm?doid=1529282.1529563},
year = {2009}
}
@inproceedings{Santos2015,
address = {New York, New York, USA},
author = {Santos, Alcemir Rodrigues and de Oliveira, Raphael Pereira and de Almeida, Eduardo Santana},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering - EASE '15},
doi = {10.1145/2745802.2745806},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Santos, de Oliveira, de Almeida - 2015 - Strategies for consistency checking on software product lines.pdf:pdf},
isbn = {9781450333504},
keywords = {consistency checking,literature review,mapping study,naranja,software product line engineering},
mendeley-tags = {naranja},
number = {Section 3},
pages = {1--14},
publisher = {ACM Press},
title = {{Strategies for consistency checking on software product lines}},
url = {http://dl.acm.org/citation.cfm?doid=2745802.2745806},
year = {2015}
}
@inproceedings{Rothberg2016,
address = {New York, New York, USA},
author = {Rothberg, Valentin and Dintzner, Nicolas and Ziegler, Andreas and Lohmann, Daniel},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866624},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Rothberg et al. - 2016 - Feature Models in Linux.pdf:pdf},
isbn = {9781450340199},
keywords = {CADOS,Configurability,Feature Models,Kconfig,Linux,naranja,rojo},
mendeley-tags = {naranja,rojo},
pages = {65--72},
publisher = {ACM Press},
title = {{Feature Models in Linux}},
url = {http://doi.acm.org/10.1145/2866614.2866624 http://dl.acm.org/citation.cfm?doid=2866614.2866624},
year = {2016}
}
@inproceedings{Seidl2013,
abstract = {Software product lines (SPLs) and software ecosystems (SECOs) are approaches to capturing families of closely related software systems in terms of common and variable functionality. SPLs and especially SECOs are subject to evolution to adapt to new or changed requirements resulting in different versions of the software family and its variable assets. These versions may have to be maintained and used for products even after they were superseded by newer versions. Variability models describing valid combinations of variable assets, such as feature models, capture variability in space (configuration), but not variability in time (evolution) making it impossible to respect versions of variable assets in product definitions on a conceptual level. In this paper, we propose Hyper Feature Models (HFMs) explicitly providing feature versions as configurable units for product definition. Furthermore, we provide a version-aware constraint language to specify dependencies between features and ranges of feature versions as well as a procedure to automatically select valid combinations of versions for a pre-configuration of features. We demonstrate our approach in a case study. {\textcopyright} 2014 ACM.},
address = {New York, New York, USA},
author = {Seidl, Christoph and Schaefer, Ina and A{\ss}mann, Uwe},
booktitle = {Proceedings of the Eighth International Workshop on Variability Modelling of Software-Intensive Systems - VaMoS '14},
doi = {10.1145/2556624.2556625},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Seidl, Schaefer, A{\ss}mann - 2013 - Capturing variability in space and time with hyper feature models.pdf:pdf},
isbn = {9781450325561},
keywords = {evolution,gris,hfm,hyper feature model,seco,software ecosystem,software product line,spl,variability in time},
mendeley-tags = {gris},
pages = {1--8},
publisher = {ACM Press},
title = {{Capturing variability in space and time with hyper feature models}},
url = {http://dl.acm.org/citation.cfm?doid=2556624.2556625},
year = {2013}
}
@article{Reuling2015,
abstract = {Testing every member of a product line individually is of-ten impracticable due to large number of possible product configurations. Thus, feature models are frequently used to generate samples, i.e., subsets of product configurations under test. Besides the extensively studied combinatorial interaction testing (CIT) approach for coverage-driven sam-ple generation, only few approaches exist so far adopting mutation testing to emulate faults in feature models to be detected by a sample. In this paper, we present a mutation-based sampling framework for fault-based product-line test-ing. We define a comprehensive catalog of atomic mutation operators on the graphical representation of feature models. This way, we are able (1) to also define complex mutation operators emulating more subtle faults, and (2) to classify operators semantically, e.g., to avoid redundant and equiv-alent mutants. We further introduce similarity-based mu-tant selection and higher order mutation strategies to reduce testing efforts. Our implementation is based on the graph transformation engine Henshin and is evaluated concerning effectiveness/efficiency trade-offs.},
author = {Reuling, Dennis and B{\"{u}}rdek, Johannes and Rot{\"{a}}rmel, Serge and Lochau, Malte and Kelter, Udo},
doi = {10.1145/2791060.2791074},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Reuling et al. - 2015 - Fault-based product-line testing effective sample generation based on feature-diagram mutation.pdf:pdf},
isbn = {9781450336130},
journal = {Proceedings of the 19th International Conference on Software Product Line},
keywords = {combinatorial interaction testing,gris,mutation testing},
mendeley-tags = {gris},
pages = {131--140},
title = {{Fault-based product-line testing: effective sample generation based on feature-diagram mutation}},
year = {2015}
}
@inproceedings{Nieke2016,
address = {New York, New York, USA},
author = {Nieke, Michael and Seidl, Christoph and Schuster, Sven},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866625},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Nieke, Seidl, Schuster - 2016 - Guaranteeing Configuration Validity in Evolving Software Product Lines.pdf:pdf},
isbn = {9781450340199},
keywords = {Configuration,Evolution,Software Product Line (SPL),Temporal Feature Model (TFM),gris},
mendeley-tags = {gris},
pages = {73--80},
publisher = {ACM Press},
title = {{Guaranteeing Configuration Validity in Evolving Software Product Lines}},
url = {http://dl.acm.org/citation.cfm?id=2866614.2866625 http://dl.acm.org/citation.cfm?doid=2866614.2866625},
year = {2016}
}
@article{Nadi2015,
author = {Nadi, Sarah and Kr{\"{u}}ger, Stefan},
doi = {10.1145/2866614.2866629},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Nadi, Kr{\"{u}}ger - 2015 - Variability Modeling of Cryptographic Components Clafer Experience Report.pdf:pdf},
isbn = {978-1-4503-4019-9},
journal = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems},
keywords = {Clafer,Cryptography,Variability Modeling,purpura,rojo},
mendeley-tags = {purpura,rojo},
pages = {105--112},
title = {{Variability Modeling of Cryptographic Components: Clafer Experience Report}},
url = {http://doi.acm.org/10.1145/2866614.2866629},
year = {2015}
}
@inproceedings{Mazo2015,
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
address = {New York, New York, USA},
author = {Mazo, Ra{\'{u}}l and Mu{\~{n}}oz-Fern{\'{a}}ndez, Juan C and Rinc{\'{o}}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791103},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Mazo et al. - 2015 - VariaMos.pdf:pdf},
isbn = {9781450336130},
keywords = {2,3,4,and constraint,constraints,dopler,dynamic product line,goals,models,ovm,product line engineering,purpura,simulation,tool,variability},
mendeley-tags = {purpura},
pages = {374--379},
publisher = {ACM Press},
title = {{VariaMos}},
url = {http://dl.acm.org/citation.cfm?doid=2791060.2791103},
year = {2015}
}
@inproceedings{Janota2013,
address = {New York, New York, USA},
author = {Janota, Mikol{\'{a}}{\v{s}} and Botterweck, Goetz and Marques-Silva, Joao},
booktitle = {Proceedings of the Eighth International Workshop on Variability Modelling of Software-Intensive Systems - VaMoS '14},
doi = {10.1145/2556624.2556644},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Janota, Botterweck, Marques-Silva - 2013 - On lazy and eager interactive reconfiguration.pdf:pdf},
isbn = {9781450325561},
keywords = {interactive configuration,minimal correction sets,naranja,purpura,sat},
mendeley-tags = {naranja,purpura},
pages = {1--8},
publisher = {ACM Press},
title = {{On lazy and eager interactive reconfiguration}},
url = {http://dl.acm.org/citation.cfm?doid=2556624.2556644},
year = {2013}
}
@article{Pokorny2015,
abstract = {Now we have a number of database technologies called usually NoSQL, like key-value, column-oriented, and document stores as well as search engines and graph databases. Whereas SQL software vendors offer advanced products with the capability to handle highly complex queries and transactions, NoSQL databases share rather characteristics concerning scaling and performance, as e.g. auto-sharding, distributed query support, and integrated caching. Their drawbacks can be a lack of schema or data consistency, difficulty in testing and maintaining, and absence of a higher query language. Complex data modelling and the SQL language as the only access tool to data are missing here. On the other hand, last studies show that both SQL and NoSQL databases have value for both for transactional and analytical Big Data. Top databases providers offer rearchitected database technologies combining row data stores with columnar in-memory compression enabling processing large data sets and analytical querying, often over massive, continuous data streams. The technological progress led to development of massively parallel processing analytic databases. The paper presents some details of current database technologies, their pros and cons in different application environments, and emerging trends in this area.},
author = {Pokorn{\'{y}}, Jaroslav},
doi = {10.1145/2812428.2812429},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pokorn{\'{y}} - 2015 - Database technologies in the world of big data.pdf:pdf},
isbn = {9781450333573},
journal = {Proceedings of the 16th International Conference on Computer Systems and Technologies - CompSysTech '15},
keywords = {big analytics,big data,data distribution,database technologies,newsql databases,nosql databases,transaction processing},
pages = {1--12},
title = {{Database technologies in the world of big data}},
url = {http://dl.acm.org/citation.cfm?doid=2812428.2812429},
year = {2015}
}
@article{Schmid2011,
abstract = {It has been shown that product line engineering can significantly improve the productivity, quality and time-to-market of software development by leveraging extensive reuse. Variability models are currently the most advanced approach to define, document and manage the commonalities and variabilities of reusable artifacts such as software components, requirements, test cases, etc. These models provide the basis for automating the derivation of new products and are thus the key artifact to leverage the flexibility and adaptability of systems in a product line. Among the existing approaches to variability modeling feature modeling and decision modeling have gained most importance. A significant amount of research exists on comparing and analyzing different feature modeling approaches. However, despite their significant role in product line research and practical applications, only little effort has been devoted to compare and analyze decision modeling approaches. In order to address this shortcoming and to provide a basis for more structured research on decision modeling in the future, we present a comparative analysis of representative approaches. We identify their major modeling concepts and present an analysis of their commonalities and variabilities.},
author = {Schmid, Klaus and Rabiser, Rick and Gr{\"{u}}nbacher, Paul},
doi = {10.1145/1944892.1944907},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Schmid, Rabiser, Gr{\"{u}}nbacher - 2011 - A comparison of decision modeling approaches in product lines.pdf:pdf},
isbn = {9781450305709},
issn = {9781450305709},
journal = {Proceedings of the 5th Workshop on Variability Modeling of Software-Intensive Systems - VaMoS '11},
keywords = {Product lines,comparison,decision models,survey.,variability modeling},
pages = {119--126},
title = {{A comparison of decision modeling approaches in product lines}},
url = {http://portal.acm.org/citation.cfm?doid=1944892.1944907},
year = {2011}
}
@article{Robinson2010,
abstract = {Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework.We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues. {\textcopyright} 2010 ACM.},
author = {Robinson, William N. and Ding, Yi},
doi = {10.1145/1842713.1842717},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Robinson, Ding - 2010 - A survey of customization support in agent-based business process simulation tools.pdf:pdf},
isbn = {1049-3301},
issn = {10493301},
journal = {ACM Transactions on Modeling and Computer Simulation},
number = {3},
pages = {1--29},
title = {{A survey of customization support in agent-based business process simulation tools}},
url = {http://portal.acm.org/citation.cfm?doid=1842713.1842717},
volume = {20},
year = {2010}
}
@article{Quinton2015,
author = {Quinton, Cl{\'{e}}ment and Rabiser, Rick and Vierhauser, Michael and Gr{\"{u}}nbacher, Paul and Baresi, Luciano},
doi = {10.1145/2791060.2791101},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Quinton et al. - 2015 - Evolution in dynamic software product lines challenges and perspectives.pdf:pdf},
isbn = {978-1-4503-3613-0},
journal = {Proceedings of the 19th International Conference on Software Product Line Pages - SPLC'15},
keywords = {consistency,dynamic software product lines,evolution},
pages = {126--130},
title = {{Evolution in dynamic software product lines: challenges and perspectives}},
url = {http://dl.acm.org/citation.cfm?id=2791060.2791101},
year = {2015}
}
@inproceedings{London2015,
abstract = {In this study, we discuss the possible application of the ubiquitous complex network approach for information extraction from educational data. Since a huge amount of data (which is detailed as well) is produced by the complex administration systems of educational institutes, instead of the classical statistical methods, new types of data processing techniques are required to handle it. We define several suitable network representations of students, teachers and subjects in public education and present some possible ways of how graph mining techniques can be used to get detailed information about them. Depending on the construction of the underlying graph, we examine several network models and discuss which are the most appropriate graph mining tools (like community detection and ranking and centrality measures) that can be applied on them. Lastly, we attempt to highlight the many advantages of using graph-based data mining in educational data against the classical evaluation techniques.},
address = {New York, New York, USA},
author = {London, Andr{\'{a}}s and Pelyhe, {\'{A}}ron and Holl{\'{o}}, Csaba and N{\'{e}}meth, Tam{\'{a}}s},
booktitle = {Proceedings of the 16th International Conference on Computer Systems and Technologies - CompSysTech '15},
doi = {10.1145/2812428.2812436},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/London et al. - 2015 - Applying graph-based data mining concepts to the educational sphere.pdf:pdf},
isbn = {9781450333573},
keywords = {complex networks,data mining,educational evaluation,graph mining,mathematical modeling,purpura},
mendeley-tags = {purpura},
pages = {358--365},
publisher = {ACM Press},
title = {{Applying graph-based data mining concepts to the educational sphere}},
url = {http://doi.acm.org/10.1145/2812428.2812436 http://dl.acm.org/citation.cfm?doid=2812428.2812436},
year = {2015}
}
@inproceedings{M.Harman2014,
abstract = {This paper1 presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that ap- ply computational search techniques to problems in software product line engineering. Having surveyed the recent explo- sion in SBSE for SPL research activity, we highlight some di- rections for future work. We focus on suggestions for the de- velopment of recent advances in genetic improvement, show- ing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new prod- ucts with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
address = {New York, New York, USA},
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
booktitle = {Proceedings of the 18th International Software Product Line Conference on - SPLC '14},
doi = {10.1145/2648511.2648513},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Harman et al. - 2014 - Search based software engineering for software product line engineering.pdf:pdf},
isbn = {9781450327404},
issn = {9781450327404 (ISBN)},
keywords = {genetic programming,program synthesis,rojo,sbse,spl},
mendeley-tags = {rojo},
pages = {5--18},
publisher = {ACM Press},
title = {{Search based software engineering for software product line engineering}},
url = {http://doi.acm.org/10.1145/2648511.2648513 http://dl.acm.org/citation.cfm?doid=2648511.2648513},
volume = {1},
year = {2014}
}
@article{Fenske2015,
abstract = {Highly-configurable software systems (also called software product lines) gain momentum in both, academia and industry. For instance, the Linux kernel comes with over 12 000 configuration options and thus, can be customized to run on nearly every kind of system. To a large degree, this configurability is achieved through variable code structures, for instance, using conditional compilation. Such source code variability adds a new dimension of complexity, thus giving rise to new possibilities for design flaws. Code smells are an established concept to describe design flaws or decay in source code. However, existing smells have no notion of variability and thus do not support flaws regarding variable code structures. In this paper, we propose an initial catalog of four variability-aware code smells. We discuss the appearance and negative effects of these smells and present code examples from real-world systems. To evaluate our catalog, we have conducted a survey amongst 15 researchers from the field of software product lines. The results confirm that our proposed smells (a) have been observed in existing product lines and (b) are considered to be problematic for common software development activities, such as program comprehension, maintenance, and evolution.},
author = {Fenske, Wolfram and Schulze, Sandro},
doi = {10.1145/2701319.2701321},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Fenske, Schulze - 2015 - Code Smells Revisited A Variability Perspective.pdf:pdf},
isbn = {978-1-4503-3273-6},
journal = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems},
keywords = {Code Smells,Design Defects,Software Product Lines,Variability,naranja},
mendeley-tags = {naranja},
pages = {3:3----3:10},
title = {{Code Smells Revisited: A Variability Perspective}},
url = {http://doi.acm.org/10.1145/2701319.2701321},
year = {2015}
}
@article{Felfernig2015a,
abstract = {Knowledge about formal (semantic) interpretations of natural language domain descriptions is crucial for avoiding communication overheads between domain experts and knowledge engineers. In this paper we report preliminary results of an empirical study in which we investigated in which way natural language statements are formalized by knowledge engineers. We summarize the findings of our study and discuss aspects to be taken into account in order to avoid additional (often not needed) iterations in configuration knowledge engineering processes. This work is exploratory and intended to figure out open issues for future work.},
author = {Felfernig, Alexander and Reiterer, Stefan and Stettinger, Martin and Tiihonen, Juha},
doi = {10.1145/2701319.2701327},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Felfernig et al. - 2015 - Towards Understanding Cognitive Aspects of Configuration Knowledge Formalization.pdf:pdf},
isbn = {9781450332736},
journal = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '15},
keywords = {Cognitive Aspects,Empirical Studies,Knowledge Engineering,Variability Modeling,verde},
mendeley-tags = {verde},
pages = {117--123},
title = {{Towards Understanding Cognitive Aspects of Configuration Knowledge Formalization}},
url = {http://dl.acm.org/citation.cfm?id=2701319.2701327},
year = {2015}
}
@inproceedings{TerBeek2015,
address = {New York, New York, USA},
author = {ter Beek, M. H. and Legay, A. and Lafuente, A. Lluch and Vandin, A.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791087},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/ter Beek et al. - 2015 - Statistical analysis of probabilistic models of software product lines with quantitative constraints.pdf:pdf},
isbn = {9781450336130},
keywords = {gris},
mendeley-tags = {gris},
pages = {11--15},
publisher = {ACM Press},
title = {{Statistical analysis of probabilistic models of software product lines with quantitative constraints}},
url = {http://dl.acm.org/citation.cfm?doid=2791060.2791087},
year = {2015}
}
@article{EdwardsM.2015,
author = {{Edwards M.}, Rashid A Rayson P},
doi = {10.1145/2811403},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Edwards M. - 2015 - A systematic survey of online data mining technology intended for law enforcement.pdf:pdf},
issn = {03600300 (ISSN)},
journal = {ACM Computing Surveys},
keywords = {naranja},
mendeley-tags = {naranja},
number = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84945398245{\&}partnerID=40{\&}md5=328a99120df9ccad49817e34c6c79c2b},
title = {{A systematic survey of online data mining technology intended for law enforcement}},
volume = {48,1, 15,,},
year = {2015}
}
@article{Felfernig2015,
abstract = {Automated testing and debugging of knowledge bases (such as configuration knowledge bases and feature models) is an important contribution to manage knowledge evolution efficiently. However, existing approaches rely on the assumption of consistent test suites which are always kept up-to-date within the scope of different knowledge base maintenance cycles. In this paper we introduce diagnosis techniques that actively guide stakeholders (knowledge engineers and domain experts) in the process of testing and debugging knowledge bases. These techniques take into account faulty test cases and constraints and recommend diagnoses which are the source of a given inconsistency.},
author = {Felfernig, Alexander and Reiterer, Stefan and Stettinger, Martin and Tiihonen, Juha},
doi = {10.1145/2701319.2701320},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Felfernig et al. - 2015 - Intelligent Techniques for Configuration Knowledge Evolution.pdf:pdf},
isbn = {9781450332736},
journal = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '15},
keywords = {Automated Debugging,Configuration,Feature Models,purpura,rojo},
mendeley-tags = {purpura,rojo},
pages = {51--58},
title = {{Intelligent Techniques for Configuration Knowledge Evolution}},
url = {http://dl.acm.org/citation.cfm?id=2701319.2701320},
year = {2015}
}
@inproceedings{Guillaume2015,
abstract = {Feature modeling is a widely used formalism to characterize a set of products (also called configurations). $\backslash$r$\backslash$nAs a manual elaboration is a long and arduous task, numerous techniques have been proposed to reverse engineer feature models from various kinds of artefacts. But none of them synthesize feature attributes (or constraints over attributes) despite the practical relevance of attributes for documenting the different values across a range of products.$\backslash$r$\backslash$nIn this report, we develop an algorithm for synthesizing attributed feature models given a set of product descriptions. $\backslash$r$\backslash$nWe present sound, complete, and parametrizable techniques for computing all possible hierarchies, feature groups, placements of feature attributes, domain values, and constraints. $\backslash$r$\backslash$nWe perform a complexity analysis w.r.t. number of features, attributes, configurations, and domain size. We also evaluate the scalability of our synthesis procedure using randomized configuration matrices. $\backslash$r$\backslash$nThis report is a first step that aims to describe the foundations for synthesizing attributed feature models.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1502.04645},
author = {B{\'{e}}can, Guillaume and Behjati, Razieh and Gotlieb, Arnaud and Acher, Mathieu},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791068},
eprint = {1502.04645},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/B{\'{e}}can et al. - 2015 - Synthesis of attributed feature models from product descriptions.pdf:pdf},
isbn = {9781450336130},
keywords = {attributed feature models,gris,naranja,personal,product descriptions},
mendeley-tags = {gris,naranja,personal},
pages = {1--10},
publisher = {ACM Press},
title = {{Synthesis of attributed feature models from product descriptions}},
url = {http://dl.acm.org/citation.cfm?doid=2791060.2791068},
year = {2015}
}
@inproceedings{Capilla2015,
address = {New York, New York, USA},
author = {Capilla, Rafael and Hinchey, Mike and D{\'{i}}az, Francisco J.},
booktitle = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '15},
doi = {10.1145/2701319.2701322},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Capilla, Hinchey, D{\'{i}}az - 2015 - Collaborative Context Features for Critical Systems.pdf:pdf},
isbn = {9781450332736},
keywords = {Feature modeling,adaptation,context features,context-aware systems,rojo,runtime},
mendeley-tags = {rojo},
pages = {43--50},
publisher = {ACM Press},
title = {{Collaborative Context Features for Critical Systems}},
url = {http://dl.acm.org/citation.cfm?id=2701319.2701322 http://dl.acm.org/citation.cfm?doid=2701319.2701322},
year = {2015}
}
@inproceedings{Capilla2014,
address = {New York, New York, USA},
author = {Capilla, Rafael},
booktitle = {Proceedings of the 18th International Software Product Line Conference on Companion Volume for Workshops, Demonstrations and Tools - SPLC '14},
doi = {10.1145/2647908.2655960},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Capilla - 2014 - From feature modeling to context variability modeling.pdf:pdf},
isbn = {9781450327398},
keywords = {azul},
mendeley-tags = {azul},
pages = {13--13},
publisher = {ACM Press},
title = {{From feature modeling to context variability modeling}},
url = {http://dl.acm.org/citation.cfm?doid=2647908.2655960},
year = {2014}
}
@inproceedings{Simmonds2016,
address = {New York, New York, USA},
author = {Blum, Fabian Rojas and Simmonds, Jocelyn and Bastarrica, Mar{\'{i}}a Cecilia},
booktitle = {Proceedings of the 2015 International Conference on Software and System Process - ICSSP 2015},
doi = {10.1145/2785592.2785605},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Blum, Simmonds, Bastarrica - 2015 - Software process line discovery.pdf:pdf},
isbn = {9781450333467},
keywords = {naranja,noise,process discovery,software process lines,variability},
mendeley-tags = {naranja},
number = {August 2015},
pages = {127--136},
publisher = {ACM Press},
title = {{Software process line discovery}},
url = {http://dl.acm.org/citation.cfm?doid=2785592.2785605},
year = {2015}
}
@inproceedings{Bezerra2016,
address = {New York, New York, USA},
author = {Bezerra, Carla I M and Monteiro, Jos{\'{e}} Maria and Andrade, Rossana M C and Rocha, Lincoln S},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866617},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bezerra et al. - 2016 - Analyzing the Feature Models Maintainability over their Evolution Process.pdf:pdf},
isbn = {9781450340199},
keywords = {Evolution,Feature Model,Maintainability,naranja,purpura},
mendeley-tags = {naranja,purpura},
pages = {17--24},
publisher = {ACM Press},
title = {{Analyzing the Feature Models Maintainability over their Evolution Process}},
url = {http://doi.acm.org/10.1145/2866614.2866617 http://dl.acm.org/citation.cfm?doid=2866614.2866617},
year = {2016}
}
@inproceedings{Berger2013,
abstract = {Over more than two decades, numerous variability modeling techniques have been introduced in academia and industry. However, little is known about the actual use of these techniques. While dozens of experience reports on software product line engineering exist, only very few focus on variability modeling. This lack of empirical data threatens the validity of existing techniques, and hinders their improvement. As part of our effort to improve empirical understanding of variability modeling, we present the results of a survey questionnaire distributed to industrial practitioners. These results provide insights into application scenarios and perceived benefits of variability modeling, the notations and tools used, the scale of industrial models, and experienced challenges and mitigation strategies.},
address = {New York, New York, USA},
author = {Berger, Thorsten and Rublack, Ralf and Nair, Divya and Atlee, Joanne M. and Becker, Martin and Czarnecki, Krzysztof and W{\c{a}}sowski, Andrzej},
booktitle = {Proceedings of the Seventh International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '13},
doi = {10.1145/2430502.2430513},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Berger et al. - 2013 - A survey of variability modeling in industrial practice.pdf:pdf},
isbn = {9781450315418},
issn = {00985589},
keywords = {naranja,rojo},
mendeley-tags = {naranja,rojo},
pages = {1},
publisher = {ACM Press},
title = {{A survey of variability modeling in industrial practice}},
url = {http://dl.acm.org/citation.cfm?doid=2430502.2430513},
year = {2013}
}
@article{Barn,
author = {Barn, Balbir S and Clark, Tony and Ali, Almaas and Arif, Rabia},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Barn et al. - Unknown - A Systematic Mapping Study of the Current Practice of Indian Software Engineering.pdf:pdf},
isbn = {9781450340182},
keywords = {indian it industry,isec,systematic mapping study},
pages = {89--98},
title = {{A Systematic Mapping Study of the Current Practice of Indian Software Engineering}}
}
@article{Castelluccia2014,
author = {Castelluccia, Daniela and Boffoli, Nicola},
doi = {10.1145/2579281.2579294},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Castelluccia, Boffoli - 2014 - Service-oriented product lines.pdf:pdf},
isbn = {978-3-642-36583-6},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
number = {2},
pages = {1--6},
title = {{Service-oriented product lines}},
url = {http://dl.acm.org/citation.cfm?doid=2579281.2579294},
volume = {39},
year = {2014}
}
@article{Kitchenham2015,
abstract = {This keynote discusses the need for more robust statistical methods. For visualizing data I suggest using Kernel density plots rather than box plots. For parametric analysis, I propose more robust measures of central location such as trimmed means, which can support reliable tests of the differences between the central location of two or more samples. In addition, I also recommend non-parametric effect sizes such as Cliff's $\delta$ and Brunner and Munzel's p-hat that avoid some of the problems with rank-based non-parametric methods.},
author = {Kitchenham, Barbara},
doi = {10.1145/2745802.2747956},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kitchenham - 2015 - Robust statistical methods.pdf:pdf},
isbn = {9781450333504},
journal = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering - EASE '15},
keywords = {cliff,kernel density,p -hat,robust statistics,s $\delta$,trimmed means},
pages = {1--6},
title = {{Robust statistical methods}},
url = {http://dl.acm.org/citation.cfm?doid=2745802.2747956},
year = {2015}
}
@inproceedings{Thum2016,
address = {New York, New York, USA},
author = {Th{\"{u}}m, Thomas and Winkelmann, Tim and Schr{\"{o}}ter, Reimar and Hentschel, Martin and Kr{\"{u}}ger, Stefan},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866628},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Th{\"{u}}m et al. - 2016 - Variability Hiding in Contracts for Dependent Software Product Lines.pdf:pdf},
isbn = {9781450340199},
keywords = {Multi product line,deductive verification,method contracts,rojo},
mendeley-tags = {rojo},
pages = {97--104},
publisher = {ACM Press},
title = {{Variability Hiding in Contracts for Dependent Software Product Lines}},
url = {http://doi.acm.org/10.1145/2866614.2866628 http://dl.acm.org/citation.cfm?doid=2866614.2866628},
year = {2016}
}
@article{Thum2014,
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
author = {Th{\"{u}}m, Thomas and Apel, Sven and K{\"{a}}stner, Christian and Schaefer, Ina and Saake, Gunter},
doi = {10.1145/2580950},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Th{\"{u}}m et al. - 2014 - A Classification and Survey of Analysis Strategies for Software Product Lines.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {verde},
mendeley-tags = {verde},
month = {jun},
number = {1},
pages = {1--45},
title = {{A Classification and Survey of Analysis Strategies for Software Product Lines}},
url = {http://dl.acm.org/citation.cfm?doid=2620784.2580950},
volume = {47},
year = {2014}
}
@inproceedings{Yamany2015,
author = {Yamany, Ahmed Eid El and Elgamel, Mohamed Shaheen},
booktitle = {The 7th International Conference on Information Technology},
doi = {10.15849/icit.2015.0114},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Yamany, Elgamel - 2015 - Smart OptiSelect Preference Based Innovative Framework for User-in-the-Loop Feature Selection in Software Produ.pdf:pdf},
isbn = {9789957858339},
keywords = {feature models,gris,machine learning,multi-objective optimization,non-dominant solutions,optimal feature selection,pareto front,search-based software engineering,software product lines,uil,user-in-the-loop},
mendeley-tags = {gris},
month = {may},
pages = {657--666},
publisher = {Al-Zaytoonah University of Jordan},
title = {{Smart OptiSelect Preference Based Innovative Framework for User-in-the-Loop Feature Selection in Software Product Lines}},
url = {http://icit.zuj.edu.jo/icit15/DOI/Software{\_}Engineering/0114.pdf},
volume = {2015},
year = {2015}
}
@article{Lee2014,
author = {Lee, Jaejoon and Kang, Kyo C. and Sawyer, Pete and Lee, Hyesun},
doi = {10.1007/s00766-013-0183-6},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2014 - A holistic approach to feature modeling for product line requirements engineering.pdf:pdf},
issn = {0947-3602},
journal = {Requirements Engineering},
keywords = {Feature modeling,Feature modeling viewpoints,Feature space,Goal modeling,Product line requirements engineering,azul,naranja},
mendeley-tags = {azul,naranja},
month = {nov},
number = {4},
pages = {377--395},
title = {{A holistic approach to feature modeling for product line requirements engineering}},
url = {http://link.springer.com/10.1007/s00766-013-0183-6},
volume = {19},
year = {2014}
}
@article{Eleuterio2015,
abstract = {Software Product Lines (SPLs) are emerging techniques where several artifacts are reused (domain), and some are customized (variation points). An SPL can bind variation points statically (compilation time) or dynamically (runtime). Dynamic Software Product Lines (DSPLs) us e dynamic bind ing to adapt to the environment or requirements changes. DSPLs are commonly used to build dependable systems, defined as systems with the ability to avoid service failures more frequent or severe than is acceptable. Main dependability attribu tes are availability, confidentiality, integrity, reliability, maintainability, and safety. To better understand this context, a Systematic Mapping Study (SMS) was applied searching proposals that include dependability attributes in DSPLs. Our results sugg est that few studies handle dependability in DSPL context. We selected only nine primary studies in this context. Also, four of them provide explicit support for SOA and were analyzed as a secondary result},
author = {Eleut{\'{e}}rio, Jane D A S and Gaia, Felipe N and Rodrigues, Gena{\'{i}}na N and Rubira, Cec{\'{i}}lia M F},
doi = {10.13140/RG.2.1.2717.4880},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Eleut{\'{e}}rio et al. - 2015 - Dependable Dynamic Software Product Line--a Systematic Mapping Study.pdf:pdf},
keywords = {purpura},
mendeley-tags = {purpura},
number = {April},
title = {{Dependable Dynamic Software Product Line--a Systematic Mapping Study}},
year = {2015}
}
@book{BoughzalaI.;OhO.;Reiter-Palmon2013,
address = {Berlin, Heidelberg},
author = {{Boughzala, I.; Oh, O.; Reiter-Palmon}, R.},
booktitle = {Collaboration and Technology},
doi = {10.1007/978-3-642-41347-6},
editor = {Antunes, Pedro and Gerosa, Marco Aur{\'{e}}lio and Sylvester, Allan and Vassileva, Julita and de Vreede, Gert-Jan},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Boughzala, I. Oh, O. Reiter-Palmon - 2013 - Collaboration and Technology.pdf:pdf},
isbn = {978-3-642-41346-9},
keywords = {azul},
mendeley-tags = {azul},
number = {November},
pages = {94--109},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Collaboration and Technology}},
url = {http://link.springer.com/10.1007/978-3-642-41347-6},
volume = {8224},
year = {2013}
}
@incollection{Antonio2009,
abstract = {Feature modeling is an important technique to capture commonalities and variabilities in a software product line ({\{}SPL).{\}} However, this kind of models shows a specific perspective, which is not sufficient to express all the characteristics and constraints of an {\{}SPL.{\}} Using a goal-oriented approach, such as i*, to complement (and help define) feature models would improve such models enhancing meaning and justification to features. Goal-oriented modelling provides a way to identify variabilities at an early phase of requirements, allowing alternative options to satisfy stakeholder's goals. The aim of this work is to benefit software product lines from the framework i*, a more expressive approach to requirements engineering of {\{}SPLs.{\}}},
author = {Ant{\'{o}}nio, Sandra and Ara{\'{u}}jo, Jo{\~{a}}o and Silva, Carla},
booktitle = {Proceedings of the {\{}ER{\}} 2009 Workshops ({\{}CoMoL{\}}, {\{}ETheCoM{\}}, {\{}FP-UML{\}}, {\{}MOST-ONISW{\}}, {\{}QoIS{\}}, {\{}RIGiM{\}}, {\{}SeCoGIS){\}} on Advances in Conceptual Modeling - Challenging Perspectives},
doi = {10.1007/978-3-642-04947-7_34},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ant{\'{o}}nio, Ara{\'{u}}jo, Silva - 2009 - Adapting the i Framework for Software Product Lines.pdf:pdf},
isbn = {978-3-642-04946-0},
keywords = {Goal-oriented approach,feature model,i* Framework,requirements engineering,rojo,software product line},
mendeley-tags = {rojo},
pages = {286--295},
title = {{Adapting the i* Framework for Software Product Lines}},
url = {http://dx.doi.org/10.1007/978-3-642-04947-7{\_}34 http://link.springer.com/10.1007/978-3-642-04947-7{\_}34},
year = {2009}
}
@article{Wang2014,
abstract = {In the context of product lines, test case selection aims at obtaining a set of relevant test cases for a product from the entire set of test cases available for a product line. While working on a research-based innovation project on automated testing of product lines of Video Conferencing Systems (VCSs) developed by Cisco, we felt the need to devise a cost-effective way of selecting relevant test cases for a product. To fulfill such need, we propose a systematic and automated test selection methodology using: 1) Feature Model for Testing (FM{\_}T) to capture commonalities and variabilities of a product line; 2) Component Family Model for Testing (CFM{\_}T) to model the structure of test case repository; 3) A tool to automatically build restrictions from CFM{\_}T to FM{\_}T and traces from CFM{\_}T to the actual test cases. Using our methodology, a test engineer is only required to select relevant features through FM{\_}T at a higher level of abstraction for a product and the corresponding test cases will be obtained automatically. We evaluate our methodology by applying it to a VCS product line called Saturn with seven commercial products and the results show that our methodology can significantly reduce cost measured as test selection time and at the same time achieves higher effectiveness (feature coverage, feature pairwise coverage and fault detection) as compared with the current manual process. Moreover, we conduct a questionnaire-based study to solicit the views of test engineers who are involved in developing FM{\_}T and CFM{\_}T. The results show that test engineers are positive about adapting our methodology in their current practice. Finally, we present a set of lessons learnt while applying product line engineering at Cisco for test case selection.},
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
doi = {10.1007/s10664-014-9345-5},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2016 - A systematic test case selection methodology for product lines results and insights from an industrial case study.pdf:pdf},
isbn = {13823256 (ISSN)},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Component family model,Feature model,Product line,Test case selection,gris,personal,rojo},
mendeley-tags = {gris,personal,rojo},
month = {aug},
number = {4},
pages = {1586--1622},
title = {{A systematic test case selection methodology for product lines: results and insights from an industrial case study}},
url = {http://link.springer.com/10.1007/s10664-014-9345-5},
volume = {21},
year = {2016}
}
@article{Cary1960,
author = {Cary, Edmond and Jumpelt, R.W.},
doi = {10.1075/babel.6.1.03pro},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Cary, Jumpelt - 1960 - Proceedings of the International Congress on Translation.pdf:pdf},
isbn = {1851667482},
issn = {05219744},
journal = {Babel},
keywords = {degree of usability eclipse,featureide,software product line engineering},
number = {1},
pages = {9--9},
title = {{Proceedings of the International Congress on Translation}},
volume = {6},
year = {1960}
}
@book{Sartori2011,
abstract = {This research presents a brief review of the different definitions of Web 2.0 and presents the most important Web 2.0 Technologies that underlie the evolution of the Web. We map these Web 2.0 technologies to the Social Computing Principles and describe the different relations and patterns that occur. We argue that creating insight into the relations between Web 2.0 Technologies and Principles will help enable the creation of more successful services and accommodate a better understanding of Web 2.0 and its social aspects.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sartori, F. and Manouselis, N. and Sicilia, M. {\'{A}}.},
booktitle = {Information Systems, E-Learning, and Knowledge Management Research2},
doi = {10.1007/978-3-642-17641-8},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Sartori, Manouselis, Sicilia - 2011 - Communications in Computer and Information Science.pdf:pdf},
isbn = {9783642102394},
issn = {18650929},
pages = {16--21},
pmid = {15003161},
title = {{Communications in Computer and Information Science}},
year = {2011}
}
@article{PerezLamancha2010,
abstract = {This article presents a systematic review of the literature about Testing in Software Product Lines. The objective is to analyze the existing approaches to testing in software product lines, discussing the significant issues related to this area of knowledge and providing an up-to-date state of the art, which can serve as a basis for innovative research activities. The paper includes an analysis on how SPL research can contribute to dynamize the research in software testing.},
author = {{P{\'{e}}rez Lamancha}, Beatriz and Polo, Macario and Piattini, Mario},
doi = {10.1007/978-3-642-29578-2},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/P{\'{e}}rez Lamancha, Polo, Piattini - 2010 - Systematic Review on Software Product Line Testing.pdf:pdf},
isbn = {978-3-642-29577-5},
issn = {18650929},
journal = {5th International Conference, ICSOFT 2010, Athens, Greece, July 22-24, 2010. Revised Selected Papers},
keywords = {Software product lines,Software testing,Survey,Systematic review,Test},
pages = {pp. 58--71},
title = {{Systematic Review on Software Product Line Testing}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-29578-2},
volume = {170},
year = {2010}
}
@article{Saake2011,
author = {Saake, Gunter and Eisenecker, Ulrich and A{\ss}mann, U},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Saake, Eisenecker, A{\ss}mann - 2011 - Towards Flexible Feature Composition Static and Dynamic Binding in Software Product Lines.pdf:pdf},
journal = {Phd Thesis},
keywords = {ards flexible feature composition,in software product lines,static and dynamic binding},
title = {{Towards Flexible Feature Composition: Static and Dynamic Binding in Software Product Lines}},
url = {http://wwwiti.cs.uni-magdeburg.de/{~}rosenmue/publications/DissRosenmueller.pdf},
year = {2011}
}
@article{Anwar2014,
abstract = {Software Engineering (SE) is a new field compared to other sciences. The term Software Engineering first appeared in late 1950s. SE from its beginning has been continuously in the process of evolution. New approaches, methods, tools and techniques are introduced frequently. The future of Software Engineering is a hot topic and every year many publications discuss the same. The focus of this paper is to explore subareas of SE, predict the possible future of SE and provide a guide to practitioners to choose their careers according to the evolution of SE.},
author = {Anwar, Zeeshan and Bibi, Nazia and Ahsan, Ali},
doi = {10.1145/2579281.2579291},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Anwar, Bibi, Ahsan - 2014 - The Future of Software Engineering A Survey.pdf:pdf},
issn = {0163-5948},
journal = {SIGSOFT Softw. Eng. Notes},
keywords = {Artificial Intelligence,Cloud computing,reverse engineering,scalable spiral process model,software engineering,web development},
number = {2},
pages = {1--3},
title = {{The Future of Software Engineering: A Survey}},
url = {http://doi.acm.org/10.1145/2579281.2579291},
volume = {39},
year = {2014}
}
@article{Astromskis2015,
abstract = {Characterizing how users interact with software has many applications. For example, to understand which features are used, in which sequence operations are performed, etc. can help to understand how the user interface could be improved, to identify missing features, or to identify scenarios which are good candidates for test cases. This paper presents an industrial case study in which we investigate how users interact with an enterprise resource planning software using process mining. Our case study illustrates how we identify user interaction processes, the encountered advantages, and the faced challenges. One of the major findings is that the decision how to group events into cases is crucial for the application of the method.},
author = {Astromskis, Saulius and Janes, Andrea and Mairegger, Michael},
doi = {10.1145/2785592.2785612},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Astromskis, Janes, Mairegger - 2015 - A Process Mining Approach to Measure How Users Interact with Software An Industrial Case Study.pdf:pdf},
isbn = {9781450333467},
journal = {ICSSP 2015 Proceedings of the 2015 International Conference on Software and System Process},
keywords = {process mining,user interaction analysis},
pages = {137--141},
title = {{A Process Mining Approach to Measure How Users Interact with Software : An Industrial Case Study}},
url = {http://dl.acm.org/citation.cfm?id=2785612},
year = {2015}
}
@article{GrupodelBancoMundial2016,
author = {{Grupo del Banco Mundial}},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Grupo del Banco Mundial - 2016 - Indicadores del desarrollo mundial Banco de datos mundial.pdf:pdf},
pages = {2015},
title = {{Indicadores del desarrollo mundial| Banco de datos mundial}},
url = {http://databank.bancomundial.org/data/reports.aspx?source=2{\&}country=ESP{\&}series={\&}period={\#}},
year = {2016}
}
@article{Breiman2001,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Breiman, Leo},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 2001 - Random Forests.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Machine learning},
keywords = {icle},
pages = {5--32},
pmid = {25246403},
title = {{Random Forests}},
volume = {45.1},
year = {2001}
}
@article{Cao2001a,
abstract = {Traffic variables on an uncongested Internet wire exhibit a pervasive nonstationarity. As the rate of new TCP connections increases, arrival processes (packet and connection) tend locally toward Poisson, and time series variables (packet sizes, transferred file sizes, and connection round-trip times) tend locally toward independent. The cause of the nonstationarity is superposition: the intermingling of sequences of connections between different source-destination pairs, and the intermingling of sequences of packets from different connections. We show this empirically by extensive study of packet traces for nine links coming from four packet header databases. We show it theoretically by invoking the mathematical theory of point processes and time series. If the connection rate on a link gets sufficiently high, the variables can be quite close to Poisson and independent; if major congestion occurs on the wire before the rate gets sufficiently high, then the progression toward Poisson and independent can be arrested for some variables.},
author = {Cao, Jin and Cleveland, William S. and Lin, Dong and Sun, Don X.},
doi = {10.1145/384268.378440},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Cao et al. - 2001 - On the nonstationarity of Internet traffic.pdf:pdf},
isbn = {1581133340},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
number = {1},
pages = {102--112},
title = {{On the nonstationarity of Internet traffic}},
volume = {29},
year = {2001}
}
@book{Turabian1996,
abstract = {This is a highly technical equivalent manual to the APA but the style is Chicago. Unlikely to be very useful.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Turabian, Kate L},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2281411},
eprint = {arXiv:1011.1669v3},
isbn = {0226816273},
issn = {01621459},
number = {277},
pages = {108},
pmid = {1813},
title = {{A manual for writers of term papers, theses, and dissertations}},
url = {http://www.jstor.org/stable/2281411?origin=crossref},
volume = {52},
year = {1996}
}
@techreport{Ardila2015a,
author = {Ardila, Diego Silva},
pages = {1--64},
title = {{Bolet{\{}{\'{i}}{\}}n t{\{}{\'{e}}{\}}cnico}},
year = {2015}
}
@article{Dix2003,
author = {Dix, Alan},
pages = {165--178},
title = {{Chapter 13}},
year = {2003}
}
@book{Internet1974,
author = {Internet, As},
booktitle = {Cells Tissues Organs},
doi = {10.1159/000144207},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hornick, Marcad{\'{e}}, Venkayala - 2007 - Solving Problems in Industry.pdf:pdf},
isbn = {0131470469},
issn = {1422-6405},
number = {62},
pages = {15--20},
pmid = {21728113},
title = {{Chapter 2}},
volume = {87},
year = {1974}
}
@book{Reinhartz-Berger2013,
author = {Reinhartz-Berger, Iris and Sturm, Arnon and Clark, Tony and Cohen, Sholom and Bettin, Jorn},
booktitle = {Domain Engineering: Product Lines, Languages, and Conceptual Models},
doi = {10.1007/978-3-642-36654-3},
isbn = {9783642366543},
issn = {0471028959},
pages = {1--404},
title = {{Domain engineering: Product lines, languages, and conceptual models}},
year = {2013}
}
@article{Zhang2011,
abstract = {Context: Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective: The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method: We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. Results: We conducted two participant-observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion: We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE. {\{}{\textcopyright}{\}} 2011 Elsevier B.V. All rights reserved.},
author = {Zhang, He and Babar, Muhammad Ali and Tell, Paolo},
doi = {10.1016/j.infsof.2010.12.010},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Evidence-based software engineering,Quasi-gold standard,Search strategy,Systematic literature review},
number = {6},
pages = {625--637},
publisher = {Elsevier B.V.},
title = {{Identifying relevant studies in software engineering}},
url = {http://dx.doi.org/10.1016/j.infsof.2010.12.010},
volume = {53},
year = {2011}
}
@article{National,
author = {National, Army},
pages = {1--2},
title = {{Chapter 19}}
}
@article{Use2004,
author = {Use, Water},
doi = {10.1163/ej.9789004155947.i-937.23},
isbn = {9789004155947},
pages = {145--156},
title = {{Chapter 15}},
year = {2004}
}
@article{Choudhary2009,
abstract = {In modern manufacturing environments, vast amounts of data are collected in database management systems and data warehouses from all involved areas, including product and process design, assembly, materials planning, quality control, scheduling, maintenance, fault detection etc. Data mining has emerged as an important tool for knowledge acquisition from the manufacturing databases. This paper reviews the literature dealing with knowledge discovery and data mining applications in the broad domain of manufacturing with a special emphasis on the type of functions to be performed on the data. The major data mining functions to be performed include characterization and description, association, classification, prediction, clustering and evolution analysis. The papers reviewed have therefore been categorized in these five categories. It has been shown that there is a rapid growth in the application of data mining in the context of manufacturing processes and enterprises in the last 3 years. This review reveals the progressive applications and existing gaps identified in the context of data mining in manufacturing. A novel text mining approach has also been used on the abstracts and keywords of 150 papers to identify the research gaps and find the linkages between knowledge area, knowledge type and the applied data mining tools and techniques.},
author = {Choudhary, A K and Harding, J A and Tiwari, M K},
doi = {10.1007/s10845-008-0145-x},
isbn = {0956-5515},
issn = {09565515},
journal = {Journal of Intelligent Manufacturing},
keywords = {Data mining,Knowledge discovery,Literature review,Manufacturing,Text mining},
number = {5},
pages = {501--521},
title = {{Data mining in manufacturing: A review based on the kind of knowledge}},
volume = {20},
year = {2009}
}
@article{War2003,
author = {War, Right and War, Wrong},
journal = {Imagine},
pages = {273--297},
title = {{Chapter 11}},
year = {2003}
}
@article{Ayaz2015,
abstract = {Compressive strength and UPV parameters are the methods that are used to determine high-volume mineral admixture concrete quality. But experiments for all levels of these parameters are expensive, difficult and time consuming. For determination of output values, classifiers with model extraction features can be used. In this study, classifiers, with the rule-based M5 rule and tree model M5P in the area of data mining are used to predict the compressive strength and UPV of concrete mixtures after 3, 7, 28 and 120 days of curing. The M5 rule and tree model M5P are tested using the available test data of 40 different concrete mix-designs gathered from literature [1]. The input of the model is a variable data set corresponding to concrete mixture proportions. The findings of this study indicated that the M5 rule and tree model M5P models are sufficient tools for estimating the compressive strength and UPV of concrete. 97{\{}{\%}{\}} and 87{\{}{\%}{\}} success is obtained in predicting compressive strength and UPV results, respectively.},
author = {Ayaz, Yaşar and Kocamaz, Adnan Fatih and Karako{\c{c}}, Mehmet Burhan},
doi = {10.1016/j.conbuildmat.2015.06.029},
issn = {09500618},
journal = {Construction and Building Materials},
keywords = {Compressive strength,Concrete,Data mining,M5 rule,Tree model M5P,UPV,purpura},
mendeley-tags = {purpura},
month = {sep},
pages = {235--240},
title = {{Modeling of compressive strength and UPV of high-volume mineral-admixtured concrete using rule-based M5 rule and tree model M5P classifiers}},
url = {http://www.sciencedirect.com/science/article/pii/S0950061815007114 http://linkinghub.elsevier.com/retrieve/pii/S0950061815007114},
volume = {94},
year = {2015}
}
@book{Melorose2015,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\alpha{\$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\{}{\AA}{\}} for the interface backbone atoms) increased from 21{\{}{\%}{\}} with default Glide SP settings to 58{\{}{\%}{\}} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\{}{\%}{\}} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\{}{\%}{\}} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ingvaldsen, Jon Espen and ??zg??bek, ??zlem and Gulla, Jon Atle},
booktitle = {CEUR Workshop Proceedings},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {16130073},
keywords = {Mobile,Named entity disambiguation,Natural language processing,News,Recommender system},
pages = {33--36},
pmid = {25246403},
title = {{Context-aware user-driven news recommendation}},
volume = {1542},
year = {2015}
}
@article{Kitchenham2007a,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
author = {Kitchenham, Barbara and Charters, S},
doi = {10.1145/1134285.1134500},
isbn = {1595933751},
issn = {00010782},
journal = {Engineering},
pages = {1051},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
volume = {2},
year = {2007}
}
@article{Zhang2011a,
abstract = {Context: Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective: The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method: We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. Results: We conducted two participant-observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion: We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Zhang, He and Babar, Muhammad Ali and Tell, Paolo},
doi = {10.1016/j.infsof.2010.12.010},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Evidence-based software engineering,Quasi-gold standard,Search strategy,Systematic literature review},
number = {6},
pages = {625--637},
publisher = {Elsevier B.V.},
title = {{Identifying relevant studies in software engineering}},
url = {http://dx.doi.org/10.1016/j.infsof.2010.12.010},
volume = {53},
year = {2011}
}
@book{MarkF.HornickErikMarcade2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Table, The Periodic},
booktitle = {Cells Tissues Organs},
doi = {10.1159/000144207},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hornick, Marcad{\'{e}}, Venkayala - 2007 - Solving Problems in Industry.pdf:pdf},
isbn = {0131470469},
issn = {1422-6405},
number = {62},
pages = {15--20},
pmid = {21728113},
publisher = {Java Data Mining: Strategy, Standard, and Practice A Practical Guide for architecture, design, and implementation},
title = {{Chapter 2}},
volume = {87},
year = {1974}
}
@misc{Gomaa2004,
abstract = {BACKGROUND: Although smoking prevalence remains strikingly high in homeless populations ({\{}{\~{}}{\}}70{\{}{\%}{\}} and three times the US national average), smoking cessation studies usually exclude homeless persons. Novel evidence-based interventions are needed for this high-risk subpopulation of smokers.{\$}\backslash{\$}n{\$}\backslash{\$}nPURPOSE: To describe the aims and design of a first-ever smoking cessation clinical trial in the homeless population. The study was a two-group randomized community-based trial that enrolled participants (n = 430) residing across eight homeless shelters and transitional housing units in Minnesota. The study objective was to test the efficacy of motivational interviewing (MI) for enhancing adherence to nicotine replacement therapy (NRT; nicotine patch) and smoking cessation outcomes.{\$}\backslash{\$}n{\$}\backslash{\$}nMETHODS: Participants were randomized to one of the two groups: active (8 weeks of NRT + 6 sessions of MI) or control (NRT + standard care). Participants attended six in-person assessment sessions and eight retention visits at a location of their choice over 6 months. Nicotine patch in 2-week doses was administered at four visits over the first 8 weeks of the 26-week trial. The primary outcome was cotinine-verified 7-day point-prevalence abstinence at 6 months. Secondary outcomes included adherence to nicotine patch assessed through direct observation and patch counts. Other outcomes included the mediating and/or moderating effects of comorbid psychiatric and substance abuse disorders.{\$}\backslash{\$}n{\$}\backslash{\$}nRESULTS: Lessons learned from the community-based cessation randomized trial for improving recruitment and retention in a mobile and vulnerable population included: (1) the importance of engaging the perspectives of shelter leadership by forming and convening a Community Advisory Board; (2) locating the study at the shelters for more visibility and easier access for participants; (3) minimizing exclusion criteria to allow enrollment of participants with stable psychiatric comorbid conditions; (4) delaying the baseline visit from the eligibility visit by a week to protect against attrition; and (5) regular and persistent calls to remind participants of upcoming appointments using cell phones and shelter-specific channels of communication.{\$}\backslash{\$}n{\$}\backslash{\$}nLIMITATIONS: The study's limitations include generalizability due to the sample drawn from a single Midwestern city in the United States. Since inclusion criteria encompassed willingness to use NRT patch, all participants were motivated and were ready to quit smoking at the time of enrollment in the study. Findings from the self-select group will be generalizable only to those motivated and ready to quit smoking. High incentives may limit the degree to which the intervention is replicable.{\$}\backslash{\$}n{\$}\backslash{\$}nCONCLUSIONS: Lessons learned reflect the need to engage communities in the design and implementation of community-based clinical trials with vulnerable populations.},
author = {Goldade, Kate and Whembolua, Guy-Lucien and Thomas, Janet and Eischen, Sara and Guo, Hongfei and Connett, John and {Des Jarlais}, Don and Resnicow, Ken and Gelberg, Lillian and Owen, Greg and Grant, Jon and Ahluwalia, Jasjit S and Okuyemi, Kolawole S},
booktitle = {Clinical trials (London, England)},
doi = {10.1177/1740774511423947},
isbn = {1740774511423},
issn = {1740-7753},
keywords = {Community Networks,Female,Homeless Persons,Humans,Interviews as Topic,Male,Minnesota,Motivation,Patient Compliance,Patient Compliance: psychology,Research Design,Smoking Cessation,Tobacco Use Cessation Products,Tobacco Use Disorder,Tobacco Use Disorder: drug therapy,Treatment Outcome},
number = {6},
pages = {744--754},
pmid = {22167112},
title = {{Designing a smoking cessation intervention for the unique needs of homeless persons: a community-based randomized clinical trial.}},
url = {http://europepmc.org/articles/PMC4729365/?report=abstract},
volume = {8},
year = {2011}
}
@article{Lasers2004,
author = {Lasers, Light},
pages = {199--257},
title = {{Chapter 9}},
year = {2004}
}
@book{Kang2010,
author = {Kang, Kc and Sugumaran, V and Park, S},
isbn = {9781420068429},
pages = {561},
title = {{Applied software product line engineering}},
url = {http://books.google.com.mx/books/about/Applied{\%}7B{\_}{\%}7DSoftware{\%}7B{\_}{\%}7DProduct{\%}7B{\_}{\%}7DLine{\%}7B{\_}{\%}7DEngineerin.html?id=7XO{\%}7B{\_}{\%}7Dghvkpt4C{\%}7B{\&}{\%}7Dredir{\%}7B{\_}{\%}7Desc=y},
year = {2010}
}
@book{Kwak2011,
address = {New York, NY},
author = {Kwak, Minjung and Kim, Harrison},
booktitle = {Engineering Optimization Journal},
doi = {10.1007/978-1-4614-7937-6},
editor = {Simpson, Timothy W. and Jiao, Jianxin and Siddique, Zahed and H{\"{o}}ltt{\"{a}}-Otto, Katja},
isbn = {978-1-4614-7936-9},
number = {3},
pages = {707--735},
publisher = {Springer New York},
title = {{Advances in Product Family and Product Platform Design}},
url = {http://link.springer.com/10.1007/978-1-4614-7937-6},
volume = {43},
year = {2014}
}
@book{Almeida1994,
address = {Totowa, NJ},
author = {Almeida, Henrique a and Ba, Paulo J},
doi = {10.1007/978-1-61779-764-4},
editor = {Liebschner, Michael A.K.},
isbn = {978-1-61779-763-7},
keywords = {biofabrication,computer,scaffold vascularization,scaffolds,tissue engineering},
pages = {301--339},
publisher = {Humana Press},
series = {Methods in Molecular Biology},
title = {{Computer-Aided Tissue Engineering}},
url = {http://link.springer.com/10.1007/978-1-61779-764-4},
volume = {868},
year = {2012}
}
@article{Limited2006,
author = {Limited, Woodhead Publishing},
pages = {259--271},
title = {{Chapter 10}},
year = {2006}
}
@article{Dictionary2010,
author = {Dictionary, Oxford English},
pages = {629--667},
title = {{Chapter 17}},
year = {2010}
}
@book{Simpson2006,
abstract = {Designing products and product families so they may be customized for the global marketplace and achieving these goals in abbreviated time period, while maintaining production efficiencies are the keys to successful manufacturing operations. The research on these areas has matured rapidly over the last decade. Today's highly competitive and volatile marketplace is reshaping the way many companies do business as rapid innovation and mass customization offer a new form of competitive advantage. In response, companies like Sony, Black and Decker, and Kodak have successfully implemented strategies to design and develop an entire family of products to satisfy a wide variety of customer requirements. Product Platform and Product Family Design: Methods and Applications discusses how product platform and product family design can be used successfully to: -increase variety within a product line, -shorten manufacturing lead times, - reduce overall costs within a product line.The material available here will serve as both a reference and a hands-on guide for practitioners involved in the design, planning and production of products. Included are real-life case studies that explain the benefits of platform based product development.},
author = {Simpson, Timothy W and Siddique, Zahed and Jiao, Jianxin},
booktitle = {Product Platform and Product Family Design: Methods and Applications},
doi = {10.1007/0-387-29197-0},
isbn = {0387257217},
pages = {1--548},
title = {{Product platform and product family design: Methods and applications}},
year = {2006}
}
@article{Wieringa2006a,
abstract = {The article presents the outcome of the discussions by members of the steering committee of the International Electrical and Electronic Engineering Requirements Engineering (RE) Conference, regarding paper classification and evaluation criteria for RE papers. Section two of the article sketches the rationale for the classification. Section three presents the classification, and section four concludes with a discussion of background ideas and related work.},
author = {Wieringa, Roel and Maiden, Neil and Mead, Nancy and Rolland, Colette},
doi = {10.1007/s00766-005-0021-6},
isbn = {0947-3602},
issn = {0947-3602},
journal = {Requirements Engineering},
keywords = {Paper classification,Paper evaluation criteria,Requirements engineering research,Research methods},
month = {mar},
number = {1},
pages = {102--107},
title = {{Requirements engineering paper classification and evaluation criteria: a proposal and a discussion}},
url = {http://link.springer.com/10.1007/s00766-005-0021-6},
volume = {11},
year = {2006}
}
@article{Revolution2012,
author = {Revolution, T H E},
doi = {10.1016/B978-0-12-375674-9.10007-2},
isbn = {978-1-119-12754-3},
pages = {1--16},
title = {{Chapter 7}},
volume = {1},
year = {2012}
}
@article{Moline1997,
author = {Moline, East},
doi = {10.1177/0896920512450815},
isbn = {0760601437},
issn = {0896-9205},
pages = {519--520},
pmid = {49233143},
title = {{About the Authors}},
volume = {8331},
year = {1997}
}
@article{Ayaz2015a,
abstract = {Compressive strength and UPV parameters are the methods that are used to determine high-volume mineral admixture concrete quality. But experiments for all levels of these parameters are expensive, difficult and time consuming. For determination of output values, classifiers with model extraction features can be used. In this study, classifiers, with the rule-based M5 rule and tree model M5P in the area of data mining are used to predict the compressive strength and UPV of concrete mixtures after 3, 7, 28 and 120days of curing. The M5 rule and tree model M5P are tested using the available test data of 40 different concrete mix-designs gathered from literature [1]. The input of the model is a variable data set corresponding to concrete mixture proportions. The findings of this study indicated that the M5 rule and tree model M5P models are sufficient tools for estimating the compressive strength and UPV of concrete. 97{\{}{\%}{\}} and 87{\{}{\%}{\}} success is obtained in predicting compressive strength and UPV results, respectively.},
author = {Ayaz, Yaşar and Kocamaz, Adnan Fatih and Karako{\c{c}}, Mehmet Burhan},
doi = {10.1016/j.conbuildmat.2015.06.029},
issn = {09500618},
journal = {Construction and Building Materials},
keywords = {Compressive strength,Concrete,Data mining,M5 rule,Tree model M5P,UPV},
month = {sep},
pages = {235--240},
title = {{Modeling of compressive strength and UPV of high-volume mineral-admixtured concrete using rule-based M5 rule and tree model M5P classifiers}},
url = {http://www.sciencedirect.com/science/article/pii/S0950061815007114},
volume = {94},
year = {2015}
}
@article{Petersen2008a,
archivePrefix = {arXiv},
arxivId = {2227123},
author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
doi = {10.1016/j.infsof.2015.03.007},
eprint = {2227123},
isbn = {0-7695-2555-5},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {evidence based software engineering,systematic mapping studies,systematic reviews},
month = {aug},
pages = {1--18},
title = {{Guidelines for conducting systematic mapping studies in software engineering: An update}},
url = {http://dl.acm.org/citation.cfm?id=2227123 http://linkinghub.elsevier.com/retrieve/pii/S0950584915000646},
volume = {64},
year = {2015}
}
@article{Berthier2007,
abstract = {Usar para Introducci{\{}{\'{o}}{\}}n a la Soc. de la Inf.},
author = {Berthier, a},
journal = {Conocimiento y Sociedad},
number = {2006},
pages = {1--11},
title = {{El sistema de referencias Harvard}},
url = {http://www.ucbcba.edu.bo/Documentos/El{\%}7B{\_}{\%}7Dsistema{\%}7B{\_}{\%}7Dde{\%}7B{\_}{\%}7Dreferencias{\%}7B{\_}{\%}7DHarvard.pdf},
year = {2007}
}
@article{Picken2005,
author = {Picken, Felicity},
doi = {10.1007/978-1-62703-447-0},
isbn = {9781627034470},
pages = {1--8},
title = {{Chapter 18}},
year = {2005}
}
@article{Mazo2008,
author = {Mazo, Ra{\'{u}}l and Salinesi, Camille},
journal = {Techniques},
number = {September},
pages = {1--153},
title = {{Methods, Techniques and Tools for Product Line Model Verification}},
url = {http://hal-paris1.archives-ouvertes.fr/halshs-00323675/},
year = {2008}
}
@article{Perovsek2015,
abstract = {Inductive Logic Programming (ILP) and Relational Data Mining (RDM) address the task of inducing models or patterns from multi-relational data. One of the established approaches to RDM is propositionalization, characterized by transforming a relational database into a single-table representation. This paper presents a propositionalization technique called wordification which can be seen as a transformation of a relational database into a corpus of text documents. Wordification constructs simple, easy to understand features, acting as words in the transformed Bag-Of-Words representation. This paper presents the wordification methodology, together with an experimental comparison of several propositionalization approaches on seven relational datasets. The main advantages of the approach are: simple implementation, accuracy comparable to competitive methods, and greater scalability, as it performs several times faster on all experimental databases. Furthermore, the wordification methodology and the evaluation procedure are implemented as executable workflows in the web-based data mining platform ClowdFlows. The implemented workflows include also several other ILP and RDM algorithms, as well as the utility components that were added to the platform to enable access to these techniques to a wider research audience.},
author = {Perov{\v{s}}ek, Matic and Vavpeti{\v{c}}, An{\v{z}}e and Kranjc, Janez and Cestnik, Bojan and Lavra{\v{c}}, Nada},
doi = {10.1016/j.eswa.2015.04.017},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Inductive Logic Programming,Propositionalization,Relational Data Mining,Text mining,Wordification},
month = {oct},
number = {17-18},
pages = {6442--6456},
title = {{Wordification: Propositionalization by unfolding relational data into bags of words}},
url = {http://www.sciencedirect.com/science/article/pii/S095741741500247X},
volume = {42},
year = {2015}
}
@incollection{Analysis2007,
author = {Falgarone, G{\'{e}}raldine and Chiocchia, Gilles},
doi = {10.1016/S0065-230X(09)04008-1},
isbn = {978-0-12-108380-9},
issn = {1370-0049},
pages = {139--170},
title = {{Chapter 8 Clusterin}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0065230X09040081},
year = {2009}
}
@article{Nanculef2014,
abstract = {We present a method for the classification of multi-labeled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labeled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labeled streams.?? 2014 Elsevier Ltd. All rights reserved.},
author = {??anculef, Ricardo and Flaounas, Ilias and Cristianini, Nello},
doi = {10.1016/j.eswa.2014.02.017},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Data streams,Feature hashing,Massive data mining,Multi-label classification,Text classification},
month = {sep},
number = {11},
pages = {5431--5450},
title = {{Efficient classification of multi-labeled text streams by clashing}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414000803},
volume = {41},
year = {2014}
}
@techreport{Ardila2015,
address = {Bogot{\{}{\'{a}}{\}}},
author = {Ardila, Diego Silva},
institution = {DANE},
pages = {1--64},
title = {{Indicadores B{\{}{\'{a}}{\}}sicos de Tenencia y Uso de Tecnolog{\{}{\'{i}}{\}}as de la Informaci{\{}{\'{o}}{\}}n y Comunicaci{\{}{\'{o}}{\}}n en empresas 2013 Cifras Definitivas}},
year = {2015}
}
@article{Lewandowski2015a,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\alpha{\$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\{}{\AA}{\}} for the interface backbone atoms) increased from 21{\{}{\%}{\}} with default Glide SP settings to 58{\{}{\%}{\}} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\{}{\%}{\}} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\{}{\%}{\}} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lewandowski, Clare M},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {The effects of brief mindfulness intervention on acute pain experience: An examination of individual difference},
keywords = {icle},
pages = {346--356},
pmid = {25246403},
title = {{No Title No Title}},
volume = {1},
year = {2015}
}
@article{Wohlin2014a,
author = {Wohlin, Claes},
doi = {10.1145/2601248.2601268},
isbn = {9781450324762},
journal = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering - EASE '14},
keywords = {replication,snowball search,snowballing,systematic,systematic literature review},
pages = {1--10},
title = {{Guidelines for snowballing in systematic literature studies and a replication in software engineering}},
url = {http://dl.acm.org/citation.cfm?doid=2601248.2601268},
year = {2014}
}
@article{Perovsek2015a,
abstract = {Inductive Logic Programming (ILP) and Relational Data Mining (RDM) address the task of inducing models or patterns from multi-relational data. One of the established approaches to RDM is propositionalization, characterized by transforming a relational database into a single-table representation. This paper presents a propositionalization technique called wordification which can be seen as a transformation of a relational database into a corpus of text documents. Wordification constructs simple, easy to understand features, acting as words in the transformed Bag-Of-Words representation. This paper presents the wordification methodology, together with an experimental comparison of several propositionalization approaches on seven relational datasets. The main advantages of the approach are: simple implementation, accuracy comparable to competitive methods, and greater scalability, as it performs several times faster on all experimental databases. Furthermore, the wordification methodology and the evaluation procedure are implemented as executable workflows in the web-based data mining platform ClowdFlows. The implemented workflows include also several other ILP and RDM algorithms, as well as the utility components that were added to the platform to enable access to these techniques to a wider research audience.},
author = {Perov{\v{s}}ek, Matic and Vavpeti{\v{c}}, An{\v{z}}e and Kranjc, Janez and Cestnik, Bojan and Lavra{\v{c}}, Nada},
doi = {10.1016/j.eswa.2015.04.017},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Inductive Logic Programming,Propositionalization,Relational Data Mining,Text mining,Wordification,purpura,rojo},
mendeley-tags = {purpura,rojo},
month = {oct},
number = {17-18},
pages = {6442--6456},
title = {{Wordification: Propositionalization by unfolding relational data into bags of words}},
url = {http://www.sciencedirect.com/science/article/pii/S095741741500247X http://linkinghub.elsevier.com/retrieve/pii/S095741741500247X},
volume = {42},
year = {2015}
}
@article{Management2008,
author = {Management, Designing},
doi = {10.1227/01.NEU.0000028161.91504.4F},
isbn = {9789966029171},
pages = {103--116},
title = {{Chapter 5}},
year = {2008}
}
@article{Equations1910,
author = {Equations, Basic},
doi = {10.1016/B978-0-444-41284-3.50010-2},
isbn = {9781627034470},
pages = {117--129},
title = {{Chapter 6}},
year = {1910}
}
@book{Pohl2010,
author = {Kang, Kyo C and Sugumaran, Vijayan and Park, Sooyong},
booktitle = {Auerbach Publications, CRC press Taylor {\{}{\&}{\}} Francis Group},
isbn = {1420068415, 9781420068412},
pages = {539},
title = {{Applied Software Product Line Engineering}},
year = {2010}
}
@article{Lewandowski2015,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\alpha{\$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\{}{\AA}{\}} for the interface backbone atoms) increased from 21{\{}{\%}{\}} with default Glide SP settings to 58{\{}{\%}{\}} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\{}{\%}{\}} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\{}{\%}{\}} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lewandowski, Clare M},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {The effects of brief mindfulness intervention on acute pain experience: An examination of individual difference},
keywords = {icle},
pmid = {25246403},
title = {{No Title No Title}},
volume = {1},
year = {2015}
}
@article{Development2007,
author = {Development, Protecting Psychosocial},
doi = {10.1007/978-1-4419-7344-3},
isbn = {9781441973443},
pages = {387--406},
title = {{Chapter 14}},
year = {2007}
}
@article{Wohlin2014,
abstract = {Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably. Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review. Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches. Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review. Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.},
author = {Wohlin, Claes},
doi = {10.1145/2601248.2601268},
isbn = {9781450324762},
journal = {18th International Conference on Evaluation and Assessment in Software Engineering (EASE 2014)},
keywords = {replication,snowball search,snowballing,systematic literature review,systematic mapping studies},
pages = {1--10},
title = {{Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering}},
url = {http://dl.acm.org/citation.cfm?doid=2601248.2601268},
year = {2014}
}
@article{Dietel2011,
author = {Dietel, Ron},
doi = {10.1007/978-94-6091-478-2_16},
isbn = {978-94-6091-478-2},
pages = {55--60},
title = {{Chapter 16}},
volume = {1},
year = {2011}
}
@article{Hornick2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dubielzig, Richard R and Ketring, Kerry and McLellan, Gillian J and Albert, Daniel M},
doi = {10.1016/B978-0-7020-2797-0.00001-1},
eprint = {arXiv:1011.1669v3},
isbn = {1111001111},
issn = {15710661},
journal = {Veterinary Ocular Pathology},
number = {July},
pages = {1--8},
pmid = {20314319},
title = {{The principles and practice of ocular pathology}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9780702027970000011},
volume = {35},
year = {2010}
}
@article{Aboriginal2003,
author = {Aboriginal, Achieving},
doi = {10.1007/978-},
isbn = {9780511557712},
issn = {1422-6405},
number = {August 2000},
pages = {25--49},
pmid = {21728113},
title = {{Chapter 2}},
volume = {14},
year = {2003}
}
@article{Kitchenham2007,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
author = {Budgen, David and Brereton, Pearl},
doi = {10.1145/1134285.1134500},
isbn = {1595933751},
issn = {00010782},
journal = {Proceeding of the 28th international conference on Software engineering - ICSE '06},
pages = {1051},
title = {{Performing systematic literature reviews in software engineering}},
url = {http://portal.acm.org/citation.cfm?doid=1134285.1134500},
volume = {2},
year = {2006}
}
@article{Enshen2005,
abstract = {To sum up, the individuality and universality of energy efficient buildings have both significant difference and a close relationship, any one-sided view is erroneous. In this monograph, the analysis on the hourly load, daily, monthly or categorized energy consumption and its relative variation rates can help readers understand the macroscopic (annual) variation law of building energy consumption and the essence of building energy efficiency more deeply in terms of microscopic (hourly), super-microscopic (daily) and sub-macroscopic (monthly) aspects. These conclusions may provide a powerful support for the enactment and implementation of building energy efficient standards in all countries. The above studies may luckily be considered as an important leap in the analysis of building energy consumption and evaluation of building energy efficiency. The authors would be very pleased if their publication could arouse valuable opinions from experts all over the world to improve the study steadily and do its bit in the stipulation and implementation of building energy efficiency standards as well as the evaluation of energy efficiency buildings to every country of the world. ?? 2004 Elsevier Ltd. All rights reserved.},
author = {Enshen, Long},
doi = {10.1016/j.buildenv.2004.09.009},
issn = {03601323},
journal = {Building and Environment},
number = {4 SPEC. ISS.},
pages = {439--441},
title = {{Guide to readers}},
volume = {40},
year = {2005}
}
@book{Izenman2006,
abstract = {File swarming (or file sharing) is one of the most important applications in P2P networks. In this paper, we propose a stochastic framework to analyze a file-swarming system under realistic setting: constraints in upload/download capacity, collaboration among peers and incentive for chunk exchange. We first extend the results in the coupon system [L. Massoulie, M. Vojnovic, Coupon replication systems, in: Proc. ACM SIGMETRICS, Banff, Alberta, Canada, 2005] by providing a tighter performance bound. Then we generalize the coupon system by considering peers with limited upload and download capacity. We illustrate the last-piece problem and show the effectiveness of using forward error-correction (FEC) code and/or multiple requests to improve the performance. Lastly, we propose a framework to analyze an incentive-based file-swarming system. The stochastic framework we propose can serve as a basis for other researchers to analyze and design more advanced features of file-swarming systems. {\{}{\textcopyright}{\}} 2007 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lin, Minghong and Fan, Bin and Lui, John C S and Chiu, Dah Ming},
booktitle = {Performance Evaluation},
doi = {10.1016/j.peva.2007.06.006},
eprint = {arXiv:1011.1669v3},
isbn = {9780387781884},
issn = {01665316},
keywords = {BitTorrent,P2P file sharing,Performance modeling},
number = {9-12},
pages = {856--875},
pmid = {10911016},
title = {{Stochastic analysis of file-swarming systems}},
url = {http://books.google.com/books?id=9tv0taI8l6YC},
volume = {64},
year = {2007}
}
@book{NationalAeronauticsandSpaceAdministration2007,
author = {{National Aeronautics and Space Administration}},
booktitle = {Nasa},
doi = {NASA/SP-2007-6105Rev1},
isbn = {3016210134},
pages = {22--29},
title = {{NASA Systems Engineering Handbook}},
url = {http://adsabs.harvard.edu/full/1995nse..book.....S},
year = {2007}
}
@article{Wieringa2006,
abstract = {The article presents the outcome of the discussions by members of the steering committee of the International Electrical and Electronic Engineering Requirements Engineering (RE) Conference, regarding paper classification and evaluation criteria for RE papers. Section two of the article sketches the rationale for the classification. Section three presents the classification, and section four concludes with a discussion of background ideas and related work.},
author = {Wieringa, Roel and Maiden, Neil and Mead, Nancy and Rolland, Colette},
doi = {10.1007/s00766-005-0021-6},
isbn = {0947-3602},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Paper classification,Paper evaluation criteria,Requirements engineering research,Research methods},
number = {1},
pages = {102--107},
title = {{Requirements engineering paper classification and evaluation criteria: A proposal and a discussion}},
volume = {11},
year = {2006}
}
@book{Hastie2015,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
isbn = {9781498712170},
title = {{Statistical Learning with Sparsity}},
year = {2015}
}
@article{Group2009,
author = {Group, Francis},
doi = {10.1016/B978-0-7020-2797-0.00001-1},
isbn = {1111001111},
issn = {15710661},
pages = {1--4},
pmid = {20314319},
title = {{Chapter 1}},
volume = {1},
year = {2009}
}
@article{Ramasubbu2012,
abstract = {This study develops and empirically tests the idea that the impact of structural complexity on perfective maintenance of object-oriented software is significantly determined by the team strategy of programmers (independent or collaborative). We analyzed two key dimensions of software structure, coupling and cohesion, with respect to the maintenance effort and the perceived ease-of-maintenance by pairs of programmers. Hypotheses based on the distributed cognition and task interdependence theoretical frameworks were tested using data collected from a controlled lab experiment employing professional programmers. The results show a significant interaction effect between coupling, cohesion, and programmer team strategy on both maintenance effort and perceived ease-of-maintenance. Highly cohesive and low-coupled programs required lower maintenance effort and were perceived to be easier to maintain than the low-cohesive programs and high-coupled programs. Further, our results would predict that managers who strategically allocate maintenance tasks to either independent or collaborative programming teams depending on the structural complexity of software could lower their team's maintenance effort by as much as 70 percent over managers who use simple uniform resource allocation policies. These results highlight the importance of achieving congruence between team strategies employed by collaborating programmers and the structural complexity of software. {\{}{\&}{\}}copy; 2012 IEEE.},
author = {Ramasubbu, Narayan and Kemerer, Chris F and Hong, Jeff},
doi = {10.1109/TSE.2011.88},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ramasubbu, Kemerer, Hong - 2012 - Structural complexity and programmer team strategy An experimental test(2).pdf:pdf},
isbn = {10.1109/TSE.2011.88},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {CK metrics,Object-oriented programming,complexity measures,maintenance process,programming teams,software management,software productivity,software quality},
number = {5},
pages = {1--15},
title = {{Structural complexity and programmer team strategy: An experimental test}},
volume = {38},
year = {2012}
}
@article{Petersen2008,
abstract = {Systematic configuration management is important for successful software product lines. We can use aspect-oriented software development to decompose software product lines based on features that can ease configuration management. In this paper, we present a military maintenance product line that employs such strategy. In particular, we applied a specific approach, feature based modeling (FBM), in the construction of the system. We have extended FBM to address properties specific to product line. We will discuss the advantages of FBM when applied to product lines. Such gains include the functional decomposition of the system along user requirements (features) as aspects. Moreover, those features exhibit unidirectional dependency (i.e. among any two features, at most one depend on another) that enables developers to analyze the effect of any modification they may make on any feature. In addition, any variations can be captured as aspects which can also be incorporated easily into the core asset if such variation is deemed to be important enough to be included in the product line for further evolution. [ABSTRACT FROM AUTHOR]},
author = {MAREW, TEGEGNE and KIM, JUNGYOON and BAE, D O O HWAN},
doi = {10.1142/S0218194007003112},
isbn = {0-7695-2555-5},
issn = {02181940},
journal = {International Journal of Software Engineering {\{}{\&}{\}} Knowledge Engineering},
keywords = {evidence based software engineering,systematic mapping studies,systematic reviews},
number = {1},
pages = {33--55},
title = {{Systematic Functional Decomposition in a Product Line Using Aspect-Oriented Software Development:: a Case Study}},
url = {http://content.ebscohost.com/ContentServer.asp?T=P{\%}7B{\&}{\%}7DP=AN{\%}7B{\&}{\%}7DK=22674743{\%}7B{\&}{\%}7DS=R{\%}7B{\&}{\%}7DD=bth{\%}7B{\&}{\%}7DEbscoContent=dGJyMNHX8kSeqK44zdnyOLCmr0qeprZSr6e4SrCWxWXS{\%}7B{\&}{\%}7DContentCustomer=dGJyMPGosk+xq65QuePfgeyx44Dt6fIA{\$}{\%}5C{\$}nhttp://search.ebscohost.com/login.aspx?direct=true{\%}7B{\&}{\%}7Ddb=},
volume = {17},
year = {2007}
}
@article{Reading2005,
author = {Reading, Further},
journal = {Behavioral Ecology},
pages = {683--689},
title = {{Further Reading}},
year = {2005}
}
@book{Capilla2013,
address = {Berlin, Heidelberg},
author = {Capilla, Rafael and Bosch, J and Kang, Kyo-Chul},
doi = {10.1007/978-3-642-36583-6},
editor = {Capilla, Rafael and Bosch, Jan and Kang, Kyo-Chul},
isbn = {978-3-642-36582-9},
pages = {317},
publisher = {Springer Berlin Heidelberg},
title = {{Systems and Software Variability Management}},
url = {http://link.springer.com/10.1007/978-3-642-36583-6},
year = {2013}
}
@book{VanDerLinden2007,
abstract = {Software product lines represent perhaps the most exciting paradigm shift in software development since the advent of high-level programming languages. Nowhere else in software engineering have we seen such breathtaking improvements in cost, quality, time to market, and developer productivity, often registering in the order-of-magnitude range. While the underlying concepts are straightforward enough - building a family of related products or systems by planned and careful reuse of a base of generalized software development assets - the devil can be in the details, as successful product line practice can involve organizational change, business process change, and technology change. The authors ideally combine academic research results with industrial real-world experiences, thus presenting a broad view on product line engineering so that both managers and technical specialists will benefit from reading it. After presenting a common framework for the description of the industrial case studies, they capture the wealth of knowledge that eight companies have gathered during the introduction of the software product line engineering approach in their daily practice. After reading this book, you will understand all the relevant aspects, regarding business, architecture, process, and organizational issues, of applying software product line engineering. If you consider using a product line approach in your organization, or if you want to improve your current practices you will find a rich set of useful information at your fingertips - from practitioners to practitioners.},
author = {{Van Der Linden}, Frank and Schmid, Klaus and Rommes, Eelco},
booktitle = {Software Product Lines in Action: The Best Industrial Practice in Product Line Engineering},
doi = {10.1007/978-3-540-71437-8},
isbn = {9783540714361},
pages = {1--333},
title = {{Software product lines in action: The best industrial practice in product line engineering}},
year = {2007}
}
@article{Comunicaciones2014,
author = {las Comunicaciones, Ministerio de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y},
keywords = {MINTIC},
pages = {177},
title = {{INFORME RENDICIÓN DE CUENTAS Ministerio de Tecnologías de la Información y las Comunicaciones – Fondo de Tecnologías de la Información y las Comunicaciones}},
url = {http://www.mintic.gov.co/portal/604/articles-6423{\%}7B{\_}{\%}7Drecurso{\%}7B{\_}{\%}7D5.pdf},
year = {2014}
}
@article{Mangalova2014a,
abstract = {The paper deals with a modeling procedure which aims to predict the power output of wind farm electricity generators. The following modeling steps are proposed: factor selection, raw data pretreatment, model evaluation and optimization. Both heuristic and formal methods are combined to construct the model. The basic modeling approach here is the k-nearest neighbors method.},
author = {Mangalova, E. and Agafonov, E.},
doi = {10.1016/j.ijforecast.2013.07.008},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Mangalova, Agafonov - 2014 - Wind power forecasting using the mmlmath altimg=si14.gif display=inline overflow=scroll xmlnsxocs=httpwww.e.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Cross-validation,Data mining,Energy forecasting*,Feature selection,Forecasting competitions*,Nonparametric models,Regression tree,rojo},
mendeley-tags = {rojo},
month = {apr},
number = {2},
pages = {402--406},
title = {{Wind power forecasting using the {\textless}mml:math altimg="si14.gif" display="inline" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207013000848 http://linkinghub.elsevier.com/retrieve/pii/S0169207013000848},
volume = {30},
year = {2014}
}
@article{Liang2015a,
abstract = {Financial distress prediction is always important for financial institutions in order for them to assess the financial health of enterprises and individuals. Bankruptcy prediction and credit scoring are two important issues in financial distress prediction where various statistical and machine learning techniques have been employed to develop financial prediction models. Since there are no generally agreed upon financial ratios as input features for model development, many studies consider feature selection as a pre-processing step in data mining before constructing the models. However, most works only focused on applying specific feature selection methods over either bankruptcy prediction or credit scoring problem domains. In this work, a comprehensive study is conducted to examine the effect of performing filter and wrapper based feature selection methods on financial distress prediction. In addition, the effect of feature selection on the prediction models obtained using various classification techniques is also investigated. In the experiments, two bankruptcy and two credit datasets are used. In addition, three filter and two wrapper based feature selection methods combined with six different prediction models are studied. Our experimental results show that there is no the best combination of the feature selection method and the classification technique over the four datasets. Moreover, depending on the chosen techniques, performing feature selection does not always improve the prediction performance. However, on average performing the genetic algorithm and logistic regression for feature selection can provide prediction improvements over the credit and bankruptcy datasets respectively.},
author = {Liang, Deron and Tsai, Chih-Fong and Wu, Hsin-Ting},
doi = {10.1016/j.knosys.2014.10.010},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Liang, Tsai, Wu - 2015 - The effect of feature selection on financial distress prediction.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Bankruptcy prediction,Credit scoring,Data mining,Feature selection,Financial distress prediction,rojo},
mendeley-tags = {rojo},
month = {jan},
pages = {289--297},
title = {{The effect of feature selection on financial distress prediction}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705114003773},
volume = {73},
year = {2015}
}
@article{Atzmueller2015a,
abstract = {Communities can intuitively be defined as subsets of nodes of a graph with a dense structure in the corresponding subgraph. However, for mining such communities usually only structural aspects are taken into account. Typically, no concise nor easily interpretable community description is provided. For tackling this issue, this paper focuses on description-oriented community detection using subgroup discovery. In order to provide both structurally valid and interpretable communities we utilize the graph structure as well as additional descriptive features of the graph's nodes. A descriptive community pattern built upon these features then describes and identifies a community, i.e., a set of nodes, and vice versa. Essentially, we mine patterns in the “description space” characterizing interesting sets of nodes (i.e., subgroups) in the “graph space”; the interestingness of a community is evaluated by a selectable quality measure. We aim at identifying communities according to standard community quality measures, while providing characteristic descriptions of these communities at the same time. For this task, we propose several optimistic estimates of standard community quality functions to be used for efficient pruning of the search space in an exhaustive branch-and-bound algorithm. We demonstrate our approach in an evaluation using five real-world data sets, obtained from three different social media applications.},
author = {Atzmueller, Martin and Doerfel, Stephan and Mitzlaff, Folke},
doi = {10.1016/j.ins.2015.05.008},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Atzmueller, Doerfel, Mitzlaff - 2015 - Description-oriented community detection using exhaustive subgroup discovery.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Community detection,Exceptional model mining,Exploratory data analysis,Optimistic estimates,Social network analysis,Subgroup discovery,purpura},
mendeley-tags = {purpura},
month = {feb},
pages = {965--984},
title = {{Description-oriented community detection using exhaustive subgroup discovery}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025515003667 http://linkinghub.elsevier.com/retrieve/pii/S0020025515003667},
volume = {329},
year = {2016}
}
@article{Nanculef2014a,
abstract = {We present a method for the classification of multi-labeled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labeled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labeled streams.},
author = {{\~{N}}anculef, Ricardo and Flaounas, Ilias and Cristianini, Nello},
doi = {10.1016/j.eswa.2014.02.017},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Data streams,Feature hashing,Massive data mining,Multi-label classification,Text classification},
month = {sep},
number = {11},
pages = {5431--5450},
title = {{Efficient classification of multi-labeled text streams by clashing}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414000803},
volume = {41},
year = {2014}
}
@article{Poria2015a,
abstract = {A huge number of videos are posted every day on social media platforms such as Facebook and YouTube. This makes the Internet an unlimited source of information. In the coming decades, coping with such information and mining useful knowledge from it will be an increasingly difficult task. In this paper, we propose a novel methodology for multimodal sentiment analysis, which consists in harvesting sentiments from Web videos by demonstrating a model that uses audio, visual and textual modalities as sources of information. We used both feature- and decision-level fusion methods to merge affective information extracted from multiple modalities. A thorough comparison with existing works in this area is carried out throughout the paper, which demonstrates the novelty of our approach. Preliminary comparative experiments with the YouTube dataset show that the proposed multimodal system achieves an accuracy of nearly 80{\%}, outperforming all state-of-the-art systems by more than 20{\%}.},
author = {Poria, Soujanya and Cambria, Erik and Howard, Newton and Huang, Guang-Bin and Hussain, Amir},
doi = {10.1016/j.neucom.2015.01.095},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Poria et al. - 2016 - Fusing audio, visual and textual clues for sentiment analysis from multimodal content.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Big social data analysis,Emotions,Extreme learning machines,Multimodal fusion,Multimodal sentiment analysis,Opinion mining,Sentiment analysis},
month = {aug},
title = {{Fusing Audio, Visual and Textual Clues for Sentiment Analysis from Multimodal Content}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231215011297},
year = {2015}
}
@book{Hastie2015,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Wainwright - 2015 - Statistical Learning with Sparsity.pdf:pdf},
isbn = {9781498712170},
title = {{Statistical Learning with Sparsity}},
year = {2015}
}
@inproceedings{Caruana2006,
abstract = {A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.},
address = {New York, New York, USA},
author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143865},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Caruana, Niculescu-Mizil - 2006 - An empirical comparison of supervised learning algorithms.pdf:pdf},
isbn = {1595933832},
issn = {1595933832},
keywords = {gris},
mendeley-tags = {gris},
number = {1},
pages = {161--168},
publisher = {ACM Press},
title = {{An empirical comparison of supervised learning algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143865},
volume = {C},
year = {2006}
}
@book{Melorose2015,
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wohlin, Claes and Runeson, Per and H{\"{o}}st, Martin and Ohlsson, Magnus C. and Regnell, Bj{\"{o}}rn and Wessl{\'{e}}n, Anders},
booktitle = {Statewide Agricultural Land Use Baseline 2015},
doi = {10.1007/978-3-642-29044-2},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wohlin et al. - 2012 - Experimentation in Software Engineering.pdf:pdf},
isbn = {978-3-642-29043-5},
issn = {1098-6596},
keywords = {azul,icle},
mendeley-tags = {azul},
pmid = {25246403},
publisher = {Springer Berlin Heidelberg},
title = {{Experimentation in Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-642-29044-2},
volume = {1},
year = {2012}
}
@book{Kwak2011,
address = {New York, NY},
author = {Kwak, Minjung and Kim, Harrison},
booktitle = {Advances in Product Family and Product Platform Design},
doi = {10.1007/978-1-4614-7937-6},
editor = {Simpson, Timothy W. and Jiao, Jianxin and Siddique, Zahed and H{\"{o}}ltt{\"{a}}-Otto, Katja},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kwak, Kim - 2014 - Advances in Product Family and Product Platform Design.pdf:pdf},
isbn = {978-1-4614-7936-9},
keywords = {naranja,rojo},
mendeley-tags = {naranja,rojo},
number = {3},
pages = {707--735},
publisher = {Springer New York},
title = {{Advances in Product Family and Product Platform Design}},
url = {http://link.springer.com/10.1007/978-1-4614-7937-6},
volume = {43},
year = {2014}
}
@book{Capilla2013,
address = {Berlin, Heidelberg},
author = {Capilla, Rafael and Bosch, J. and Kang, Kyo-Chul},
doi = {10.1007/978-3-642-36583-6},
editor = {Capilla, Rafael and Bosch, Jan and Kang, Kyo-Chul},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Capilla, Bosch, Kang - 2013 - Systems and Software Variability Management.pdf:pdf},
isbn = {978-3-642-36582-9},
keywords = {verde},
mendeley-tags = {verde},
pages = {317},
publisher = {Springer Berlin Heidelberg},
title = {{Systems and Software Variability Management}},
url = {http://link.springer.com/10.1007/978-3-642-36583-6},
year = {2013}
}
@book{VanDerLinden2007,
abstract = {Software product lines represent perhaps the most exciting paradigm shift in software development since the advent of high-level programming languages. Nowhere else in software engineering have we seen such breathtaking improvements in cost, quality, time to market, and developer productivity, often registering in the order-of-magnitude range. While the underlying concepts are straightforward enough - building a family of related products or systems by planned and careful reuse of a base of generalized software development assets - the devil can be in the details, as successful product line practice can involve organizational change, business process change, and technology change. The authors ideally combine academic research results with industrial real-world experiences, thus presenting a broad view on product line engineering so that both managers and technical specialists will benefit from reading it. After presenting a common framework for the description of the industrial case studies, they capture the wealth of knowledge that eight companies have gathered during the introduction of the software product line engineering approach in their daily practice. After reading this book, you will understand all the relevant aspects, regarding business, architecture, process, and organizational issues, of applying software product line engineering. If you consider using a product line approach in your organization, or if you want to improve your current practices you will find a rich set of useful information at your fingertips - from practitioners to practitioners.},
address = {Berlin, Heidelberg},
author = {van der Linden, Frank and Schmid, Klaus and Rommes, Eelco},
booktitle = {Software Product Lines in Action: The Best Industrial Practice in Product Line Engineering},
doi = {10.1007/978-3-540-71437-8},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/van der Linden, Schmid, Rommes - 2007 - Software Product Lines in Action.pdf:pdf},
isbn = {978-3-540-71436-1},
keywords = {gris},
mendeley-tags = {gris},
pages = {1--333},
publisher = {Springer Berlin Heidelberg},
title = {{Software Product Lines in Action}},
url = {http://link.springer.com/10.1007/978-3-540-71437-8},
year = {2007}
}
@misc{,
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Production Line to Frontline 001 North American P-51 Mustang.pdf.pdf:pdf},
title = {{Production Line to Frontline 001 North American P-51 Mustang.pdf}}
}
@book{Kang2010,
author = {Kang, Kc and Sugumaran, V and Park, S},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kang, Sugumaran, Park - 2010 - Applied software product line engineering.pdf:pdf},
isbn = {9781420068429},
pages = {561},
title = {{Applied software product line engineering}},
url = {http://books.google.com.mx/books/about/Applied{\_}Software{\_}Product{\_}Line{\_}Engineerin.html?id=7XO{\_}ghvkpt4C{\&}redir{\_}esc=y},
year = {2010}
}
@article{Berthier2007,
abstract = {Usar para Introducci{\'{o}}n a la Soc. de la Inf.},
author = {Berthier, a},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Berthier - 2007 - El sistema de referencias Harvard.pdf:pdf},
journal = {Conocimiento y Sociedad},
number = {2006},
pages = {1--11},
title = {{El sistema de referencias Harvard}},
url = {http://www.ucbcba.edu.bo/Documentos/El{\_}sistema{\_}de{\_}referencias{\_}Harvard.pdf},
year = {2007}
}
@book{Reinhartz-Berger2013,
address = {Berlin, Heidelberg},
author = {Reinhartz-Berger, Iris and Sturm, Arnon and Clark, Tony and Cohen, Sholom and Bettin, Jorn},
booktitle = {Domain Engineering: Product Lines, Languages, and Conceptual Models},
doi = {10.1007/978-3-642-36654-3},
editor = {Reinhartz-Berger, Iris and Sturm, Arnon and Clark, Tony and Cohen, Sholom and Bettin, Jorn},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Reinhartz-Berger et al. - 2013 - Domain Engineering.pdf:pdf},
isbn = {978-3-642-36653-6},
issn = {0471028959},
pages = {1--404},
publisher = {Springer Berlin Heidelberg},
title = {{Domain Engineering}},
url = {http://link.springer.com/10.1007/978-3-642-36654-3},
year = {2013}
}
@misc{Gomaa2004,
abstract = {Although smoking prevalence remains strikingly high in homeless populations ({\~{}}70{\%} and three times the US national average), smoking cessation studies usually exclude homeless persons. Novel evidence-based interventions are needed for this high-risk subpopulation of smokers.},
author = {Gomaa, Hassan},
booktitle = {Clinical trials London England},
doi = {10.1177/1740774511423947},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Gomaa - 2004 - Designing Software Product Lines with UML From Use Cases to Pattern-Based Software Architectures.chm:chm},
isbn = {0201775956},
issn = {17407753},
number = {6},
pages = {744--54},
pmid = {22167112},
title = {{Designing Software Product Lines with UML: From Use Cases to Pattern-Based Software Architectures}},
url = {http://www.amazon.com/dp/0201775956},
volume = {8},
year = {2004}
}
@misc{,
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Osprey - Production Line to Frontline 03 - P-38 Lightning.pdf.pdf:pdf},
title = {{Osprey - Production Line to Frontline  03 - P-38 Lightning.pdf}}
}
@book{NationalAeronauticsandSpaceAdministration2007,
booktitle = {Nasa},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2007 - NASA Systems Engineering Handbook.pdf:pdf},
isbn = {978-0-16-079747-7},
title = {{NASA Systems Engineering Handbook}},
year = {2007}
}
@article{Mazo2008,
author = {Mazo, Ra{\'{u}}l and Salinesi, Camille},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Mazo, Salinesi - 2008 - Methods, Techniques and Tools for Product Line Model Verification.pdf:pdf},
journal = {Techniques},
number = {September},
pages = {1--153},
title = {{Methods, Techniques and Tools for Product Line Model Verification}},
url = {http://hal-paris1.archives-ouvertes.fr/halshs-00323675/},
year = {2008}
}
@book{Simpson2006,
abstract = {Designing products and product families so they may be customized for the global marketplace and achieving these goals in abbreviated time period, while maintaining production efficiencies are the keys to successful manufacturing operations. The research on these areas has matured rapidly over the last decade. Today's highly competitive and volatile marketplace is reshaping the way many companies do business as rapid innovation and mass customization offer a new form of competitive advantage. In response, companies like Sony, Black and Decker, and Kodak have successfully implemented strategies to design and develop an entire family of products to satisfy a wide variety of customer requirements. Product Platform and Product Family Design: Methods and Applications discusses how product platform and product family design can be used successfully to: -increase variety within a product line, -shorten manufacturing lead times, - reduce overall costs within a product line.The material available here will serve as both a reference and a hands-on guide for practitioners involved in the design, planning and production of products. Included are real-life case studies that explain the benefits of platform based product development.},
address = {Boston, MA},
author = {Simpson, Timothy W. and Siddique, Zahed and Jiao, Jianxin},
booktitle = {Product Platform and Product Family Design: Methods and Applications},
doi = {10.1007/0-387-29197-0},
editor = {Simpson, Timothy W. and Siddique, Zahed and Jiao, Jianxin Roger},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Simpson, Siddique, Jiao - 2006 - Product Platform and Product Family Design.pdf:pdf},
isbn = {978-0-387-25721-1},
pages = {1--548},
publisher = {Springer US},
title = {{Product Platform and Product Family Design}},
url = {http://link.springer.com/10.1007/0-387-29197-0},
year = {2006}
}
@book{Pohl2010,
author = {Pohl, Klaus and B{\"{o}}ckle, G{\"{u}}nter and van der Linden, Frank},
booktitle = {Auerbach Publications, CRC press Taylor {\&} Francis Group},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pohl, B{\"{o}}ckle, Linden - 2010 - Software Product Line Engineering Foundations, Principles, and Techniques.pdf:pdf},
isbn = {1420068415, 9781420068412},
pages = {539},
title = {{Software Product Line Engineering Foundations, Principles, and Techniques}},
year = {2010}
}
@book{,
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2015 - REUTILIZACI{\'{O}}N Y LA INDUSTRIA DEL SOFTWARE Hacia la Aplicaci{\'{o}}n de la Industria del Software en Colombia.pdf:pdf},
isbn = {9789584658579},
title = {{REUTILIZACI{\'{O}}N Y LA INDUSTRIA DEL SOFTWARE Hacia la Aplicaci{\'{o}}n de la Industria del Software en Colombia}},
year = {2015}
}
@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
address = {New York, NY},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Elements},
doi = {10.1007/b94608},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning(2).pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
keywords = {verde},
mendeley-tags = {verde},
pages = {337--387},
pmid = {15512507},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
volume = {1},
year = {2009}
}
@article{Ramasubbu2012,
abstract = {This study develops and empirically tests the idea that the impact of structural complexity on perfective maintenance of object-oriented software is significantly determined by the team strategy of programmers (independent or collaborative). We analyzed two key dimensions of software structure, coupling and cohesion, with respect to the maintenance effort and the perceived ease-of-maintenance by pairs of programmers. Hypotheses based on the distributed cognition and task interdependence theoretical frameworks were tested using data collected from a controlled lab experiment employing professional programmers. The results show a significant interaction effect between coupling, cohesion, and programmer team strategy on both maintenance effort and perceived ease-of-maintenance. Highly cohesive and low-coupled programs required lower maintenance effort and were perceived to be easier to maintain than the low-cohesive programs and high-coupled programs. Further, our results would predict that managers who strategically allocate maintenance tasks to either independent or collaborative programming teams depending on the structural complexity of software could lower their team's maintenance effort by as much as 70 percent over managers who use simple uniform resource allocation policies. These results highlight the importance of achieving congruence between team strategies employed by collaborating programmers and the structural complexity of software. {\&}copy; 2012 IEEE.},
author = {Ramasubbu, Narayan and Kemerer, Chris F. and Hong, Jeff},
doi = {10.1109/TSE.2011.88},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ramasubbu, Kemerer, Hong - 2012 - Structural complexity and programmer team strategy An experimental test.pdf:pdf},
isbn = {10.1109/TSE.2011.88},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {CK metrics,Object-oriented programming,complexity measures,maintenance process,programming teams,software management,software productivity,software quality},
number = {5},
pages = {1--15},
title = {{Structural complexity and programmer team strategy: An experimental test}},
volume = {38},
year = {2012}
}
@book{James2013,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
address = {New York, NY},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
booktitle = {Performance Evaluation},
doi = {10.1007/978-1-4614-7138-7},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/James et al. - 2013 - An Introduction to Statistical Learning.pdf:pdf},
isbn = {978-1-4614-7137-0},
issn = {01665316},
keywords = {BitTorrent,P2P file sharing,Performance modeling},
month = {oct},
number = {9-12},
pages = {856--875},
pmid = {10911016},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{An Introduction to Statistical Learning}},
url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
volume = {103},
year = {2013}
}
@book{Hastie2009a,
address = {New York, NY},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Neural Networks},
doi = {10.1007/978-0-387-84858-7},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {978-0-387-84857-0},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
url = {http://link.springer.com/10.1007/978-0-387-84858-7},
year = {2009}
}
@book{Izenman2006,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
address = {New York, NY},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Izenman, Alan J.},
booktitle = {Performance Evaluation},
doi = {10.1007/978-0-387-78189-1},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Izenman - 2008 - Modern Multivariate Statistical Techniques.pdf:pdf},
isbn = {978-0-387-78188-4},
issn = {01665316},
keywords = {rojo},
mendeley-tags = {rojo},
month = {oct},
number = {9-12},
pages = {856--875},
pmid = {10911016},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{Modern Multivariate Statistical Techniques}},
url = {http://books.google.com/books?id=9tv0taI8l6YC http://linkinghub.elsevier.com/retrieve/pii/S0166531607000570 http://link.springer.com/10.1007/978-0-387-78189-1},
volume = {64},
year = {2008}
}
@article{Apel2009,
abstract = {Feature-oriented software development (FOSD) is a paradigm for the construction, customization, and synthesis of large-scale software systems. In this survey, we give an overview and a personal perspective on the roots of FOSD, connections to other software development paradigms, and recent developments in this field. Our aim is to point to connections between different lines of research and to identify open issues.},
author = {Apel, Sven and K{\"{a}}stner, Christian},
doi = {10.5381/jot.2009.8.5.c5},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Apel, K{\"{a}}stner - 2009 - An Overview of Feature-Oriented Software Development.pdf:pdf},
issn = {1660-1769},
journal = {The Journal of Object Technology},
number = {5},
pages = {49},
title = {{An Overview of Feature-Oriented Software Development.}},
url = {http://www.jot.fm/contents/issue{\_}2009{\_}07/column5.html},
volume = {8},
year = {2009}
}
@article{Hornick2009,
author = {Hornick, Mark F. and Marcad{\'{e}}, Erik and Venkayala, Sunil},
doi = {10.1016/B978-0-7020-2797-0.00001-1},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hornick, Marcad{\'{e}}, Venkayala - 2009 - Chapter 1.pdf:pdf},
isbn = {1111001111},
issn = {15710661},
journal = {Java Data Mining},
pages = {1--4},
pmid = {20314319},
title = {{Chapter 1}},
volume = {1},
year = {2009}
}
@incollection{Hornick2007,
abstract = {This chapter introduces the CRISP-DM standard data mining process and characterizes how JDM supports the various phases of this process. This chapter discusses data analysis and preparation in great detail and explains what to look for in data and how to address typical data quality issues. As modeling is the main focus of JDM, we explore three principal tasks—model builds, model test, and model apply. In preparation for the discussion on enterprise software architectures, this chapter discusses the role of databases and data warehouses on data mining. This chapter characterizes the architectures of data mining tools and their interplay with file systems and databases, and further explores the larger scale enterprise system involving data mining and how workflow can be used to include mining tasks in the enterprise. A standardized data mining process is explained in detail that involves a number of phases including business understanding, data understanding, data preparation, modeling, evaluation, and deployment.},
author = {Hornick, Mark F. and Marcad{\'{e}}, Erik and Venkayala, Sunil},
booktitle = {Java Data Mining},
doi = {10.1016/B978-012370452-8/50029-3},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hornick, Marcad{\'{e}}, Venkayala - 2007 - Data Mining Process(2).pdf:pdf},
isbn = {9780123704528},
pages = {51--83},
publisher = {Elsevier},
title = {{Data Mining Process}},
url = {http://www.sciencedirect.com/science/article/pii/B9780123704528500293 http://linkinghub.elsevier.com/retrieve/pii/B9780123704528500293},
year = {2007}
}
@article{Hornick2007a,
abstract = {This chapter introduces the CRISP-DM standard data mining process and characterizes how JDM supports the various phases of this process. This chapter discusses data analysis and preparation in great detail and explains what to look for in data and how to address typical data quality issues. As modeling is the main focus of JDM, we explore three principal tasks—model builds, model test, and model apply. In preparation for the discussion on enterprise software architectures, this chapter discusses the role of databases and data warehouses on data mining. This chapter characterizes the architectures of data mining tools and their interplay with file systems and databases, and further explores the larger scale enterprise system involving data mining and how workflow can be used to include mining tasks in the enterprise. A standardized data mining process is explained in detail that involves a number of phases including business understanding, data understanding, data preparation, modeling, evaluation, and deployment.},
author = {Hornick, Mark F. and Marcad{\'{e}}, Erik and Venkayala, Sunil},
doi = {10.1016/B978-012370452-8/50029-4},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hornick, Marcad{\'{e}}, Venkayala - 2007 - Data mining process.pdf:pdf},
isbn = {9780123704529},
journal = {Java Data Mining: Strategy, Standard, and Practice A Practical Guide for architecture, design, and implementation},
pages = {51--83},
title = {{Data mining process}},
url = {http://www.sciencedirect.com/science/article/pii/B9780123704528500293},
year = {2007}
}
@article{Elovici2003,
author = {Elovici, Yuval and Braha, Dan},
doi = {10.1109/TSMCA.2003.812596},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Elovici, Braha - 2003 - A decision-theoretic approach to data mining.pdf:pdf},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
month = {jan},
number = {1},
pages = {42--51},
title = {{A decision-theoretic approach to data mining}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1206454 http://ieeexplore.ieee.org/document/1206454/},
volume = {33},
year = {2003}
}
@article{Comunicaciones2014,
author = {las Comunicaciones, Ministerio de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y},
keywords = {MINTIC},
pages = {177},
title = {{INFORME RENDICI{\'{O}}N DE CUENTAS Ministerio de Tecnolog{\'{i}}as de la Información y las Comunicaciones – Fondo de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y las Comunicaciones}},
url = {http://www.mintic.gov.co/portal/604/articles-6423{\_}recurso{\_}5.pdf},
year = {2014}
}
@article{Wang1998,
abstract = {In process plant operation and control, modern distributed control and automatic data logging systems create large volumes of data that contain valuable information about normal and abnormal operations, significant disturbances, and changes in operational and control strategies. These data have tended to be underexploited for a variety of reasons, including the large volume and lack of effective automatic computer-based support tools. This paper considers a data mining system that is able to automatically cluster the data into classes corresponding to various operational modes and thereby provide some structure for analysis of behavioral responses. The method is illustrated by reference to a case study of a refinery fluid catalytic cracking process.},
author = {Wang, X Z and McGreavy, C},
doi = {10.1021/ie970620h},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wang, McGreavy - 1998 - Automatic Classification for Mining Process Operational Data.pdf:pdf},
isbn = {0888-5885},
issn = {0888-5885},
journal = {Industrial {\&} Engineering Chemistry Research},
month = {jun},
number = {6},
pages = {2215--2222},
title = {{Automatic Classification for Mining Process Operational Data}},
url = {http://dx.doi.org/10.1021/ie970620h http://pubs.acs.org/doi/abs/10.1021/ie970620h},
volume = {37},
year = {1998}
}
@article{Choudhary2009,
abstract = {In modern manufacturing environments, vast amounts of data are collected in database management systems and data warehouses from all involved areas, including product and process design, assembly, materials planning, quality control, scheduling, maintenance, fault detection etc. Data mining has emerged as an important tool for knowledge acquisition from the manufacturing databases. This paper reviews the literature dealing with knowledge discovery and data mining applications in the broad domain of manufacturing with a special emphasis on the type of functions to be performed on the data. The major data mining functions to be performed include characterization and description, association, classification, prediction, clustering and evolution analysis. The papers reviewed have therefore been categorized in these five categories. It has been shown that there is a rapid growth in the application of data mining in the context of manufacturing processes and enterprises in the last 3 years. This review reveals the progressive applications and existing gaps identified in the context of data mining in manufacturing. A novel text mining approach has also been used on the abstracts and keywords of 150 papers to identify the research gaps and find the linkages between knowledge area, knowledge type and the applied data mining tools and techniques.},
author = {Choudhary, a. K. and Harding, J. a. and Tiwari, M. K.},
doi = {10.1007/s10845-008-0145-x},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Choudhary, Harding, Tiwari - 2009 - Data mining in manufacturing a review based on the kind of knowledge.pdf:pdf},
isbn = {0956-5515},
issn = {0956-5515},
journal = {Journal of Intelligent Manufacturing},
keywords = {Data mining,Knowledge discovery,Literature review,Manufacturing,Text mining},
month = {oct},
number = {5},
pages = {501--521},
title = {{Data mining in manufacturing: a review based on the kind of knowledge}},
url = {http://link.springer.com/10.1007/s10845-008-0145-x},
volume = {20},
year = {2009}
}
@article{,
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Dynamic Product Lines.pdf:pdf},
title = {{Dynamic Product Lines}}
}
@article{Breiman1984,
author = {Moore, Dan H.},
doi = {10.1002/cyto.990080516},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Moore - 1987 - Classification and regression trees, by Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. BrooksC.pdf:pdf},
issn = {0196-4763},
journal = {Cytometry},
month = {sep},
number = {5},
pages = {534--535},
title = {{Classification and regression trees, by Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. Brooks/Cole Publishing, Monterey, 1984,358 pages, {\$}27.95}},
url = {http://doi.wiley.com/10.1002/cyto.990080516},
volume = {8},
year = {1987}
}
@incollection{Ritschard2010,
address = {Gen{\`{e}}ve},
author = {Ritschard, Gilbert},
edition = {1},
editor = {{Universit{\'{e}} de Gen{\`{e}}ve}},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ritschard - 2010 - CHAID and Earlier Supervised Tree Methods.pdf:pdf},
keywords = {AID,CHAID,Classification tree,ELISEE.,THAID,recursive partitioning,regression tree},
pages = {30},
publisher = {D{\'{e}}partement d'{\'{e}}conom{\'{e}}trie},
title = {{CHAID and Earlier Supervised Tree Methods}},
url = {http://www.unige.ch/ses/metri/},
year = {2010}
}
@article{Rolland1998,
abstract = {The requirements engineering, information systems and software engineering communities recently advocated scenario-based approaches which emphasise the user/system interaction perspective in developing computer systems. Use of examples, scenes, narrative descriptions of contexts, mock-ups and prototypes-all these ideas can be called scenario-based approaches, although exact definitions are not easy beyond stating that these approaches emphasise some description of the real world. Experience seems to tell us that people react to ‘real things' and that this helps in clarifying requirements. Indeed, the widespread acceptance of prototyping in system development points to the effectiveness of scenario-based approaches. However, we have little understanding about how scenarios should be constructed, little hard evidence about their effectiveness and even less idea about why they work. The paper is an attempt to explore some of the issues underlying scenario-based approaches in requirements engineering and to propose a framework for their classification. The framework is a four-dimensional framework which advocates that a scenario-based approach can be well defined by itsform, content, purpose andlife cycle. Every dimension is itself multifaceted and a metric is associated with each facet. Motivations for developing the framework are threefold: (a) to help in understanding and clarifying existing scenario-based approaches; (b) to situate the industrial practice of scenarios; and (c) to assist researchers develop more innovative scenario-based approaches.},
author = {Rolland, C. and {Ben Achour}, C. and Cauvet, C. and Ralyt{\'{e}}, J. and Sutcliffe, a. and Maiden, N. and Jarke, M. and Haumer, P. and Pohl, K. and Dubois, E. and Heymans, P.},
doi = {10.1007/BF02802919},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Rolland et al. - 1998 - A proposal for a scenario classification framework.pdf:pdf},
isbn = {0947-3602},
issn = {0947-3602},
journal = {Requirements Engineering},
number = {1},
pages = {23--47},
title = {{A proposal for a scenario classification framework}},
volume = {3},
year = {1998}
}
@article{Souag2015,
author = {Souag, Amina and Mazo, Ra{\'{u}}l and Salinesi, Camille and Comyn-Wattiau, Isabelle},
doi = {10.1007/s00766-015-0220-8},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Souag et al. - 2016 - Reusable knowledge in security requirements engineering a systematic mapping study.pdf:pdf},
issn = {0947-3602},
journal = {Requirements Engineering},
keywords = {verde},
mendeley-tags = {verde},
month = {jun},
number = {2},
pages = {251--283},
title = {{Reusable knowledge in security requirements engineering: a systematic mapping study}},
url = {http://link.springer.com/10.1007/s00766-015-0220-8},
volume = {21},
year = {2016}
}
@article{Kitchenham2011,
abstract = {Context: We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research. Objective: This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic. Method: We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies. Results: Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers. Conclusion: Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Kitchenham, Barbara a. and Budgen, David and {Pearl Brereton}, O.},
doi = {10.1016/j.infsof.2010.12.011},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kitchenham, Budgen, Pearl Brereton - 2011 - Using mapping studies as the basis for further research - A participant-observer case study.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Case study,Mapping studies,Software engineering,Systematic literature review},
number = {6},
pages = {638--651},
publisher = {Elsevier B.V.},
title = {{Using mapping studies as the basis for further research - A participant-observer case study}},
url = {http://dx.doi.org/10.1016/j.infsof.2010.12.011},
volume = {53},
year = {2011}
}
@inproceedings{Wohlin2014,
address = {New York, New York, USA},
author = {Wohlin, Claes},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering - EASE '14},
doi = {10.1145/2601248.2601268},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wohlin - 2014 - Guidelines for snowballing in systematic literature studies and a replication in software engineering.pdf:pdf},
isbn = {9781450324762},
keywords = {replication,snowball search,snowballing,systematic,systematic literature review},
pages = {1--10},
publisher = {ACM Press},
title = {{Guidelines for snowballing in systematic literature studies and a replication in software engineering}},
url = {http://dl.acm.org/citation.cfm?doid=2601248.2601268},
year = {2014}
}
@article{Wieringa2006,
abstract = {The article presents the outcome of the discussions by members of the steering committee of the International Electrical and Electronic Engineering Requirements Engineering (RE) Conference, regarding paper classification and evaluation criteria for RE papers. Section two of the article sketches the rationale for the classification. Section three presents the classification, and section four concludes with a discussion of background ideas and related work.},
author = {Wieringa, Roel and Maiden, Neil and Mead, Nancy and Rolland, Colette},
doi = {10.1007/s00766-005-0021-6},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wieringa et al. - 2006 - Requirements engineering paper classification and evaluation criteria A proposal and a discussion.pdf:pdf},
isbn = {0947-3602},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Paper classification,Paper evaluation criteria,Requirements engineering research,Research methods},
number = {1},
pages = {102--107},
title = {{Requirements engineering paper classification and evaluation criteria: A proposal and a discussion}},
volume = {11},
year = {2006}
}
@article{Petersen2008,
author = {MAREW, TEGEGNE and KIM, JUNGYOON and BAE, DOO HWAN},
doi = {10.1142/S0218194007003112},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/MAREW, KIM, BAE - 2007 - SYSTEMATIC FUNCTIONAL DECOMPOSITION IN A PRODUCT LINE USING ASPECT-ORIENTED SOFTWARE DEVELOPMENT A CASE STUDY.pdf:pdf},
isbn = {0-7695-2555-5},
issn = {0218-1940},
journal = {International Journal of Software Engineering and Knowledge Engineering},
keywords = {evidence based software engineering,systematic mapping studies,systematic reviews},
month = {feb},
number = {01},
pages = {33--55},
title = {{SYSTEMATIC FUNCTIONAL DECOMPOSITION IN A PRODUCT LINE USING ASPECT-ORIENTED SOFTWARE DEVELOPMENT: A CASE STUDY}},
url = {http://robertfeldt.net/publications/petersen{\_}ease08{\_}sysmap{\_}studies{\_}in{\_}se.pdf http://www.worldscientific.com/doi/abs/10.1142/S0218194007003112},
volume = {17},
year = {2007}
}
@article{Kitchenham2007,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
author = {Kitchenham, Barbara and Charters, S},
doi = {10.1145/1134285.1134500},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kitchenham, Charters - 2007 - Guidelines for performing Systematic Literature Reviews in Software Engineering.pdf:pdf},
isbn = {1595933751},
issn = {00010782},
journal = {Engineering},
pages = {1051},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
volume = {2},
year = {2007}
}
@article{Zhang2011,
abstract = {Context: Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective: The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method: We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. Results: We conducted two participant-observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion: We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Zhang, He and Babar, Muhammad Ali and Tell, Paolo},
doi = {10.1016/j.infsof.2010.12.010},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Babar, Tell - 2011 - Identifying relevant studies in software engineering.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Evidence-based software engineering,Quasi-gold standard,Search strategy,Systematic literature review},
month = {jun},
number = {6},
pages = {625--637},
publisher = {Elsevier B.V.},
title = {{Identifying relevant studies in software engineering}},
url = {http://dx.doi.org/10.1016/j.infsof.2010.12.010 http://linkinghub.elsevier.com/retrieve/pii/S0950584910002260},
volume = {53},
year = {2011}
}
@article{Li2015,
author = {Li, Peipei and Wu, Xindong and Hu, Xuegang and Wang, Hao},
doi = {10.1016/j.neucom.2015.04.024},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2015 - Learning concept-drifting data streams with random ensemble decision trees.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Concept drift,Data streams,Noisy data,Random decision tree},
month = {oct},
pages = {68--83},
publisher = {Elsevier},
title = {{Learning concept-drifting data streams with random ensemble decision trees}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215004713},
volume = {166},
year = {2015}
}
@inproceedings{Jiang2008,
abstract = {Improved product quality and accelerated software development through systematic reuse of common software assets have been the key attractions behind software product lines. Magnitude of improvement in industrial practices has been reported in the literature. However, experiences with software product line also showed that it is a rather challenging task to maintain software product lines and families over a long period of time. The time and resources needed to manage and maintain product lines increase and quality degrades as product lines evolve. This paper describes an industrial practice of software product line maintenance and evolution. The conventional software maintenance process is enhanced with data mining techniques to uncover lost reuse pattern and defects, maintain reuse, and reduce design erosion of product lines. Case studies with mobile phone product lines are described.},
author = {Jiang, Michael and Zhang, Jing and Zhao, Hong and Zhou, Yuanyuan},
booktitle = {2008 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2008.4658100},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Jiang et al. - 2008 - Maintaining software product lines {\&}{\#}x2014 an industrial practice.pdf:pdf},
isbn = {978-1-4244-2613-3},
issn = {1063-6773},
keywords = {[Electronic Manuscript]},
month = {sep},
pages = {444--447},
publisher = {IEEE},
title = {{Maintaining software product lines {\&}{\#}x2014; an industrial practice}},
url = {http://ieeexplore.ieee.org/document/4658100/},
year = {2008}
}
@techreport{Ardila2015,
address = {Bogot{\'{a}}},
author = {Ardila, Diego Silva},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ardila - 2015 - Indicadores B{\'{a}}sicos de Tenencia y Uso de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y Comunicaci{\'{o}}n en empresas 2013 Cifras Defini.pdf:pdf},
institution = {DANE},
pages = {1--64},
title = {{Indicadores B{\'{a}}sicos de Tenencia y Uso de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y Comunicaci{\'{o}}n en empresas 2013 Cifras Definitivas}},
year = {2015}
}
@article{Quinlan1986,
abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions. {\textcopyright} 1986 Kluwer Academic Publishers.},
author = {Quinlan, J. R.},
doi = {10.1007/BF00116251},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {classification,decision trees,expert systems,induction,information theory,knowledge acquisition},
month = {mar},
number = {1},
pages = {81--106},
publisher = {Kluwer Academic Publishers},
title = {{Induction of decision trees}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-33744584654{\&}partnerID=tZOtx3y1},
volume = {1},
year = {1986}
}
@article{Benavides2010,
author = {Benavides, David and Segura, Sergio and Ruiz-Cort{\'{e}}s, Antonio},
chapter = {gris},
doi = {10.1016/j.is.2010.01.001},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Benavides, Segura, Ruiz-Cort{\'{e}}s - 2010 - Automated analysis of feature models 20 years later A literature review.pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {automated analyses,feature models,literature review,software product lines},
month = {sep},
number = {6},
pages = {615--636},
title = {{Automated analysis of feature models 20 years later: A literature review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0306437910000025},
volume = {35},
year = {2010}
}
@article{Lin2013,
abstract = {Multiple-generation product lines require carefully planned strategies in order to reap the benefits of utilizing technology assets and resources efficiently over an elongated time span. In this paper, we build on our previous work in which we successfully implemented dynamic variable state models (DVSMs) to forecast the introduction timing of future generations for an existing multiple-generation product line. Here we implement DVSMs for the design of a new multiple-generation product line. We investigate the potential for using historical sales data about similar products to generate a complete set of forecasts and relevant strategic moves using DVSMs. We present a case study implementing the proposed framework on Apple Inc.'s iPad product line. Results show that the forecast performance of the model matches the realized real data, and hence we deem the DSVM to be appropriate for modeling a new multi-generation product line. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Lin, Chun-Yu and Okudan, G{\"{u}}l E.},
doi = {10.1016/j.eswa.2012.10.022},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Okudan - 2013 - Planning for multiple-generation product lines using dynamic variable state models with data input from similar pro.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Dynamic state variable models,Multiple generation product lines,purpura},
mendeley-tags = {purpura},
month = {may},
number = {6},
pages = {2013--2022},
publisher = {Elsevier Ltd},
title = {{Planning for multiple-generation product lines using dynamic variable state models with data input from similar products}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.10.022 http://linkinghub.elsevier.com/retrieve/pii/S0957417412011360},
volume = {40},
year = {2013}
}
@misc{Sayyad2012,
abstract = {Feature Models are popular tools for describing software product lines. Analysis of feature models has traditionally focused on consistency checking (yielding a yes/no answer) and product selection assistance, interactive or offline. In this paper, we describe a novel approach to identify the most critical decisions in product selection/configuration by taking advantage of a large pool of randomly generated, generally inconsistent, product variants. Range Ranking, a data mining technique, is utilized to single out the most critical design choices, reducing the job of the human designer to making less consequential decisions. A large feature model is used as a case study; we show preliminary results of the new approach to illustrate its usefulness for practical product derivation.},
author = {Sayyad, A S and Ammar, H and Menzies, T},
booktitle = {Recommendation Systems for Software Engineering (RSSE), 2012 Third International Workshop on},
doi = {10.1109/RSSE.2012.6233409},
isbn = {VO  -},
keywords = {Analytical models,Business,Computational modeling,Data mining,Feature Models,Mobile handsets,Software,USA Councils,consistency checking,data mining,design choices,design decisions,product configuration,product derivation,product design,product selection assistance,range ranking,software feature model recommendation,software management,software product line},
pages = {47--51},
title = {{Software Feature Model recommendations using data mining}},
year = {2012}
}
@article{Poria2015,
abstract = {A huge number of videos are posted every day on social media platforms such as Facebook and YouTube. This makes the Internet an unlimited source of information. In the coming decades, coping with such information and mining useful knowledge from it will be an increasingly difficult task. In this paper, we propose a novel methodology for multimodal sentiment analysis, which consists in harvesting sentiments from Web videos by demonstrating a model that uses audio, visual and textual modalities as sources of information. We used both feature- and decision-level fusion methods to merge affective information extracted from multiple modalities. A thorough comparison with existing works in this area is carried out throughout the paper, which demonstrates the novelty of our approach. Preliminary comparative experiments with the YouTube dataset show that the proposed multimodal system achieves an accuracy of nearly 80{\%}, outperforming all state-of-the-art systems by more than 20{\%}.},
author = {Poria, Soujanya and Cambria, Erik and Howard, Newton and Huang, Guang-Bin and Hussain, Amir},
doi = {10.1016/j.neucom.2015.01.095},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Poria et al. - 2016 - Fusing audio, visual and textual clues for sentiment analysis from multimodal content.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Big social data analysis,Emotions,Extreme learning machines,Multimodal fusion,Multimodal sentiment analysis,Opinion mining,Sentiment analysis},
month = {jan},
pages = {50--59},
title = {{Fusing audio, visual and textual clues for sentiment analysis from multimodal content}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231215011297 http://linkinghub.elsevier.com/retrieve/pii/S0925231215011297},
volume = {174},
year = {2016}
}
@article{Atzmueller2015,
abstract = {Communities can intuitively be defined as subsets of nodes of a graph with a dense structure in the corresponding subgraph. However, for mining such communities usually only structural aspects are taken into account. Typically, no concise nor easily interpretable community description is provided. For tackling this issue, this paper focuses on description-oriented community detection using subgroup discovery. In order to provide both structurally valid and interpretable communities we utilize the graph structure as well as additional descriptive features of the graph's nodes. A descriptive community pattern built upon these features then describes and identifies a community, i.e., a set of nodes, and vice versa. Essentially, we mine patterns in the “description space” characterizing interesting sets of nodes (i.e., subgroups) in the “graph space”; the interestingness of a community is evaluated by a selectable quality measure. We aim at identifying communities according to standard community quality measures, while providing characteristic descriptions of these communities at the same time. For this task, we propose several optimistic estimates of standard community quality functions to be used for efficient pruning of the search space in an exhaustive branch-and-bound algorithm. We demonstrate our approach in an evaluation using five real-world data sets, obtained from three different social media applications.},
author = {Atzmueller, Martin and Doerfel, Stephan and Mitzlaff, Folke},
doi = {10.1016/j.ins.2015.05.008},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Atzmueller, Doerfel, Mitzlaff - 2015 - Description-oriented community detection using exhaustive subgroup discovery.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Community detection,Exceptional model mining,Exploratory data analysis,Optimistic estimates,Social network analysis,Subgroup discovery},
month = {may},
title = {{Description-oriented community detection using exhaustive subgroup discovery}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025515003667},
year = {2015}
}
@article{Sasikala2014,
abstract = {Selection of optimal features is an important area of research in medical data mining systems. In this paper we introduce an efficient four-stage procedure – feature extraction, feature subset selection, feature ranking and classification, called as Multi-Filtration Feature Selection (MFFS), for an investigation on the improvement of detection accuracy and optimal feature subset selection. The proposed method adjusts a parameter named “variance coverage” and builds the model with the value at which maximum classification accuracy is obtained. This facilitates the selection of a compact set of superior features, remarkably at a very low cost. An extensive experimental comparison of the proposed method and other methods using four different classifiers (Na{\"{i}}ve Bayes (NB), Support Vector Machine (SVM), multi layer perceptron (MLP) and J48 decision tree) and 22 different medical data sets confirm that the proposed MFFS strategy yields promising results on feature selection and classification accuracy for medical data mining field of research.},
author = {Sasikala, S. and {Appavu alias Balamurugan}, S. and Geetha, S.},
doi = {10.1016/j.aci.2014.03.002},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Sasikala, Appavu alias Balamurugan, Geetha - 2016 - Multi Filtration Feature Selection (MFFS) to improve discriminatory ability in clini.pdf:pdf},
issn = {22108327},
journal = {Applied Computing and Informatics},
keywords = {Biomedical classification,Medical data mining,Multi Filtration Feature Selection,Principal Component Analysis,Variance coverage factor,naranja,rojo},
mendeley-tags = {naranja,rojo},
month = {jul},
number = {2},
pages = {117--127},
title = {{Multi Filtration Feature Selection (MFFS) to improve discriminatory ability in clinical data set}},
url = {http://www.sciencedirect.com/science/article/pii/S2210832714000088 http://linkinghub.elsevier.com/retrieve/pii/S2210832714000088},
volume = {12},
year = {2016}
}
@article{Cheng2015,
abstract = {Process mining techniques are designed to read process logs and extract process models from them. However, real world logs are often noisy and such logs produce bad, spaghetti-like process models. We propose a technique to sanitize noisy logs by first building a classifier on a subset of the log, and applying the classifier rules to remove noisy traces from the log. The improvement in the quality of the resulting process models is evaluated on synthetic logs from benchmark models of increasing complexity on both behavioral and structural recall and precision metrics. The results show that mined models produced from such preprocessed logs are superior on several evaluation metrics. They show better fidelity to the reference models, and are also more compact with fewer elements. A nice feature of the rule based approach is that it generalizes to any noise pattern since the nature of noise varies from one log to another. The rules can also be explained and may be further modified manually. We also give results from experiments with a real dataset.},
author = {Cheng, Hsin-Jung and Kumar, Akhil},
doi = {10.1016/j.dss.2015.08.003},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Cheng, Kumar - 2015 - Process Mining on Noisy Logs – Can log sanitization help to improve performance.pdf:pdf},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {benchmarking,log sanitization,metrics,noisy data,process mining,rules},
month = {aug},
title = {{Process Mining on Noisy Logs – Can log sanitization help to improve performance?}},
url = {http://www.sciencedirect.com/science/article/pii/S0167923615001566},
year = {2015}
}
@article{Xue2013,
abstract = {Model reduction is an important systems task with a long history in traditional chemical engineering modeling. We discuss its interplay with modern data-mining tools (such as Local Feature Analysis and Diffusion Maps) through illustrative examples, and comment on important open issues regarding applications to large systems arising in molecular/atomistic simulations.},
author = {Xue, Yuzhen and Ludovice, Peter J. and Grover, Martha A. and Nedialkova, Lilia V. and Dsilva, Carmeline J. and Kevrekidis, Ioannis G.},
doi = {10.1016/j.compchemeng.2012.06.029},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Xue et al. - 2013 - State reduction in molecular simulations.pdf:pdf},
issn = {00981354},
journal = {Computers {\&} Chemical Engineering},
keywords = {Data mining,Diffusion Maps,Local Feature Analysis,Model reduction,Principal Component Analysis,rojo},
mendeley-tags = {rojo},
month = {apr},
pages = {102--110},
title = {{State reduction in molecular simulations}},
url = {http://www.sciencedirect.com/science/article/pii/S009813541200213X http://linkinghub.elsevier.com/retrieve/pii/S009813541200213X},
volume = {51},
year = {2013}
}
@article{Chen2015,
abstract = {OBJECTIVE: Data in electronic health records (EHRs) is being increasingly leveraged for secondary uses, ranging from biomedical association studies to comparative effectiveness. To perform studies at scale and transfer knowledge from one institution to another in a meaningful way, we need to harmonize the phenotypes in such systems. Traditionally, this has been accomplished through expert specification of phenotypes via standardized terminologies, such as billing codes. However, this approach may be biased by the experience and expectations of the experts, as well as the vocabulary used to describe such patients. The goal of this work is to develop a data-driven strategy to (1) infer phenotypic topics within patient populations and (2) assess the degree to which such topics facilitate a mapping across populations in disparate healthcare systems.

METHODS: We adapt a generative topic modeling strategy, based on latent Dirichlet allocation, to infer phenotypic topics. We utilize a variance analysis to assess the projection of a patient population from one healthcare system onto the topics learned from another system. The consistency of learned phenotypic topics was evaluated using (1) the similarity of topics, (2) the stability of a patient population across topics, and (3) the transferability of a topic across sites. We evaluated our approaches using four months of inpatient data from two geographically distinct healthcare systems: (1) Northwestern Memorial Hospital (NMH) and (2) Vanderbilt University Medical Center (VUMC).

RESULTS: The method learned 25 phenotypic topics from each healthcare system. The average cosine similarity between matched topics across the two sites was 0.39, a remarkably high value given the very high dimensionality of the feature space. The average stability of VUMC and NMH patients across the topics of two sites was 0.988 and 0.812, respectively, as measured by the Pearson correlation coefficient. Also the VUMC and NMH topics have smaller variance of characterizing patient population of two sites than standard clinical terminologies (e.g., ICD9), suggesting they may be more reliably transferred across hospital systems.

CONCLUSIONS: Phenotypic topics learned from EHR data can be more stable and transferable than billing codes for characterizing the general status of a patient population. This suggests that EHR-based research may be able to leverage such phenotypic topics as variables when pooling patient populations in predictive models.},
author = {Chen, You and Ghosh, Joydeep and Bejan, Cosmin Adrian and Gunter, Carl A and Gupta, Siddharth and Kho, Abel and Liebovitz, David and Sun, Jimeng and Denny, Joshua and Malin, Bradley},
doi = {10.1016/j.jbi.2015.03.011},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2015 - Building bridges across electronic health record systems through inferred phenotypic topics.pdf:pdf},
issn = {1532-0480},
journal = {Journal of biomedical informatics},
keywords = {Clinical phenotype modeling,Computers and information processing,Data mining,Electronic medical records,Medical information systems,Pattern recognition},
month = {jun},
pages = {82--93},
pmid = {25841328},
title = {{Building bridges across electronic health record systems through inferred phenotypic topics.}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046415000544},
volume = {55},
year = {2015}
}
@article{Hwang2015,
abstract = {The taxi fleet management systems based on GPS have become an important tool for taxi businesses. Such systems can be used not only for fleet management, but also to provide useful information for taxi drivers to increase their profits by mining historical GPS trajectories. In this paper, we propose a taxi recommender system for determining the next cruising location, which could be a value-added module in fleet management systems. In the literature, three factors have been considered in different studies to address a similar objective: distance between the current location and the recommended location, waiting time for the next passengers, and expected fare for the trip. In this paper, in addition to these factors, we consider one key factor based on driver experience: what is the most likely location to pick up passengers, given the current passenger drop off location. A location-to-location graph model, referred to as an OFF–ON model, is adopted to capture the relation between the passenger drop-off location and the next passenger get-on location. We also adopt an ON–OFF model to estimate the expected fare for a trip that begins at a recommended location. A real-world dataset from CRAWDAD is used to evaluate the proposed system. A simulator that simulates the cruising behavior of taxies in the dataset and a virtual taxi that cruises based on our recommender system is developed. Our simulation results indicate that although the statistics of the historical data may be different from real-time passenger requests, our recommender system is still effective in terms of recommending more profitable cruising locations.},
author = {Hwang, Ren-Hung and Hsueh, Yu-Ling and Chen, Yu-Ting},
doi = {10.1016/j.ins.2015.03.068},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hwang, Hsueh, Chen - 2015 - An effective taxi recommender system based on a spatio-temporal factor analysis model.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Data mining,GPS data analysis,Location-based services},
month = {sep},
pages = {28--40},
title = {{An effective taxi recommender system based on a spatio-temporal factor analysis model}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025515002418},
volume = {314},
year = {2015}
}
@article{Nanculef2014,
abstract = {We present a method for the classification of multi-labeled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labeled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labeled streams.},
author = {{\~{N}}anculef, Ricardo and Flaounas, Ilias and Cristianini, Nello},
doi = {10.1016/j.eswa.2014.02.017},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/{\~{N}}anculef, Flaounas, Cristianini - 2014 - Efficient classification of multi-labeled text streams by clashing.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Data streams,Feature hashing,Massive data mining,Multi-label classification,Text classification,azul},
mendeley-tags = {azul},
month = {sep},
number = {11},
pages = {5431--5450},
title = {{Efficient classification of multi-labeled text streams by clashing}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414000803},
volume = {41},
year = {2014}
}
@article{Bouguessa2015,
abstract = {Outlier detection in mixed-attribute space is a challenging problem for which only a few approaches have been proposed. However, such existing methods suffer from the fact that there is a lack of an automatic mechanism to formally discriminate between outliers and inliers. In fact, a common approach to outlier identification is to estimate an outlier score for each object and then provide a ranked list of points, expecting outliers to come first. A major problem of such an approach is where to stop reading the ranked list? How many points should be chosen as outliers? Other methods, instead of outlier ranking, implement various strategies that depend on user-specified thresholds to discriminate outliers from inliers. Ad-hoc threshold values are often used. With such an unprincipled approach it is impossible to be objective or consistent. To alleviate these problems, we propose a principled approach based on the bivariate beta mixture model to identify outliers in mixed-attribute data. The proposed approach is able to automatically discriminate outliers from inliers and it can be applied to both mixed-type attribute and single-type (numerical or categorical) attribute data without any feature transformation. Our experimental study demonstrates the suitability of the proposed approach in comparison to mainstream methods.},
author = {Bouguessa, Mohamed},
doi = {10.1016/j.eswa.2015.07.018},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bouguessa - 2015 - A practical outlier detection approach for mixed-attribute data.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Bivariate beta,Data mining,Mixed-attribute data,Mixture model,Outlier detection},
month = {jul},
title = {{A practical outlier detection approach for mixed-attribute data}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417415004789},
year = {2015}
}
@book{Luisi2014,
abstract = {This part enters into the main territory of enterprise architecture for information systems which is as rich in technology specialties as the IT ecosystem is diverse. Most organizations fail to recognize the need for diverse specialization within architecture because they fail to understand the depth of complexity and the costs associated with mediocrity within each area of specialization. They also believe that a general practioner, which we will call a solution architect, is qualified and appropriate to address the complexities across a wide array of technology areas. In reality, this is equivalent to staffing a medical center primarily with general practioners that act as the specialists. A healthy organization maintains top specialists with which the general practioners can participate in getting expertise that is in alignment with a future state vision that reduces complexity and costs.},
author = {Luisi, James V.},
booktitle = {Pragmatic Enterprise Architecture},
doi = {10.1016/B978-0-12-800205-6.00003-2},
isbn = {9780128002056},
keywords = {10-K,10-Q,20-F,6-K,8-K,ACID,AGLC,AML,AMPP,APM,AQC,ASF,Akiban,Apache Software Foundation,Auditing,B2B,B2C,BI Architecture,BPM technology,BPMN,BSA,Bank Secrecy Act,Basel II,Basho Riak,Big Data,Big Data accelerators,Big Data architecture,Big Data deployment,Big Data ecosystem,Big Data is use case driven,Big Data the future,BigTable,Blocked Persons List,Boolean SAT,CEP,CFTC,CFTC Interim Compliant Identifier,CICI,CIP,COSO,CPU,CRAN,CRLC,Cassandra,Chief Customer Officer,Citus Data,Cloudera,Clustrix,Columbian,Committee of Sponsoring Organizations of the Tread,Commodity Futures Trading Commission,Compliance,Comprehensive R Archive Network,Couchbase,Customer Identification Program,D-Wave,DCCLC,DCLC,DFS,DGLC,DLC,DM,DOLAP,DTC,DTCC,DW,Debarred Parties List,Denied Entities List,Denied Persons List,Dodd-Frank,EEOA,EFTA,ESB,ETL,Equal Employment Opportunity Act,FBI's Most Wanted,FOSS,FSB,FSF,FTC,FTP,Fair Isaac's,Federal Trade Commission,Financial Stability Board,GFMA,GFS,GIS,GPFS,GRC,Geotime,Global Financial Markets Association,Global Watch List,Google,Google Spanner,Governance Risk and Compliance,Greenplum,HDFS,HDSF,HNC,HOLAP,HP Vertica,HR compliance,Hadoop,Hadoop HBase,Hadoop HDFS,Hbase,Hive,Hortonworks,Huffman,IBM,IP,IRR,ISDA,ISLC,ISO 17799,ISO 27000,ISO/IEC 12207,IT compliance,IT document management,Impala,Intel,International Swaps and Derivatives Association,Josephson junction,KYC,Kelvin,LDA,LDAP,LEI,LTV,LZ77,LZ78,Legal,Legal Entity Identifier,Lucidworks,Luisi Prime Numbers,MALC,MOLAP,MPP,MV pattern,MapR,MapReduce,MarkLogic,Microsoft,MongoDB,NLP,NPV,NSCC,National Securities Clearing Corporation,Neo4j,Netezza,New SQL,NewSQL,NoSQL,NuoDB,OCC,ODS,OFAC,OFCCP,OLAP,OLC,OLTP,OOA,OSLC,Office of Federal Contract Compliance Programs,Office of Foreign Assets Control,Office of the Comptroller of the Currency,OldSQL,Oracle Spatial,PEP,Pig,Politically Exposed Persons,PostGIS,R Language,R Programming Language,R extensions,RIM,ROLAP,RTOLAP,Regulation E of the Electronic Fund Transfer Act,SAP Hana,SAPI interface,SAR,SAS,SAS complex event processing,SDLC,SEC,SMP,SOA,SOX,SQL,SQLFire,SWIFT,Sarbanes Oxley,Section 314(a),Securities and Exchange Commission,Society for the Worldwide Interbank Financial Tele,Solr,Solvency II,Splice Machine,Splunk,Sqoop,Syncsort,TPM,Targeted Countries List,Teradata,Tesla,There is no magic,Treasury department,USA PATRIOT Act,United and Strengthening America by Providing Appr,VoltDB,WOLAP,Ward Systems,XBRL,ad hoc deployment,address geocoding,adiabatic,advanced search,after tax implications,aggregates,airfare,algorithm based,algorithmic approaches,analytical pattern,analyze all business initiative types,analyze business direction,analyze business pain points,analyze overlapping automation,analyze types of technological issues,anti-money laundering,application architecture,application architecture design patterns,application impact,application portfolio architecture,applications,applications architecture,architectural drawings,architectural standards,architecture ROI framework,architecture governance life cycle,array language,assembly language,assess business alignment,association discoveries,asymmetric massively parallel processing,atomicity,automation supporting divested capabilities,batch analysis,batch analytics,blackboard pattern,bubbles,build,business architecture,business capabilities,business compliance,business designated access rights,business document management,business impact,business intelligence,business metadata,business rules,cartography,class discoveries,client libraries,cluster diagrams,cockpit gauges,code breaking,columnar,command line interfaces,compare data landscapes,competing Hadoop frameworks,compiled language,compiler,complex event processing,compliance architecture,compliance metadata,compression,conceptual data architecture,conceptual data modeling,concurrent users,configuration management metadata,connectivity diagrams,consistency,content management,control systems architecture,corporate restructuring life cycle,cross data landscape metrics,cross-discipline capabilities,cryptography,customer document management,customers,data acquisition,data analysis,data center consolidation,data center consolidation life cycle,data centric life cycle,data cleansing,data communication pattern,data discovery,data governance life cycle,data governance metadata,data integration,data manipulation language,data mart,data mining,data persistence layer metadata,data profiling,data requirements,data security,data standardization,data virtualization layer,data visualization,data visualization styles,data warehouse,data warehouse architecture,data warehousing,decommission,decommissioning metadata,define RIM business data,deployment,detach general ledger,detail analysis,dimensions,directory services metadata,distributed file system,distribution pattern,divestiture life cycle,document management,document management/content management,downsize IT operations,downsize automation,downsize business operations,drag-and-drop self-service,durability,e-Commerce,ecosystem administrator metadata,enhanced technology portfolio management,enterprise architects,enterprise architecture,enterprise service bus,error handling,event-driven pattern,external data discovery,extract transform and load,fewer features,file transfer protocol,financial compliance,first-generation language,floor plans,forecasting,fourth-generation language,frameworks,free software foundation,free space,frequency of updates,gate model,general ledger integration,geographic information system,government archival records,graph DB,graph modeling,grouping and aggregation,heat maps,hedgehog principle,high-level analysis,histograms,hub and spoke,identify IT organization impact,identify RIM data,identify acquired automation,identify automation impact,identify business organization impact,identify business scope being acquired,identify dedicated operations,identify development environment impact,identify divested business capabilities,identify legal holds,identify scope of business being divested,identify shared operations,identify unstructured data of divested areas,identifying correlations in genetic code,identifying data points,implement automation strategy,inbound metadata,inception,information systems architecture,infrastructure impact,ingestion metadata,initial architecture review,insourcing life cycle,integration architecture,integration pattern,interactive pattern,internal data discovery,internal rate of return,interpreted language,intersection and difference,investors,isolation,know your client,knowledge management,labeling images and objects within images,legal and compliance oversight,legal compliance,legal hold,life cycle,life cycle architecture,lifecycle metadata,lifetime value,linear measures event modeling,lodging,logical data architecture,logical data modeling,logical design,longitudinal analysis,machine learning for problem solving (aka self-pro,mashup architecture,mashup technology,massively parallel processing,matrix vector multiplication,merger and acquisition life cycle,mergers and acquisitions,message dissemination pattern,metadata,metadata summary,natural language processing,net present value,neural nets,neural networks,niobium,node,nonstatistical analysis,nonstatistical models,novelty discoveries,object-oriented language,online analytical processing,online transaction processing,online transaction processing systems,operational data store,operational workflow impact,operations architecture,operations life cycle,operations metadata,organizing technologies into portfolios,outbound metadata,outsourcing life cycle,outsourcing partners,parallel and distributed processing,partner integration,personnel impact,personnel severance costs,physical data modeling,physical design,pipe and filter,pipeline pattern,polynomial,populating business data glossary,post-implementation,postmortem architecture review,predict behavior,predictive analytics,prime number generation,procedural language,procedural pattern,processing pattern,production,production to nonproduction data movement,proprietary hardware,quantum computing,quantum error correction,quantum processor,qubit,query language,radar diagrams,real-time analytics,records information management,reduced code set that eliminates large amounts of,reducer size and replication rates,regulators,relational algebraic operations,relational database technology in Hadoop,report and querying,reporting architecture,requirements traceability,resource sequence pattern,right-size IT operations,right-size automation,right-size business operations,rollups,rose charts,routes,routing,rules engine,safeguard RIM data,safeguard legal hold data,scatter plots,scripting language,search and discovery,second-generation language,secure canned reporting data points,selections and projections,self-service,semistructured data,service-oriented architecture,shareholders,shelf layouts,similarity joins,singular application,software development life cycle,software reuse,solution architect,stakeholder metadata,stakeholders,statistical methods,stem and leaf plots,stimulus-response predictive models,structured data,subject matter expert,survey sampling models,suspicious activity report,symmetric multiprocessing,systems architecture,team metadata,technology categories,technology metadata,technology portfolio management,technology subcategories,terrain maps,testing a scientific hypothesis,third-generation language,tier pattern,titanium,topological,traveling salesman problem,treaty zone,union,unstructured data,usage pattern,use case driven,use case planning,use case requirements,user acceptance,validate RIM reporting,validate legal hold reporting,validation,variety,vector language,velocity,vendor impact,volume,workflow architecture,workflow automation,workflow metadata},
pages = {57--188},
publisher = {Elsevier},
title = {{Pragmatic Enterprise Architecture}},
url = {http://www.sciencedirect.com/science/article/pii/B9780128002056000032},
year = {2014}
}
@article{Perovsek2015,
abstract = {Inductive Logic Programming (ILP) and Relational Data Mining (RDM) address the task of inducing models or patterns from multi-relational data. One of the established approaches to RDM is propositionalization, characterized by transforming a relational database into a single-table representation. This paper presents a propositionalization technique called wordification which can be seen as a transformation of a relational database into a corpus of text documents. Wordification constructs simple, easy to understand features, acting as words in the transformed Bag-Of-Words representation. This paper presents the wordification methodology, together with an experimental comparison of several propositionalization approaches on seven relational datasets. The main advantages of the approach are: simple implementation, accuracy comparable to competitive methods, and greater scalability, as it performs several times faster on all experimental databases. Furthermore, the wordification methodology and the evaluation procedure are implemented as executable workflows in the web-based data mining platform ClowdFlows. The implemented workflows include also several other ILP and RDM algorithms, as well as the utility components that were added to the platform to enable access to these techniques to a wider research audience.},
author = {Perov{\v{s}}ek, Matic and Vavpeti{\v{c}}, An{\v{z}}e and Kranjc, Janez and Cestnik, Bojan and Lavra{\v{c}}, Nada},
doi = {10.1016/j.eswa.2015.04.017},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Perov{\v{s}}ek et al. - 2015 - Wordification Propositionalization by unfolding relational data into bags of words.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Inductive Logic Programming,Propositionalization,Relational Data Mining,Text mining,Wordification},
month = {oct},
number = {17-18},
pages = {6442--6456},
title = {{Wordification: Propositionalization by unfolding relational data into bags of words}},
url = {http://www.sciencedirect.com/science/article/pii/S095741741500247X},
volume = {42},
year = {2015}
}
@article{Zhang2014,
abstract = {A key issue of quantitative investment (QI) product design is how to select representative features for stock prediction. However, existing stock prediction models adopt feature selection algorithms that rely on correlation analysis. This paper is the first to apply observational data-based causal analysis to stock prediction. Causalities represent direct influences between various stock features (important for stock analysis), while correlations cannot distinguish direct influences from indirect ones. This study proposes the causal feature selection (CFS) algorithm to select more representative features for better stock prediction modeling. CFS first identifies causalities between variables and then, based on the results, generates a feature subset. Based on 13-year data from the Shanghai Stock Exchanges, comparative experiments were conducted between CFS and three well-known feature selection algorithms, namely, principal component analysis (PCA), decision trees (DT; CART), and the least absolute shrinkage and selection operator (LASSO). CFS performs best in terms of accuracy and precision in most cases when combined with each of the seven baseline models, and identifies 18 important consistent features. In conclusion, CFS has considerable potential to improve the development of QI product.},
author = {Zhang, Xiangzhou and Hu, Yong and Xie, Kang and Wang, Shouyang and Ngai, E.W.T. and Liu, Mei},
doi = {10.1016/j.neucom.2014.01.057},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2014 - A causal feature selection algorithm for stock prediction modeling.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Causal discovery,Data mining,Feature selection,Stock prediction,V-structure,naranja,rojo},
mendeley-tags = {naranja,rojo},
month = {oct},
pages = {48--59},
title = {{A causal feature selection algorithm for stock prediction modeling}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231214005359 http://linkinghub.elsevier.com/retrieve/pii/S0925231214005359},
volume = {142},
year = {2014}
}
@article{Maldonado2014,
abstract = {Feature selection and classification of imbalanced data sets are two of the most interesting machine learning challenges, attracting a growing attention from both, industry and academia. Feature selection addresses the dimensionality reduction problem by determining a subset of available features to build a good model for classification or prediction, while the class-imbalance problem arises when the class distribution is too skewed. Both issues have been independently studied in the literature, and a plethora of methods to address high dimensionality as well as class-imbalance has been proposed. The aim of this work is to simultaneously explore both issues, proposing a family of methods that select those attributes that are relevant for the identification of the target class in binary classification. We propose a backward elimination approach based on successive holdout steps, whose contribution measure is based on a balanced loss function obtained on an independent subset. Our experiments are based on six highly imbalanced microarray data sets, comparing our methods with well-known feature selection techniques, and obtaining a better prediction with consistently fewer relevant features.},
author = {Maldonado, Sebasti{\'{a}}n and Weber, Richard and Famili, Fazel},
doi = {10.1016/j.ins.2014.07.015},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Maldonado, Weber, Famili - 2014 - Feature selection for high-dimensional class-imbalanced data sets using Support Vector Machines.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Data mining,Dimensionality reduction,Feature selection,Imbalanced data set,Support Vector Machine,azul,naranja},
mendeley-tags = {azul,naranja},
month = {dec},
pages = {228--246},
title = {{Feature selection for high-dimensional class-imbalanced data sets using Support Vector Machines}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025514007154 http://linkinghub.elsevier.com/retrieve/pii/S0020025514007154},
volume = {286},
year = {2014}
}
@article{Iquebal2014,
abstract = {The current research presents a methodology for classification based on Mahalanobis Distance (MD) and Association Mining using Rough Sets Theory (RST). MD has been used in Mahalanobis Taguchi System (MTS) to develop classification scheme for systems having dichotomous states or categories. In MTS, selection of important features or variables to improve classification accuracy is done using Signal-to-Noise (S/N) ratios and Orthogonal Arrays (OAs). OAs has been reviewed for limitations in handling large number of variables. Secondly, penalty for over-fitting or regularization is not included in the feature selection process for the MTS classifier. Besides, there is scope to enhance the utility of MTS to a classification-cum-causality analysis method by adding comprehensive information about the underlying process which generated the data. This paper proposes to select variables based on maximization of degree-of-dependency between Subset of System Variables (SSVs) and system classes or categories (R). Degree-of-dependency, which reflects goodness-of-model and hence goodness of the SSV, is measured by conditional probability of system states on subset of variables. Moreover, a suitable regularization factor equivalent to L0 norm is introduced in an optimization problem which jointly maximizes goodness-of-model and effect of regularization. Dependency between SSVs and R is modeled via the equivalent sets of Rough Set Theory. Two new variants of MTS classifier are developed and their performance in terms of accuracy of classification is evaluated on test datasets from five case studies. The proposed variants of MTS are observed to be performing better than existing MTS methods and other classification techniques found in literature.},
author = {Iquebal, Ashif Sikandar and Pal, Avishek and Ceglarek, Darek and Tiwari, Manoj Kumar},
doi = {10.1016/j.eswa.2014.06.019},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Iquebal et al. - 2014 - Enhancement of Mahalanobis–Taguchi System via Rough Sets based Feature Selection.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Conditional probability,Data mining,Feature Selection,IF–THEN rules,Mahalanobis Taguchi System,Orthogonal Arrays,Over-fitting,Regularization,Rough Sets},
month = {dec},
number = {17},
pages = {8003--8015},
title = {{Enhancement of Mahalanobis–Taguchi System via Rough Sets based Feature Selection}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414003601},
volume = {41},
year = {2014}
}
@article{Mangalova2014,
abstract = {The paper deals with a modeling procedure which aims to predict the power output of wind farm electricity generators. The following modeling steps are proposed: factor selection, raw data pretreatment, model evaluation and optimization. Both heuristic and formal methods are combined to construct the model. The basic modeling approach here is the k-nearest neighbors method.},
author = {Mangalova, E. and Agafonov, E.},
doi = {10.1016/j.ijforecast.2013.07.008},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Mangalova, Agafonov - 2014 - Wind power forecasting using the mmlmath altimg=si14.gif display=inline overflow=scroll xmlnsxocs=httpwww.e.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Cross-validation,Data mining,Energy forecasting*,Feature selection,Forecasting competitions*,Nonparametric models,Regression tree},
month = {apr},
number = {2},
pages = {402--406},
title = {{Wind power forecasting using the {\textless}mml:math altimg="si14.gif" display="inline" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207013000848},
volume = {30},
year = {2014}
}
@article{Liang2015,
abstract = {Financial distress prediction is always important for financial institutions in order for them to assess the financial health of enterprises and individuals. Bankruptcy prediction and credit scoring are two important issues in financial distress prediction where various statistical and machine learning techniques have been employed to develop financial prediction models. Since there are no generally agreed upon financial ratios as input features for model development, many studies consider feature selection as a pre-processing step in data mining before constructing the models. However, most works only focused on applying specific feature selection methods over either bankruptcy prediction or credit scoring problem domains. In this work, a comprehensive study is conducted to examine the effect of performing filter and wrapper based feature selection methods on financial distress prediction. In addition, the effect of feature selection on the prediction models obtained using various classification techniques is also investigated. In the experiments, two bankruptcy and two credit datasets are used. In addition, three filter and two wrapper based feature selection methods combined with six different prediction models are studied. Our experimental results show that there is no the best combination of the feature selection method and the classification technique over the four datasets. Moreover, depending on the chosen techniques, performing feature selection does not always improve the prediction performance. However, on average performing the genetic algorithm and logistic regression for feature selection can provide prediction improvements over the credit and bankruptcy datasets respectively.},
author = {Liang, Deron and Tsai, Chih-Fong and Wu, Hsin-Ting},
doi = {10.1016/j.knosys.2014.10.010},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Liang, Tsai, Wu - 2015 - The effect of feature selection on financial distress prediction.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Bankruptcy prediction,Credit scoring,Data mining,Feature selection,Financial distress prediction},
month = {jan},
pages = {289--297},
title = {{The effect of feature selection on financial distress prediction}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705114003773},
volume = {73},
year = {2015}
}
@article{Barak2015,
abstract = {In this research, a novel approach is developed to predict stocks return and risks. In this three stage method, through a comprehensive investigation all possible features which can be effective on stocks risk and return are identified. Then, in the next stage risk and return are predicted by applying data mining techniques for the given features. Finally, we develop a hybrid algorithm, on the basis of filter and function-based clustering; the important features in risk and return prediction are selected then risk and return re-predicted. The results show that the proposed hybrid model is a proper tool for effective feature selection and these features are good indicators for the prediction of risk and return. To illustrate the approach as well as to train data and test, we apply it to Tehran Stock Exchange (TSE) data from 2002 to 2011.},
author = {Barak, Sasan and Modarres, Mohammad},
doi = {10.1016/j.eswa.2014.09.026},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Barak, Modarres - 2015 - Developing an approach to evaluate stocks by forecasting effective features with data mining methods.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification algorithm,Data mining,Feature selection,Function-based clustering method,Stock market},
month = {feb},
number = {3},
pages = {1325--1339},
title = {{Developing an approach to evaluate stocks by forecasting effective features with data mining methods}},
url = {http://www.sciencedirect.com/science/article/pii/S095741741400565X http://linkinghub.elsevier.com/retrieve/pii/S095741741400565X},
volume = {42},
year = {2015}
}
@article{Fan2014,
abstract = {This paper presents a data mining (DM) based approach to developing ensemble models for predicting next-day energy consumption and peak power demand, with the aim of improving the prediction accuracy. This approach mainly consists of three steps. Firstly, outlier detection, which merges feature extraction, clustering analysis, and the generalized extreme studentized deviate (GESD), is performed to remove the abnormal daily energy consumption profiles. Secondly, the recursive feature elimination (RFE), an embedded variable selection method, is applied to select the optimal inputs to the base prediction models developed separately using eight popular predictive algorithms. The parameters of each model are then obtained through leave-group-out cross validation (LGOCV). Finally, the ensemble model is developed and the weights of the eight predictive models are optimized using genetic algorithm (GA). The approach is adopted to analyze the large energy consumption data of the tallest building in Hong Kong. The prediction accuracies of the ensemble models measured by mean absolute percentage error (MAPE) are 2.32{\%} and 2.85{\%} for the next-day energy consumption and peak power demand respectively, which are evidently higher than those of individual base models. The results also show that the outlier detection method is effective in identifying the abnormal daily energy consumption profiles. The RFE process can significantly reduce the computation load while enhancing the model performance. The ensemble models are valuable for developing strategies of fault detection and diagnosis, operation optimization and interactions between buildings and smart grid.},
author = {Fan, Cheng and Xiao, Fu and Wang, Shengwei},
doi = {10.1016/j.apenergy.2014.04.016},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Xiao, Wang - 2014 - Development of prediction models for next-day building energy consumption and peak power demand using data mini.pdf:pdf},
issn = {03062619},
journal = {Applied Energy},
keywords = {Building energy prediction,Clustering analysis,Data mining,Ensemble model,Feature extraction,Recursive feature elimination,naranja},
mendeley-tags = {naranja},
month = {aug},
pages = {1--10},
title = {{Development of prediction models for next-day building energy consumption and peak power demand using data mining techniques}},
url = {http://www.sciencedirect.com/science/article/pii/S0306261914003596 http://linkinghub.elsevier.com/retrieve/pii/S0306261914003596},
volume = {127},
year = {2014}
}
@article{Ayaz2015,
abstract = {Compressive strength and UPV parameters are the methods that are used to determine high-volume mineral admixture concrete quality. But experiments for all levels of these parameters are expensive, difficult and time consuming. For determination of output values, classifiers with model extraction features can be used. In this study, classifiers, with the rule-based M5 rule and tree model M5P in the area of data mining are used to predict the compressive strength and UPV of concrete mixtures after 3, 7, 28 and 120days of curing. The M5 rule and tree model M5P are tested using the available test data of 40 different concrete mix-designs gathered from literature [1]. The input of the model is a variable data set corresponding to concrete mixture proportions. The findings of this study indicated that the M5 rule and tree model M5P models are sufficient tools for estimating the compressive strength and UPV of concrete. 97{\%} and 87{\%} success is obtained in predicting compressive strength and UPV results, respectively.},
author = {Ayaz, Yaşar and Kocamaz, Adnan Fatih and Karako{\c{c}}, Mehmet Burhan},
doi = {10.1016/j.conbuildmat.2015.06.029},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ayaz, Kocamaz, Karako{\c{c}} - 2015 - Modeling of compressive strength and UPV of high-volume mineral-admixtured concrete using rule-based M5.pdf:pdf},
issn = {09500618},
journal = {Construction and Building Materials},
keywords = {Compressive strength,Concrete,Data mining,M5 rule,Tree model M5P,UPV},
month = {sep},
pages = {235--240},
title = {{Modeling of compressive strength and UPV of high-volume mineral-admixtured concrete using rule-based M5 rule and tree model M5P classifiers}},
url = {http://www.sciencedirect.com/science/article/pii/S0950061815007114},
volume = {94},
year = {2015}
}
@article{Darabad2015,
abstract = {Suggestion and application of a set of new features for on-line Partial Discharge (PD) monitoring, where there is no information about the type of PD is a challenging task for condition assessment of power equipments, such as a power transformer. This is looked for in this paper. So far, in past various techniques have been employed to develop a comprehensive PD monitoring system, however limited success has been achieved. One of the challenging issues in this field is the discovering of proper features capable of differentiating the involvement of possible types of PD sources. In order to examine the efficiency of the method established in this paper, which is based on application of a set of new feature spaces, texture feature analysis, followed by application of principal component analysis (PCA) and self-organizing map (SOM) is used to analyze and interpret the time-domain-captured PD data. The results of this work demonstrate the capabilities of the aforementioned features space to be used as a supplementary knowledge-base to help experts making their decisions confidently.},
author = {Darabad, V.P. and Vakilian, M. and Blackburn, T.R. and Phung, B.T.},
doi = {10.1016/j.ijepes.2015.03.016},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Darabad et al. - 2015 - An efficient PD data mining method for power transformer defect models using SOM technique.pdf:pdf},
issn = {01420615},
journal = {International Journal of Electrical Power {\&} Energy Systems},
keywords = {BMU,Ch,DDF,DGA,DWT,Data mining,Defect model,FRA,GLCM,GLDV,Gs/s,HFCT,HV,IR,Lab,Mpts,Ms/s,Ng,PC,PCA,PD,PDC,PRPD,Partial discharge,Power transformer,RVM,SNE,SOM,SVM,Support Vector Machine,best match unit,channel,dielectric dissipation factor,discrete wavelet transform,dissolved gas analysis,frequency response analysis,giga sample per second,gray level covariance matrix,gray level difference vector,gray level value,high frequency current transformer,high voltage,insulation resistance,laboratory,mega points,mega sample per second,naranja,p.f.,partial discharge,phase resolved partial discharge,polarization and depolarization current,power frequency,principal component,principal component analysis,recovery voltage measurement,self-organizing map,stochastic neighbor embedding},
mendeley-tags = {naranja},
month = {oct},
pages = {373--382},
title = {{An efficient PD data mining method for power transformer defect models using SOM technique}},
url = {http://www.sciencedirect.com/science/article/pii/S0142061515001519 http://linkinghub.elsevier.com/retrieve/pii/S0142061515001519},
volume = {71},
year = {2015}
}
@article{Zhou2015,
abstract = {Experts in finance and accounting select feature subset for corporate financial distress prediction according to their professional understanding of the characteristics of the features, while researchers in data mining often believe that data alone can tell everything and they use various mining techniques to search the feature subset without considering the financial and accounting meanings of the features. This paper investigates the performance of different financial distress prediction models with features selection approaches based on domain knowledge or data mining techniques. The empirical results show that there is no significant difference between the best classification performance of models with features selection guided by data mining techniques and that by domain knowledge. However, the combination of domain knowledge and genetic algorithm based features selection method can outperform unique domain knowledge and unique data mining based features selection method on AUC performance.},
author = {Zhou, Ligang and Lu, Dong and Fujita, Hamido},
doi = {10.1016/j.knosys.2015.04.017},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Lu, Fujita - 2015 - The performance of corporate financial distress prediction models with features selection guided by domain kno.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Data mining,Domain knowledge,Features selection,Financial distress prediction,naranja,rojo},
mendeley-tags = {naranja,rojo},
month = {sep},
pages = {52--61},
title = {{The performance of corporate financial distress prediction models with features selection guided by domain knowledge and data mining approaches}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115001616 http://linkinghub.elsevier.com/retrieve/pii/S0950705115001616},
volume = {85},
year = {2015}
}
@article{Sajadfar2015,
abstract = {This paper presents an informatics framework to apply feature-based engineering concept for cost estimation supported with data mining algorithms. The purpose of this research work is to provide a practical procedure for more accurate cost estimation by using the commonly available manufacturing process data associated with ERP systems. The proposed method combines linear regression and data-mining techniques, leverages the unique strengths of the both, and creates a mechanism to discover cost features. The final estimation function takes the user's confidence level over each member technique into consideration such that the application of the method can phase in gradually in reality by building up the data mining capability. A case study demonstrates the proposed framework and compares the results from empirical cost prediction and data mining. The case study results indicate that the combined method is flexible and promising for determining the costs of the example welding features. With the result comparison between the empirical prediction and five different data mining algorithms, the ANN algorithm shows to be the most accurate for welding operations.},
author = {Sajadfar, Narges and Ma, Yongsheng},
doi = {10.1016/j.aei.2015.06.001},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Sajadfar, Ma - 2015 - A hybrid cost estimation framework based on feature-oriented data mining approach.pdf:pdf},
issn = {14740346},
journal = {Advanced Engineering Informatics},
keywords = {Cost estimation,Data mining,ERP,Feature modeling,Welding feature,gris},
mendeley-tags = {gris},
month = {aug},
number = {3},
pages = {633--647},
title = {{A hybrid cost estimation framework based on feature-oriented data mining approach}},
url = {http://www.sciencedirect.com/science/article/pii/S1474034615000610 http://linkinghub.elsevier.com/retrieve/pii/S1474034615000610},
volume = {29},
year = {2015}
}
@article{Bhuyan2015,
abstract = {This paper addresses the selection of sub-feature from each feature using fuzzy methodologies maintaining the privacy during collection of data from participating parties in distributed environment. Based on fuzzy random variables conditional expectation is used in which two fuzzy sets are generated using Borel set that helps to determine sub-feature within certain interval. The privacy and selection of sub-feature leading to a distinguished class is the main objective of this research work. These two problems are directly related to data mining problems of classification and characterization of feature. In many cases traditional techniques are not suitable for complex databases. However our methodology provides better way for selection of sub-features under different situations. The proposed model and techniques both presents extensive theoretical analysis and experimental results. The experiments show the effectiveness and performance based on real world data set.},
author = {Bhuyan, Hemanta Kumar and Kamila, Narendra Kumar},
doi = {10.1016/j.asoc.2015.06.060},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bhuyan, Kamila - 2015 - Privacy preserving sub-feature selection in distributed data mining.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {Distributed data mining,Feature selection,Fuzzy probability,Fuzzy random variable,Privacy,purpura},
mendeley-tags = {purpura},
month = {nov},
pages = {552--569},
title = {{Privacy preserving sub-feature selection in distributed data mining}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494615004536 http://linkinghub.elsevier.com/retrieve/pii/S1568494615004536},
volume = {36},
year = {2015}
}
@article{Maldonado2015,
abstract = {Churn prediction is an important application of classification models that identify those customers most likely to attrite based on their respective characteristics described by e.g. socio-demographic and behavioral variables. Since nowadays more and more of such features are captured and stored in the respective computational systems, an appropriate handling of the resulting information overload becomes a highly relevant issue when it comes to build customer retention systems based on churn prediction models. As a consequence, feature selection is an important step of the classifier construction process. Most feature selection techniques; however, are based on statistically inspired validation criteria, which not necessarily lead to models that optimize goals specified by the respective organization. In this paper we propose a profit-driven approach for classifier construction and simultaneous variable selection based on support vector machines. Experimental results show that our models outperform conventional techniques for feature selection achieving superior performance with respect to business-related goals.},
author = {Maldonado, Sebasti{\'{a}}n and Flores, {\'{A}}lvaro and Verbraken, Thomas and Baesens, Bart and Weber, Richard},
doi = {10.1016/j.asoc.2015.05.058},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Maldonado et al. - 2015 - Profit-based feature selection using support vector machines – General framework and an application for cust.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {Churn prediction,Customer retention,Data mining,Feature selection,Maximum profit,Support vector machines,naranja},
mendeley-tags = {naranja},
month = {oct},
pages = {740--748},
title = {{Profit-based feature selection using support vector machines – General framework and an application for customer retention}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494615004093 http://linkinghub.elsevier.com/retrieve/pii/S1568494615004093},
volume = {35},
year = {2015}
}
@article{Koutanaei2015,
abstract = {Data mining techniques have numerous applications in credit scoring of customers in the banking field. One of the most popular data mining techniques is the classification method. Previous researches have demonstrated that using the feature selection (FS) algorithms and ensemble classifiers can improve the banks' performance in credit scoring problems. In this domain, the main issue is the simultaneous and the hybrid utilization of several FS and ensemble learning classification algorithms with respect to their parameters setting, in order to achieve a higher performance in the proposed model. As a result, the present paper has developed a hybrid data mining model of feature selection and ensemble learning classification algorithms on the basis of three stages. The first stage, as expected, deals with the data gathering and pre-processing. In the second stage, four FS algorithms are employed, including principal component analysis (PCA), genetic algorithm (GA), information gain ratio, and relief attribute evaluation function. In here, parameters setting of FS methods is based on the classification accuracy resulted from the implementation of the support vector machine (SVM) classification algorithm. After choosing the appropriate model for each selected feature, they are applied to the base and ensemble classification algorithms. In this stage, the best FS algorithm with its parameters setting is indicated for the modeling stage of the proposed model. In the third stage, the classification algorithms are employed for the dataset prepared from each FS algorithm. The results exhibited that in the second stage, PCA algorithm is the best FS algorithm. In the third stage, the classification results showed that the artificial neural network (ANN) adaptive boosting (AdaBoost) method has higher classification accuracy. Ultimately, the paper verified and proposed the hybrid model as an operative and strong model for performing credit scoring.},
author = {Koutanaei, Fatemeh Nemati and Sajedi, Hedieh and Khanbabaei, Mohammad},
doi = {10.1016/j.jretconser.2015.07.003},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Koutanaei, Sajedi, Khanbabaei - 2015 - A hybrid data mining model of feature selection algorithms and ensemble learning classifiers for.pdf:pdf},
issn = {09696989},
journal = {Journal of Retailing and Consumer Services},
keywords = {Classification,Credit scoring,Data mining,Ensemble learning,Feature selection,naranja},
mendeley-tags = {naranja},
month = {nov},
pages = {11--23},
title = {{A hybrid data mining model of feature selection algorithms and ensemble learning classifiers for credit scoring}},
url = {http://www.sciencedirect.com/science/article/pii/S0969698915300060 http://linkinghub.elsevier.com/retrieve/pii/S0969698915300060},
volume = {27},
year = {2015}
}
@article{Qian2015,
author = {Qian, Wenbin and Shu, Wenhao},
doi = {10.1016/j.neucom.2015.05.105},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Qian, Shu - 2015 - Mutual information criterion for feature selection from incomplete data.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {gris,naranja},
mendeley-tags = {gris,naranja},
month = {nov},
pages = {210--220},
title = {{Mutual information criterion for feature selection from incomplete data}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215008036},
volume = {168},
year = {2015}
}
@article{Bae2011,
abstract = {Many enterprises have been devoting a significant portion of their budget to product development in order to distinguish their products from those of their competitors and to make them better fit the needs and wants of customers. Hence, businesses should dBae, J. K., {\&} Kim, J. (2011). Product development with data mining techniques: A case on design of digital camera. Expert Systems with Applications, 38(8), 9274–9280. doi:10.1016/j.eswa.2011.01.030evelop product designing that could satisfy the customers' requirements since this will increase the enterprise's competitiveness and it is an essential criterion to earning higher loyalties and profits. This paper investigates the following research issues in the development of new digital camera products: (1) What exactly are the customers' "needs" and "wants" for digital camera products? (2) What features is more importance than others? (3) Can product design and planning for product lines/product collection be integrated with the knowledge of customers? (4) How can the rules help us to make a strategy during we design new digital camera? To investigate these research issues, the Apriori and C5.0 algorithms are methodologies of association rules and decision trees for data mining, which is implemented to mine customer's needs. Knowledge extracted from data mining results is illustrated as knowledge patterns and rules on a product map in order to propose possible suggestions and solutions for product design and marketing. ?? 2011 Published by Elsevier Ltd.},
author = {Bae, Jae Kwon and Kim, Jinhwa},
doi = {10.1016/j.eswa.2011.01.030},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bae, Kim - 2011 - Product development with data mining techniques A case on design of digital camera.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Association rule,Data mining based methodology,Decision tree based models,New product development,rojo},
mendeley-tags = {rojo},
month = {aug},
number = {8},
pages = {9274--9280},
title = {{Product development with data mining techniques: A case on design of digital camera}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417411000509},
volume = {38},
year = {2011}
}
@article{Liao2008,
abstract = {Retailing consists of the final activities and steps needed to place a product in the hands of the consumer or to provide services to the consumer. In fact, retailing is actually the last step in a supply chain that may stretch from Europe or Asia to the customer's hometown. Therefore, any firm that sells a product or provides a service to the final consumer is performing the retailing function. On the other hand, product line extension, which adds depth to an existing product line by introducing new products in the same product category, can give customers greater choice and help to protect the firm from flanking attack by a competitor. In addition, a product line extension is marketed under the same general brand as a previous item or items. Thus, to distinguish the brand extension from the other item(s) under the primary brand, the retailer can either add secondary brand identification or add a generic brand. This paper investigates product line and brand extension issues in the Taiwan branch of a leading international retailing company, Carrefour, which is a hypermarket retailer. This paper develops a relational database and proposes Apriori algorithm and K-means as methodologies for association rule and cluster analysis for data mining, which is then implemented to mine customer knowledge from household customers. Knowledge extraction by data mining results is illustrated as knowledge patterns/rules and clusters in order to propose suggestions and solutions to the case firm for product line and brand extensions and knowledge management. ?? 2007.},
author = {Liao, Shu-Hsien and Chen, Chyuan-Meei and Wu, Chung-Hsin},
doi = {10.1016/j.eswa.2007.01.036},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Liao, Chen, Wu - 2008 - Mining customer knowledge for product line and brand extension in retailing.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Association rules,Brand extension,Cluster analysis,Data mining,Knowledge extraction,Product line extension,Retailing,naranja,purpura},
mendeley-tags = {naranja,purpura},
month = {apr},
number = {3},
pages = {1763--1776},
title = {{Mining customer knowledge for product line and brand extension in retailing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S095741740700053X},
volume = {34},
year = {2008}
}
@incollection{MarkF.HornickErikMarcade2006,
author = {Hornick, Mark F. and Marcad{\'{e}}, Erik and Venkayala, Sunil},
booktitle = {Java Data Mining},
doi = {10.1016/B978-012370452-8/50028-1},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hornick, Marcad{\'{e}}, Venkayala - 2007 - Solving Problems in Industry.pdf:pdf},
isbn = {9780123704528},
issn = {1422-6421},
number = {62},
pages = {25--49},
pmid = {21728113},
publisher = {Elsevier},
title = {{Solving Problems in Industry}},
url = {http://www.karger.com/doi/10.1159/000144207 http://linkinghub.elsevier.com/retrieve/pii/B9780123704528500281},
volume = {87},
year = {2007}
}
